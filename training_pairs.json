[
  {
  "input": {
    "starting_page": 1,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- I\n\nINTRODUCTION\n\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 1\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em)\n\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\n#v(0.5em)\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\n#v(0.5em)\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\n#v(0.5em)\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\n#v(0.5em)\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements.\n"
  },
  {
  "input": {
    "starting_page": 1,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- I\n\nINTRODUCTION\n\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 1\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em)\n\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\n#v(0.5em)\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\n#v(0.5em)\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\n#v(0.5em)\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\n#v(0.5em)\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements.\n"
  },
  {
  "input": {
    "starting_page": 5,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- I\n\nINTRODUCTION\n\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 5\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em)\n\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\n#v(0.5em)\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\n#v(0.5em)\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\n#v(0.5em)\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\n#v(0.5em)\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements.\n"
  },
  {
  "input": {
    "starting_page": 7,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- I\n\nINTRODUCTION\n\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements.\n\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 7\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em)\n\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\n#v(0.5em)\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements.\n\n#v(0.5em)\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\n#v(0.5em)\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file including file sizes, upload timestamps, and processing status.\n\n#v(0.5em)\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\n"
  },
  {
  "input": {
    "starting_page": 10,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- I\n\nINTRODUCTION\n\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing.\n\nincluding file sizes, upload timestamps, and processing status.\n\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file\n\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 10\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em)\n\nThe technical architecture of this system is built upon a robust and scalable foundation that seamlessly integrates multiple state-of-the-art technologies to deliver exceptional performance and user experience. At its core, the application utilizes  PostgreSQL  enhanced with the  pgvector  extension to create a powerful vector database capable of storing and efficiently searching through high-dimensional embeddings generated from document content. The system employs the advanced all-mpnet-base-v2 embedding model from  SentenceTransformers , which produces 768-dimensional vector representations of text chunks, enabling superior semantic understanding and context preservation compared to traditional keyword-based search methods. This sophisticated embedding approach allows the system to understand conceptual relationships between different parts of documents, even when they use different terminology or are expressed in varying writing styles. For natural language processing and response generation, the application integrates  OpenAI's  GPT-4o-mini model, which has been specifically configured to produce structured JSON responses that can include text explanations, executable Python code for visualizations, and data extraction routines for table generation.\n\n#v(0.5em)\nThis innovative PDF  chatbot  project represents a groundbreaking approach to document interaction, transforming the traditional static reading experience into a dynamic, intelligent conversation system. Built using  Streamlit  as the primary web framework, this application enables users to upload large PDF documents up to 1GB in size and engage in natural language conversations to extract information, generate insights, and create comprehensive analyses directly from document content. The system leverages cutting-edge Retrieval-Augmented Generation (RAG) technology, which combines the power of semantic search capabilities with advanced large language models to provide accurate, contextual responses that extend far beyond simple question-answering scenarios. What truly sets this  chatbot  apart from conventional document processing tools is its sophisticated ability to automatically generate  matplotlib  visualizations, create structured data tables, and produce comprehensive analytical reports based entirely on content extracted from uploaded PDF documents. This makes it an invaluable tool for researchers, business analysts, legal professionals, students, and anyone who needs to quickly extract actionable insights from large document collections while maintaining the flexibility to perform complex analytical tasks without requiring extensive technical knowledge or manual data processing including file sizes, upload timestamps, and processing status.\n\n#v(0.5em)\nThe user experience has been carefully designed to provide an intuitive and powerful interface that makes complex document analysis accessible to users regardless of their technical background or expertise level. The main interface features a familiar chat-based design with custom avatar support for brand consistency and professional appearance, while the comprehensive sidebar provides complete document management capabilities including drag-and-drop PDF upload functionality, real-time file size validation with clear feedback mechanisms, document library management with detailed metadata display, and the ability to seamlessly switch between single-document focused searches and multi-document cross-referencing modes. The system supports both focused analysis of individual documents and comprehensive cross-document research, allowing users to identify patterns, contradictions, and complementary information across multiple sources simultaneously. The chat interface maintains conversation history throughout sessions, enabling users to build upon previous queries and maintain context across multiple interactions, while the real-time processing indicators provide clear feedback about system operations including embedding generation progress, search execution status, and response generation phases.\n\n#v(0.5em)\nThe system's intelligence extends to automatically detecting the type of response required based on user queries, whether they're requesting data visualizations, structured tables, or detailed textual analysis, and then generating appropriate responses in a structured JSON format that enables dynamic content rendering. When users request charts or graphs, the system extracts relevant numerical data from PDF content and  generates complete  matplotlib  code that produces meaningful visualizations, including bar charts for categorical comparisons, line graphs for trend analysis, pie charts for distribution visualization, and scatter plots for correlation analysis. Similarly, when users request tabular data, the system intelligently structures unstructured PDF information into clear, readable pandas  DataFrames  that organize complex information into actionable insights. The document processing capabilities include sophisticated text extraction using  PyPDFLoader , which handles various PDF formats and layouts while preserving document structure and formatting information. The system implements SHA-256 hashing for duplicate detection, ensuring efficient storage and preventing redundant processing of identical documents, while also maintaining detailed metadata about each uploaded file.\n\n#v(0.5em)\nThe practical applications of this PDF  chatbot  span across numerous professional domains and use cases, making it an incredibly versatile tool for modern knowledge work and research activities. In business intelligence scenarios, users can upload financial reports, market research documents, quarterly earnings statements, and industry analyses to quickly extract key performance metrics, generate comprehensive trend analyses, create visual representations of financial data, and produce executive summaries without manually parsing through hundreds of pages of dense documentation. Academic researchers can process multiple research papers simultaneously, compare findings across different studies, extract statistical data for meta-analyses, generate comparative tables that highlight key differences or similarities between various research methodologies and conclusions, and identify gaps in existing literature through cross-document analysis. Legal professionals can leverage the system to analyze contracts, policy documents, regulatory filings, and case law references, quickly finding specific clauses, extracting important dates and obligations, generating summary tables that organize complex legal information into actionable insights, and ensuring comprehensive coverage of relevant legal precedents and requirements.\n\n\n"
  },
  {
  "input": {
    "starting_page": 5,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- II\n\nLITERATURE  SURVEY\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 5\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\n#v(1em)\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\n#v(1em)\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\n#v(1em)\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\n#v(1em)\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\n#v(1em)\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\n#v(1em)\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\n#v(1em)\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\n#v(1em)\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\n#v(1em)\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\n#v(1em)\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\n#v(1em)\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\n#v(1em)\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios.\n\n"
  },
  {
  "input": {
    "starting_page": 5,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- II\n\nLITERATURE  SURVEY\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 5\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\n#v(1em)\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\n#v(1em)\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\n#v(1em)\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\n#v(1em)\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\n#v(1em)\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\n#v(1em)\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\n#v(1em)\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\n#v(1em)\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\n#v(1em)\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\n#v(1em)\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\n#v(1em)\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\n#v(1em)\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios.\n\n"
  },
  {
  "input": {
    "starting_page": 8,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- II\n\nLITERATURE  SURVEY\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 8\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\n#v(1em)\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\n#v(1em)\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\n#v(1em)\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\n#v(1em)\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\n#v(1em)\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\n#v(1em)\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\n#v(1em)\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\n#v(1em)\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\n#v(1em)\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\n#v(1em)\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\n#v(1em)\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\n#v(1em)\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios.\n\n"
  },
  {
  "input": {
    "starting_page": 5,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- II\n\nLITERATURE  SURVEY\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 5\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\n#v(1em)\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\n#v(1em)\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\n#v(1em)\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\n#v(1em)\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\n#v(1em)\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\n#v(1em)\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\n#v(1em)\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\n#v(1em)\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\n#v(1em)\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\n#v(1em)\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\n#v(1em)\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\n#v(1em)\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios.\n\n"
  },
  {
  "input": {
    "starting_page": 15,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- II\n\nLITERATURE  SURVEY\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios.\n\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 15\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nThere are some previous studies to develop document-based conversational AI systems and retrieval-augmented generation in the literature.\n\n#v(1em)\nLewis et al. (2020) introduced the foundational RAG architecture that combines parametric and non-parametric memory for language generation tasks. They used dense passage retrieval with BERT-based encoders for both questions and passages, demonstrating significant improvements in open-domain question answering. Their results showed that RAG models can generate more factual and knowledge-grounded responses compared to purely parametric approaches, with future work focusing on scaling to larger knowledge bases.\n\n#v(1em)\nReimers  and  Gurevych  (2019) proposed Sentence-BERT for generating semantically meaningful sentence embeddings using  siamese  and triplet network structures. They modified BERT's pooling strategy and fine-tuned on sentence pair classification tasks to produce fixed-size embeddings suitable for similarity comparison. The system achieved superior performance on semantic textual similarity benchmarks, with future research directions focusing on multilingual sentence embeddings and cross-lingual retrieval applications.\n\n#v(1em)\nKarpukhin  et al. (2020) developed Dense Passage Retrieval (DPR) system using dual-encoder architecture with BERT models for encoding questions and passages separately. They implemented contrastive learning with in-batch negatives and hard negative mining for training effective dense representations. Results demonstrated that DPR significantly outperforms BM25 sparse retrieval on multiple open-domain QA datasets, with the authors discussing future applications in real-time retrieval systems.\n\n#v(1em)\nGuu  et al. (2020) implemented REALM (Retrieval-Augmented Language Model Pre-training) by integrating retrieval directly into the pre-training process using a knowledge retriever based on dense representations. They used masked language modeling with retrieved passages and implemented asynchronous updates for the retrieval index during training. Results showed improved performance on knowledge-intensive tasks like Open-QA, with the authors planning to extend the approach to larger-scale knowledge sources and multi-modal retrieval.\n\n#v(1em)\nChen et al. (2021) developed  LayoutLM  for document understanding by incorporating spatial information alongside textual content using 2D position embeddings. They pre-trained on large-scale document images with masked visual-language modeling objectives and fine-tuned on various document AI tasks. The system achieved state-of-the-art results on form understanding and document classification benchmarks, with future  work focusing on end-to-end document parsing and table extraction capabilities.\n\n#v(1em)\nBorgeaud  et al. (2022) introduced RETRO (Retrieval-Enhanced Transformer) which integrates retrieval into every layer of the transformer architecture using cross-attention mechanisms. They implemented chunked cross-attention with a trillion-token retrieval database and demonstrated that smaller RETRO models can outperform much larger standard transformers. The system showed significant improvements in language modeling perplexity, with future applications planned for dialogue systems and knowledge-intensive generation tasks.\n\n#v(1em)\nThakur  et al. (2021) conducted comprehensive evaluation of sentence embedding methods across diverse tasks using the BEIR benchmark suite. They evaluated multiple models including SBERT, all-mpnet-base-v2, and domain-specific embeddings on information retrieval tasks spanning different domains. Results demonstrated that all-mpnet-base-v2 consistently achieves superior performance across various retrieval scenarios, with the authors suggesting future research on domain adaptation and zero-shot transfer learning for embedding models.\n\n#v(1em)\nCui et al. (2019) developed  DataShot  system for automatic visualization generation from natural language queries using neural sequence-to-sequence models. They implemented attention-based encoder-decoder architecture trained on natural language descriptions paired with visualization specifications. Results showed promising accuracy in generating appropriate chart types and data mappings, with the authors discussing future work on handling more complex multi-chart visualizations and interactive dashboard generation.\n\n#v(1em)\nAustin et al. (2021) created Codex for generating functional code from natural language descriptions using transformer-based language models trained on large code repositories. They fine-tuned GPT models specifically for Python code generation tasks including data visualization libraries like  matplotlib  and  seaborn . The system achieved high success rates in generating executable visualization code, with future enhancements focusing on debugging capabilities and multi-language code generation.\n\n#v(1em)\nJohnson et al. (2019) proposed FAISS ( Facebook  AI Similarity Search) library implementing optimized algorithms for dense vector similarity search at scale. They developed GPU-accelerated implementations of approximate nearest neighbor search using product quantization and hierarchical clustering approaches. Performance evaluations demonstrated sub-linear search complexity with minimal accuracy loss, with the authors planning future optimizations for distributed search across multiple machines and streaming vector updates.\n\n#v(1em)\nBudzianowski et al. (2018) developed  MultiWOZ  benchmark for task-oriented dialogue systems incorporating multi-domain conversations with complex dialogue state tracking requirements. They implemented neural baseline models using hierarchical encoder-decoder architectures with attention mechanisms for handling multi-turn conversations. The benchmark enabled standardized evaluation of dialogue systems, with the authors suggesting future extensions to include more diverse domains and conversational scenarios.\n\n#v(1em)\nMalkov  and  Yashunin  (2018) introduced Hierarchical Navigable Small World (HNSW) graphs for  approximate nearest neighbor search in high-dimensional spaces. They implemented multi-layer graph construction with probabilistic layer assignment and greedy search algorithms for efficient navigation. Results showed superior performance compared to existing indexing methods on various datasets, with future research directions focusing on dynamic graph updates and memory-efficient implementations for large-scale applications.\n\n\n\n"
  },
  {
  "input": {
    "starting_page": 9,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- III\n\nSystem Requirement  Specification\n\n3.1 Hardware Requirements\n\nMinimum System Specifications\n\nCPU : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\nRAM : 16GB DDR4\n\nStorage : 100GB SSD\n\nGPU : Optional - NVIDIA GTX 1660 with CUDA support\n\nNetwork : Stable broadband internet connection\n\nRecommended System Specifications\n\nCPU : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\nRAM : 32GB DDR4\n\nStorage : 500GB  NVMe  SSD\n\nGPU : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\nNetwork : High-speed broadband (100+ Mbps)\n\nProduction Environment Specifications\n\nCPU : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\nRAM : 64GB+ DDR4 ECC memory\n\nStorage : 1TB+ enterprise SSD with RAID configuration\n\nGPU : NVIDIA A100 or V100 for enterprise-scale processing\n\nNetwork : Enterprise-grade infrastructure with load balancing\n\n3.2 Software Requirements\n\nOperating System\n\nPrimary : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\nSecondary : Windows 10/11 Professional or Windows Server 2019/2022\n\nAlternative :  macOS  11+ (development environments only)\n\nCore Runtime Environment\n\nPython : Version 3.8+ (Recommended: Python 3.10)\n\nPackage Manager : pip 21.0+\n\nEnvironment Manager :  virtualenv  or  conda\n\nProcess Manager :  systemd  or supervisor (production)\n\nDatabase System\n\nPrimary Database :  PostgreSQL  13+\n\nVector Extension :  pgvector  0.4.0+\n\nOptional Extensions :  PostGIS  for spatial data\n\nConnection Pooling :  pgbouncer  (production environments)\n\nWeb Framework and Frontend\n\nWeb Framework :  Streamlit  1.28+\n\nUI Components : Built-in  Streamlit  widgets and components\n\nFile Upload : Support for files up to 1GB\n\nSession Management :  Streamlit  session state\n\nStyling : Custom CSS and avatar integration\n\nCore Python Libraries\n\nDocument Processing :  langchain  0.0.340,  langchain -community 0.0.12\n\nPDF Processing :  PyPDFLoader  for text extraction\n\nText Processing :  RecursiveCharacterTextSplitter  for chunking\n\nVector Embeddings : sentence-transformers 2.2.2, transformers 4.35.0\n\nDeep Learning : torch 2.1.0 (with CUDA support if available)\n\nDatabase Integration : psycopg2-binary 2.9.7\n\nData Processing : pandas 2.1.3,  numpy  1.24.3\n\nVisualization :  matplotlib  3.8.2 ( Agg  backend)\n\nUtilities : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\nAI Models and APIs\n\nLanguage Model API\n\nModel :  OpenAI  GPT-4o-mini\n\nEmbedding Model\n\nModel : all-mpnet-base-v2 ( SentenceTransformers )\n\nModel Size : 420MB download\n\nVector Dimensions : 768\n\nContext Window : 384 tokens\n\nBatch Size : 32 texts per batch\n\nVector Database Configuration\n\nSimilarity Metric : Cosine distance\n\nSimilarity Threshold : 0.8\n\nSearch Algorithm : Approximate nearest neighbor\n\nFallback Strategy : Broader search without threshold\n\nEnvironment Configuration\n\nRequired Environment Variables\n\nOPENAI_API_KEY :  OpenAI  API authentication key\n\nDB_HOST : Database server hostname (default:  localhost )\n\nDB_PORT : Database port (default: 5432)\n\nDB_NAME : Database name (default:  pdf_chatbot )\n\nDB_USER : Database username (default:  postgres )\n\nDB_PASSWORD : Database password\n\nMAX_FILE_SIZE : Maximum upload size (default: 1GB)\n\nSecurity Requirements\n\nFile Validation : SHA-256 hashing for  deduplication\n\nInput Sanitization : Parameterized database queries\n\nAPI Security : Environment variable management\n\nCode Execution : Sandboxed Python execution environment"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 9\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*3.1 Hardware Requirement*\n\n*Minimum System Specifications*\n\n   *CPU* : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\n   *RAM* : 16GB DDR4\n\n   *Storage* : 100GB SSD\n  \n   *GPU* : Optional - NVIDIA GTX 1660 with CUDA support\n\n   *Network* : Stable broadband internet connection\n\n  #v(1em)\n  *Recommended System Specifications*\n\n   *CPU* : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\n   *RAM* : 32GB DDR4\n\n   *Storage* : 500GB  NVMe  SSD\n\n   *GPU* : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\n   *Network* : High-speed broadband (100+ Mbps)\n\n  #v(1em)\n*Production Environment Specifications*\n\n   *CPU* : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\n   *RAM* : 64GB+ DDR4 ECC memory\n\n   *Storage* : 1TB+ enterprise SSD with RAID configuration\n\n   *GPU* : NVIDIA A100 or V100 for enterprise-scale processing\n\n   *Network* : Enterprise-grade infrastructure with load balancing\n\n#pagebreak()\n*3.2 Software Requirements*\n\n*Operating System*\n\n *Primary* : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\n *Secondary* : Windows 10/11 Professional or Windows Server 2019/2022\n\n *Alternative* :  macOS  11+ (development environments only)\n\n#v(1em)\n*Core Runtime Environment*\n\n *Python* : Version 3.8+ (Recommended: Python 3.10)\n\n *Package Manager* : pip 21.0+\n\n *Environment Manager* :  virtualenv  or  conda\n\n *Process Manager* :  systemd  or supervisor (production)\n\n#v(1em)\n*Database System*\n\n *Primary Database* :  PostgreSQL  13+\n\n *Vector Extension* :  pgvector  0.4.0+\n\n *Optional Extensions* :  PostGIS  for spatial data\n\n *Connection Pooling* :  pgbouncer  (production environments)\n\n#v(1em)\n*Web Framework and Frontend*\n\n *Web Framework* :  Streamlit  1.28+\n\n *UI Components* : Built-in  Streamlit  widgets and components\n\n *File Upload* : Support for files up to 1GB\n\n *Session Management* :  Streamlit  session state\n\n *Styling* : Custom CSS and avatar integration\n\n#pagebreak()\n*Core Python Libraries*\n\n *Document Processing* :  langchain  0.0.340,  langchain -community 0.0.12\n\n *PDF Processing* :  PyPDFLoader  for text extraction\n\n *Text Processing* :  RecursiveCharacterTextSplitter  for chunking\n\n *Vector Embeddings* : sentence-transformers 2.2.2, transformers 4.35.0\n\n *Deep Learning* : torch 2.1.0 (with CUDA support if available)\n\n *Database Integration* : psycopg2-binary 2.9.7\n\n *Data Processing* : pandas 2.1.3,  numpy  1.24.3\n\n *Visualization* :  matplotlib  3.8.2 ( Agg  backend)\n\n *Utilities* : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\n#v(1em)\n*AI Models and APIs*\n\n Language Model API\n\n *Model* :  OpenAI  GPT-4o-mini\n\n Embedding Model\n\n *Model* : all-mpnet-base-v2 ( SentenceTransformers )\n\n *Model Size* : 420MB download\n\n *Vector Dimensions* : 768\n\n *Context Window* : 384 tokens\n\n *Batch Size* : 32 texts per batch\n\n#v(1em)\n*Vector Database Configuration*\n\n *Similarity Metric* : Cosine distance\n\n *Similarity Threshold* : 0.8\n\n *Search Algorithm* : Approximate nearest neighbor\n\n *Fallback Strategy* : Broader search without threshold\n\n#v(1em)\n*Environment Configuration*\n\n*Required Environment Variables*\n\n *OPENAI_API_KEY* :  OpenAI  API authentication key\n\n *DB_HOST* : Database server hostname (default:  localhost )\n\n *DB_PORT* : Database port (default: 5432)\n\n *DB_NAME* : Database name (default:  pdf_chatbot )\n\n *DB_USER* : Database username (default:  postgres )\n\n *DB_PASSWORD* : Database password\n\n *MAX_FILE_SIZE* : Maximum upload size (default: 1GB)\n\n#v(1em)\n*Security Requirements*\n\n *File Validation* : SHA-256 hashing for  deduplication\n\n *Input Sanitization* : Parameterized database queries\n\n *API Security* : Environment variable management\n\n *Code Execution* : Sandboxed Python execution environment\n"
  },
  {
  "input": {
    "starting_page": 29,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- III\n\nSystem Requirement  Specification\n\n3.1 Software Requirements\n\nOperating System\n\nPrimary : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\nSecondary : Windows 10/11 Professional or Windows Server 2019/2022\n\nAlternative :  macOS  11+ (development environments only)\n\nCore Runtime Environment\n\nPython : Version 3.8+ (Recommended: Python 3.10)\n\nPackage Manager : pip 21.0+\n\nEnvironment Manager :  virtualenv  or  conda\n\nProcess Manager :  systemd  or supervisor (production)\n\nDatabase System\n\nPrimary Database :  PostgreSQL  13+\n\nVector Extension :  pgvector  0.4.0+\n\nOptional Extensions :  PostGIS  for spatial data\n\nConnection Pooling :  pgbouncer  (production environments)\n\nWeb Framework and Frontend\n\nWeb Framework :  Streamlit  1.28+\n\nUI Components : Built-in  Streamlit  widgets and components\n\nFile Upload : Support for files up to 1GB\n\nSession Management :  Streamlit  session state\n\nStyling : Custom CSS and avatar integration\n\nCore Python Libraries\n\nDocument Processing :  langchain  0.0.340,  langchain -community 0.0.12\n\nPDF Processing :  PyPDFLoader  for text extraction\n\nText Processing :  RecursiveCharacterTextSplitter  for chunking\n\nVector Embeddings : sentence-transformers 2.2.2, transformers 4.35.0\n\nDeep Learning : torch 2.1.0 (with CUDA support if available)\n\nDatabase Integration : psycopg2-binary 2.9.7\n\nData Processing : pandas 2.1.3,  numpy  1.24.3\n\nVisualization :  matplotlib  3.8.2 ( Agg  backend)\n\nUtilities : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\nAI Models and APIs\n\nLanguage Model API\n\nModel :  OpenAI  GPT-4o-mini\n\nEmbedding Model\n\nModel : all-mpnet-base-v2 ( SentenceTransformers )\n\nModel Size : 420MB download\n\nVector Dimensions : 768\n\nContext Window : 384 tokens\n\nBatch Size : 32 texts per batch\n\nVector Database Configuration\n\nSimilarity Metric : Cosine distance\n\nSimilarity Threshold : 0.8\n\nSearch Algorithm : Approximate nearest neighbor\n\nFallback Strategy : Broader search without threshold\n\nEnvironment Configuration\n\nRequired Environment Variables\n\nOPENAI_API_KEY :  OpenAI  API authentication key\n\nDB_HOST : Database server hostname (default:  localhost )\n\nDB_PORT : Database port (default: 5432)\n\nDB_NAME : Database name (default:  pdf_chatbot )\n\nDB_USER : Database username (default:  postgres )\n\nDB_PASSWORD : Database password\n\nMAX_FILE_SIZE : Maximum upload size (default: 1GB)\n\nSecurity Requirements\n\nFile Validation : SHA-256 hashing for  deduplication\n\nInput Sanitization : Parameterized database queries\n\nAPI Security : Environment variable management\n\nCode Execution : Sandboxed Python execution environment\n\n3.2 Hardware Requirements\n\nMinimum System Specifications\n\nCPU : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\nRAM : 16GB DDR4\n\nStorage : 100GB SSD\n\nGPU : Optional - NVIDIA GTX 1660 with CUDA support\n\nNetwork : Stable broadband internet connection\n\nRecommended System Specifications\n\nCPU : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\nRAM : 32GB DDR4\n\nStorage : 500GB  NVMe  SSD\n\nGPU : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\nNetwork : High-speed broadband (100+ Mbps)\n\nProduction Environment Specifications\n\nCPU : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\nRAM : 64GB+ DDR4 ECC memory\n\nStorage : 1TB+ enterprise SSD with RAID configuration\n\nGPU : NVIDIA A100 or V100 for enterprise-scale processing\n\nNetwork : Enterprise-grade infrastructure with load balancing"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 29\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n\n*3.1 Software Requirements*\n\n*Operating System*\n\n *Primary* : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\n *Secondary* : Windows 10/11 Professional or Windows Server 2019/2022\n\n *Alternative* :  macOS  11+ (development environments only)\n\n#v(1em)\n*Core Runtime Environment*\n\n *Python* : Version 3.8+ (Recommended: Python 3.10)\n\n *Package Manager* : pip 21.0+\n\n *Environment Manager* :  virtualenv  or  conda\n\n *Process Manager* :  systemd  or supervisor (production)\n\n#v(1em)\n*Database System*\n\n *Primary Database* :  PostgreSQL  13+\n\n *Vector Extension* :  pgvector  0.4.0+\n\n *Optional Extensions* :  PostGIS  for spatial data\n\n *Connection Pooling* :  pgbouncer  (production environments)\n\n#v(1em)\n*Web Framework and Frontend*\n\n *Web Framework* :  Streamlit  1.28+\n\n *UI Components* : Built-in  Streamlit  widgets and components\n\n *File Upload* : Support for files up to 1GB\n\n *Session Management* :  Streamlit  session state\n\n *Styling* : Custom CSS and avatar integration\n\n#pagebreak()\n*Core Python Libraries*\n\n *Document Processing* :  langchain  0.0.340,  langchain -community 0.0.12\n\n *PDF Processing* :  PyPDFLoader  for text extraction\n\n *Text Processing* :  RecursiveCharacterTextSplitter  for chunking\n\n *Vector Embeddings* : sentence-transformers 2.2.2, transformers 4.35.0\n\n *Deep Learning* : torch 2.1.0 (with CUDA support if available)\n\n *Database Integration* : psycopg2-binary 2.9.7\n\n *Data Processing* : pandas 2.1.3,  numpy  1.24.3\n\n *Visualization* :  matplotlib  3.8.2 ( Agg  backend)\n\n *Utilities* : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\n#v(1em)\n*AI Models and APIs*\n\n Language Model API\n\n *Model* :  OpenAI  GPT-4o-mini\n\n Embedding Model\n\n *Model* : all-mpnet-base-v2 ( SentenceTransformers )\n\n *Model Size* : 420MB download\n\n *Vector Dimensions* : 768\n\n *Context Window* : 384 tokens\n\n *Batch Size* : 32 texts per batch\n\n#v(1em)\n*Vector Database Configuration*\n\n *Similarity Metric* : Cosine distance\n\n *Similarity Threshold* : 0.8\n\n *Search Algorithm* : Approximate nearest neighbor\n\n *Fallback Strategy* : Broader search without threshold\n\n#v(1em)\n*Environment Configuration*\n\n*Required Environment Variables*\n\n *OPENAI_API_KEY* :  OpenAI  API authentication key\n\n *DB_HOST* : Database server hostname (default:  localhost )\n\n *DB_PORT* : Database port (default: 5432)\n\n *DB_NAME* : Database name (default:  pdf_chatbot )\n\n *DB_USER* : Database username (default:  postgres )\n\n *DB_PASSWORD* : Database password\n\n *MAX_FILE_SIZE* : Maximum upload size (default: 1GB)\n\n#v(1em)\n*Security Requirements*\n\n *File Validation* : SHA-256 hashing for  deduplication\n\n *Input Sanitization* : Parameterized database queries\n\n *API Security* : Environment variable management\n\n *Code Execution* : Sandboxed Python execution environment\n\n#v(1em)\n*3.2 Hardware Requirement*\n\n*Minimum System Specifications*\n\n   *CPU* : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\n   *RAM* : 16GB DDR4\n\n   *Storage* : 100GB SSD\n  \n   *GPU* : Optional - NVIDIA GTX 1660 with CUDA support\n\n   *Network* : Stable broadband internet connection\n\n  #pagebreak()\n  *Recommended System Specifications*\n\n   *CPU* : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\n   *RAM* : 32GB DDR4\n\n   *Storage* : 500GB  NVMe  SSD\n\n   *GPU* : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\n   *Network* : High-speed broadband (100+ Mbps)\n\n  #v(1em)\n*Production Environment Specifications*\n\n   *CPU* : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\n   *RAM* : 64GB+ DDR4 ECC memory\n\n   *Storage* : 1TB+ enterprise SSD with RAID configuration\n\n   *GPU* : NVIDIA A100 or V100 for enterprise-scale processing\n\n   *Network* : Enterprise-grade infrastructure with load balancing"
  },
  {
  "input": {
    "starting_page": 17,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- III\n\nSystem Requirement  Specification\n\n3.1 Software Requirements\n\nDatabase System\n\nPrimary Database :  PostgreSQL  13+\n\nVector Extension :  pgvector  0.4.0+\n\nOptional Extensions :  PostGIS  for spatial data\n\nConnection Pooling :  pgbouncer  (production environments)\n\nWeb Framework and Frontend\n\nWeb Framework :  Streamlit  1.28+\n\nUI Components : Built-in  Streamlit  widgets and components\n\nFile Upload : Support for files up to 1GB\n\nSession Management :  Streamlit  session state\n\nStyling : Custom CSS and avatar integration\n\nOperating System\n\nPrimary : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\nSecondary : Windows 10/11 Professional or Windows Server 2019/2022\n\nAlternative :  macOS  11+ (development environments only)\n\nCore Runtime Environment\n\nPython : Version 3.8+ (Recommended: Python 3.10)\n\nPackage Manager : pip 21.0+\n\nEnvironment Manager :  virtualenv  or  conda\n\nProcess Manager :  systemd  or supervisor (production)\n\nCore Python Libraries\n\nDocument Processing :  langchain  0.0.340,  langchain -community 0.0.12\n\nPDF Processing :  PyPDFLoader  for text extraction\n\nText Processing :  RecursiveCharacterTextSplitter  for chunking\n\nVector Embeddings : sentence-transformers 2.2.2, transformers 4.35.0\n\nDeep Learning : torch 2.1.0 (with CUDA support if available)\n\nDatabase Integration : psycopg2-binary 2.9.7\n\nData Processing : pandas 2.1.3,  numpy  1.24.3\n\nVisualization :  matplotlib  3.8.2 ( Agg  backend)\n\nUtilities : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\nAI Models and APIs\n\nLanguage Model API\n\nModel :  OpenAI  GPT-4o-mini\n\nEmbedding Model\n\nModel : all-mpnet-base-v2 ( SentenceTransformers )\n\nModel Size : 420MB download\n\nVector Dimensions : 768\n\nContext Window : 384 tokens\n\nBatch Size : 32 texts per batch\n\nVector Database Configuration\n\nSimilarity Metric : Cosine distance\n\nSimilarity Threshold : 0.8\n\nSearch Algorithm : Approximate nearest neighbor\n\nFallback Strategy : Broader search without threshold\n\nEnvironment Configuration\n\nRequired Environment Variables\n\nOPENAI_API_KEY :  OpenAI  API authentication key\n\nDB_HOST : Database server hostname (default:  localhost )\n\nDB_PORT : Database port (default: 5432)\n\nDB_NAME : Database name (default:  pdf_chatbot )\n\nDB_USER : Database username (default:  postgres )\n\nDB_PASSWORD : Database password\n\nMAX_FILE_SIZE : Maximum upload size (default: 1GB)\n\nSecurity Requirements\n\nFile Validation : SHA-256 hashing for  deduplication\n\nInput Sanitization : Parameterized database queries\n\nAPI Security : Environment variable management\n\nCode Execution : Sandboxed Python execution environment\n\n3.2 Hardware Requirements\n\nMinimum System Specifications\n\nCPU : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\nRAM : 16GB DDR4\n\nStorage : 100GB SSD\n\nGPU : Optional - NVIDIA GTX 1660 with CUDA support\n\nNetwork : Stable broadband internet connection\n\nRecommended System Specifications\n\nCPU : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\nRAM : 32GB DDR4\n\nStorage : 500GB  NVMe  SSD\n\nGPU : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\nNetwork : High-speed broadband (100+ Mbps)\n\nProduction Environment Specifications\n\nCPU : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\nRAM : 64GB+ DDR4 ECC memory\n\nStorage : 1TB+ enterprise SSD with RAID configuration\n\nGPU : NVIDIA A100 or V100 for enterprise-scale processing\n\nNetwork : Enterprise-grade infrastructure with load balancing"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 17\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n\n*3.1 Software Requirements*\n\n*Database System*\n\n *Primary Database* :  PostgreSQL  13+\n\n *Vector Extension* :  pgvector  0.4.0+\n\n *Optional Extensions* :  PostGIS  for spatial data\n\n *Connection Pooling* :  pgbouncer  (production environments)\n\n#v(1em)\n*Web Framework and Frontend*\n\n *Web Framework* :  Streamlit  1.28+\n\n *UI Components* : Built-in  Streamlit  widgets and components\n\n *File Upload* : Support for files up to 1GB\n\n *Session Management* :  Streamlit  session state\n\n *Styling* : Custom CSS and avatar integration\n\n#v(1em)\n\n*Operating System*\n\n *Primary* : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\n *Secondary* : Windows 10/11 Professional or Windows Server 2019/2022\n\n *Alternative* :  macOS  11+ (development environments only)\n\n#v(1em)\n*Core Runtime Environment*\n\n *Python* : Version 3.8+ (Recommended: Python 3.10)\n\n *Package Manager* : pip 21.0+\n\n *Environment Manager* :  virtualenv  or  conda\n\n *Process Manager* :  systemd  or supervisor (production)\n\n#v(1em)\n*Core Python Libraries*\n\n *Document Processing* :  langchain  0.0.340,  langchain -community 0.0.12\n\n *PDF Processing* :  PyPDFLoader  for text extraction\n\n *Text Processing* :  RecursiveCharacterTextSplitter  for chunking\n\n *Vector Embeddings* : sentence-transformers 2.2.2, transformers 4.35.0\n\n *Deep Learning* : torch 2.1.0 (with CUDA support if available)\n\n *Database Integration* : psycopg2-binary 2.9.7\n\n *Data Processing* : pandas 2.1.3,  numpy  1.24.3\n\n *Visualization* :  matplotlib  3.8.2 ( Agg  backend)\n\n *Utilities* : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\n#v(1em)\n*AI Models and APIs*\n\n Language Model API\n\n *Model* :  OpenAI  GPT-4o-mini\n\n Embedding Model\n\n *Model* : all-mpnet-base-v2 ( SentenceTransformers )\n\n *Model Size* : 420MB download\n\n *Vector Dimensions* : 768\n\n *Context Window* : 384 tokens\n\n *Batch Size* : 32 texts per batch\n\n#v(1em)\n*Environment Configuration*\n\n*Required Environment Variables*\n\n *OPENAI_API_KEY* :  OpenAI  API authentication key\n\n *DB_HOST* : Database server hostname (default:  localhost )\n\n *DB_PORT* : Database port (default: 5432)\n\n *DB_NAME* : Database name (default:  pdf_chatbot )\n\n *DB_USER* : Database username (default:  postgres )\n\n *DB_PASSWORD* : Database password\n\n *MAX_FILE_SIZE* : Maximum upload size (default: 1GB)\n\n#v(1em)\n*Security Requirements*\n\n *File Validation* : SHA-256 hashing for  deduplication\n\n *Input Sanitization* : Parameterized database queries\n\n *API Security* : Environment variable management\n\n *Code Execution* : Sandboxed Python execution environment\n\n#v(1em)\n*3.2 Hardware Requirement*\n\n*Minimum System Specifications*\n\n   *CPU* : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\n   *RAM* : 16GB DDR4\n\n   *Storage* : 100GB SSD\n  \n   *GPU* : Optional - NVIDIA GTX 1660 with CUDA support\n\n   *Network* : Stable broadband internet connection\n\n  #v(1em)\n  *Recommended System Specifications*\n\n   *CPU* : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\n   *RAM* : 32GB DDR4\n\n   *Storage* : 500GB  NVMe  SSD\n\n   *GPU* : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\n   *Network* : High-speed broadband (100+ Mbps)\n\n  #v(1em)\n*Production Environment Specifications*\n\n   *CPU* : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\n   *RAM* : 64GB+ DDR4 ECC memory\n\n   *Storage* : 1TB+ enterprise SSD with RAID configuration\n\n   *GPU* : NVIDIA A100 or V100 for enterprise-scale processing\n\n   *Network* : Enterprise-grade infrastructure with load balancing"
  },
  {
  "input": {
    "starting_page": 67,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- III\n\nSystem Requirement  Specification\n\n3.1 Hardware Requirements\n\nRecommended System Specifications\n\nCPU : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\nRAM : 32GB DDR4\n\nStorage : 500GB  NVMe  SSD\n\nGPU : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\nNetwork : High-speed broadband (100+ Mbps)\n\nProduction Environment Specifications\n\nCPU : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\nRAM : 64GB+ DDR4 ECC memory\n\nStorage : 1TB+ enterprise SSD with RAID configuration\n\nGPU : NVIDIA A100 or V100 for enterprise-scale processing\n\nNetwork : Enterprise-grade infrastructure with load balancing\n\nMinimum System Specifications\n\nCPU : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\nRAM : 16GB DDR4\n\nStorage : 100GB SSD\n\nGPU : Optional - NVIDIA GTX 1660 with CUDA support\n\nNetwork : Stable broadband internet connection\n\n3.2 Software Requirements\n\nDatabase System\n\nPrimary Database :  PostgreSQL  13+\n\nVector Extension :  pgvector  0.4.0+\n\nOptional Extensions :  PostGIS  for spatial data\n\nConnection Pooling :  pgbouncer  (production environments)\n\nWeb Framework and Frontend\n\nWeb Framework :  Streamlit  1.28+\n\nUI Components : Built-in  Streamlit  widgets and components\n\nFile Upload : Support for files up to 1GB\n\nSession Management :  Streamlit  session state\n\nStyling : Custom CSS and avatar integration\n\nOperating System\n\nPrimary : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\nSecondary : Windows 10/11 Professional or Windows Server 2019/2022\n\nAlternative :  macOS  11+ (development environments only)\n\nCore Runtime Environment\n\nPython : Version 3.8+ (Recommended: Python 3.10)\n\nPackage Manager : pip 21.0+\n\nEnvironment Manager :  virtualenv  or  conda\n\nProcess Manager :  systemd  or supervisor (production)\n\nAI Models and APIs\n\nLanguage Model API\n\nModel :  OpenAI  GPT-4o-mini\n\nEmbedding Model\n\nModel : all-mpnet-base-v2 ( SentenceTransformers )\n\nModel Size : 420MB download\n\nVector Dimensions : 768\n\nContext Window : 384 tokens\n\nBatch Size : 32 texts per batch\n\nVector Database Configuration\n\nSimilarity Metric : Cosine distance\n\nSimilarity Threshold : 0.8\n\nSearch Algorithm : Approximate nearest neighbor\n\nFallback Strategy : Broader search without threshold\n\nEnvironment Configuration\n\nRequired Environment Variables\n\nOPENAI_API_KEY :  OpenAI  API authentication key\n\nDB_HOST : Database server hostname (default:  localhost )\n\nDB_PORT : Database port (default: 5432)\n\nDB_NAME : Database name (default:  pdf_chatbot )\n\nDB_USER : Database username (default:  postgres )\n\nDB_PASSWORD : Database password\n\nMAX_FILE_SIZE : Maximum upload size (default: 1GB)\n\nSecurity Requirements\n\nFile Validation : SHA-256 hashing for  deduplication\n\nInput Sanitization : Parameterized database queries\n\nAPI Security : Environment variable management\n\nCode Execution : Sandboxed Python execution environment\n\nCore Python Libraries\n\nDocument Processing :  langchain  0.0.340,  langchain -community 0.0.12\n\nPDF Processing :  PyPDFLoader  for text extraction\n\nText Processing :  RecursiveCharacterTextSplitter  for chunking\n\nVector Embeddings : sentence-transformers 2.2.2, transformers 4.35.0\n\nDeep Learning : torch 2.1.0 (with CUDA support if available)\n\nDatabase Integration : psycopg2-binary 2.9.7\n\nData Processing : pandas 2.1.3,  numpy  1.24.3\n\nVisualization :  matplotlib  3.8.2 ( Agg  backend)\n\nUtilities : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 67\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*3.1 Hardware Requirement*\n\n  *Recommended System Specifications*\n\n   *CPU* : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\n   *RAM* : 32GB DDR4\n\n   *Storage* : 500GB  NVMe  SSD\n\n   *GPU* : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\n   *Network* : High-speed broadband (100+ Mbps)\n\n  #v(1em)\n*Production Environment Specifications*\n\n   *CPU* : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\n   *RAM* : 64GB+ DDR4 ECC memory\n\n   *Storage* : 1TB+ enterprise SSD with RAID configuration\n\n   *GPU* : NVIDIA A100 or V100 for enterprise-scale processing\n\n   *Network* : Enterprise-grade infrastructure with load balancing\n\n  #v(1em)\n  *Minimum System Specifications*\n\n   *CPU* : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\n   *RAM* : 16GB DDR4\n\n   *Storage* : 100GB SSD\n  \n   *GPU* : Optional - NVIDIA GTX 1660 with CUDA support\n\n   *Network* : Stable broadband internet connection\n\n#pagebreak()\n*3.2 Software Requirements*\n\n*Database System*\n\n *Primary Database* :  PostgreSQL  13+\n\n *Vector Extension* :  pgvector  0.4.0+\n\n *Optional Extensions* :  PostGIS  for spatial data\n\n *Connection Pooling* :  pgbouncer  (production environments)\n\n#v(1em)\n*Web Framework and Frontend*\n\n *Web Framework* :  Streamlit  1.28+\n\n *UI Components* : Built-in  Streamlit  widgets and components\n\n *File Upload* : Support for files up to 1GB\n\n *Session Management* :  Streamlit  session state\n\n *Styling* : Custom CSS and avatar integration\n\n#v(1em)\n\n*Operating System*\n\n *Primary* : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\n *Secondary* : Windows 10/11 Professional or Windows Server 2019/2022\n\n *Alternative* :  macOS  11+ (development environments only)\n\n#v(1em)\n*Core Runtime Environment*\n\n *Python* : Version 3.8+ (Recommended: Python 3.10)\n\n *Package Manager* : pip 21.0+\n\n *Environment Manager* :  virtualenv  or  conda\n\n *Process Manager* :  systemd  or supervisor (production)\n\n#pagebreak()\n*AI Models and APIs*\n\n Language Model API\n\n *Model* :  OpenAI  GPT-4o-mini\n\n Embedding Model\n\n *Model* : all-mpnet-base-v2 ( SentenceTransformers )\n\n *Model Size* : 420MB download\n\n *Vector Dimensions* : 768\n\n *Context Window* : 384 tokens\n\n *Batch Size* : 32 texts per batch\n\n#v(1em)\n*Environment Configuration*\n\n*Required Environment Variables*\n\n *OPENAI_API_KEY* :  OpenAI  API authentication key\n\n *DB_HOST* : Database server hostname (default:  localhost )\n\n *DB_PORT* : Database port (default: 5432)\n\n *DB_NAME* : Database name (default:  pdf_chatbot )\n\n *DB_USER* : Database username (default:  postgres )\n\n *DB_PASSWORD* : Database password\n\n *MAX_FILE_SIZE* : Maximum upload size (default: 1GB)\n\n#v(1em)\n*Security Requirements*\n\n *File Validation* : SHA-256 hashing for  deduplication\n\n *Input Sanitization* : Parameterized database queries\n\n *API Security* : Environment variable management\n\n *Code Execution* : Sandboxed Python execution environment\n\n#v(1em)\n*Core Python Libraries*\n\n *Document Processing* :  langchain  0.0.340,  langchain -community 0.0.12\n\n *PDF Processing* :  PyPDFLoader  for text extraction\n\n *Text Processing* :  RecursiveCharacterTextSplitter  for chunking\n\n *Vector Embeddings* : sentence-transformers 2.2.2, transformers 4.35.0\n\n *Deep Learning* : torch 2.1.0 (with CUDA support if available)\n\n *Database Integration* : psycopg2-binary 2.9.7\n\n *Data Processing* : pandas 2.1.3,  numpy  1.24.3\n\n *Visualization* :  matplotlib  3.8.2 ( Agg  backend)\n\n *Utilities* : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re"
  },
  {
  "input": {
    "starting_page": 9,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- III\n\nSystem Requirement  Specification\n\n3.1 Software Requirements\n\nAI Models and APIs\n\nLanguage Model API\n\nModel :  OpenAI  GPT-4o-mini\n\nEmbedding Model\n\nModel : all-mpnet-base-v2 ( SentenceTransformers )\n\nModel Size : 420MB download\n\nVector Dimensions : 768\n\nContext Window : 384 tokens\n\nBatch Size : 32 texts per batch\n\nVector Database Configuration\n\nSimilarity Metric : Cosine distance\n\nSimilarity Threshold : 0.8\n\nSearch Algorithm : Approximate nearest neighbor\n\nFallback Strategy : Broader search without threshold\n\nOperating System\n\nPrimary : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\nSecondary : Windows 10/11 Professional or Windows Server 2019/2022\n\nAlternative :  macOS  11+ (development environments only)\n\nCore Runtime Environment\n\nPython : Version 3.8+ (Recommended: Python 3.10)\n\nPackage Manager : pip 21.0+\n\nEnvironment Manager :  virtualenv  or  conda\n\nProcess Manager :  systemd  or supervisor (production)\n\nWeb Framework and Frontend\n\nWeb Framework :  Streamlit  1.28+\n\nUI Components : Built-in  Streamlit  widgets and components\n\nFile Upload : Support for files up to 1GB\n\nSession Management :  Streamlit  session state\n\nStyling : Custom CSS and avatar integration\n\nDatabase System\n\nPrimary Database :  PostgreSQL  13+\n\nVector Extension :  pgvector  0.4.0+\n\nOptional Extensions :  PostGIS  for spatial data\n\nConnection Pooling :  pgbouncer  (production environments)\n\nCore Python Libraries\n\nDocument Processing :  langchain  0.0.340,  langchain -community 0.0.12\n\nPDF Processing :  PyPDFLoader  for text extraction\n\nText Processing :  RecursiveCharacterTextSplitter  for chunking\n\nVector Embeddings : sentence-transformers 2.2.2, transformers 4.35.0\n\nDeep Learning : torch 2.1.0 (with CUDA support if available)\n\nDatabase Integration : psycopg2-binary 2.9.7\n\nData Processing : pandas 2.1.3,  numpy  1.24.3\n\nVisualization :  matplotlib  3.8.2 ( Agg  backend)\n\nUtilities : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\nEnvironment Configuration\n\nRequired Environment Variables\n\nOPENAI_API_KEY :  OpenAI  API authentication key\n\nDB_HOST : Database server hostname (default:  localhost )\n\nDB_PORT : Database port (default: 5432)\n\nDB_NAME : Database name (default:  pdf_chatbot )\n\nDB_USER : Database username (default:  postgres )\n\nDB_PASSWORD : Database password\n\nMAX_FILE_SIZE : Maximum upload size (default: 1GB)\n\nSecurity Requirements\n\nFile Validation : SHA-256 hashing for  deduplication\n\nInput Sanitization : Parameterized database queries\n\nAPI Security : Environment variable management\n\nCode Execution : Sandboxed Python execution environment\n\n3.2 Hardware Requirements\n\nMinimum System Specifications\n\nCPU : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\nRAM : 16GB DDR4\n\nStorage : 100GB SSD\n\nGPU : Optional - NVIDIA GTX 1660 with CUDA support\n\nNetwork : Stable broadband internet connection\n\nRecommended System Specifications\n\nCPU : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\nRAM : 32GB DDR4\n\nStorage : 500GB  NVMe  SSD\n\nGPU : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\nNetwork : High-speed broadband (100+ Mbps)\n\nProduction Environment Specifications\n\nCPU : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\nRAM : 64GB+ DDR4 ECC memory\n\nStorage : 1TB+ enterprise SSD with RAID configuration\n\nGPU : NVIDIA A100 or V100 for enterprise-scale processing\n\nNetwork : Enterprise-grade infrastructure with load balancing"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 9\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n\n*3.1 Software Requirements*\n\n*AI Models and APIs*\n\n Language Model API\n\n *Model* :  OpenAI  GPT-4o-mini\n\n Embedding Model\n\n *Model* : all-mpnet-base-v2 ( SentenceTransformers )\n\n *Model Size* : 420MB download\n\n *Vector Dimensions* : 768\n\n *Context Window* : 384 tokens\n\n *Batch Size* : 32 texts per batch\n\n Vector Database Configuration\n\n *Similarity Metric* : Cosine distance\n\n *Similarity Threshold* : 0.8\n\n *Search Algorithm* : Approximate nearest neighbor\n\n *Fallback Strategy* : Broader search without threshold\n\n#v(1em)\n*Operating System*\n\n *Primary* : Linux ( Ubuntu  20.04+ LTS,  CentOS  8+, RHEL 8+)\n\n *Secondary* : Windows 10/11 Professional or Windows Server 2019/2022\n\n *Alternative* :  macOS  11+ (development environments only)\n\n#pagebreak()\n*Core Runtime Environment*\n\n *Python* : Version 3.8+ (Recommended: Python 3.10)\n\n *Package Manager* : pip 21.0+\n\n *Environment Manager* :  virtualenv  or  conda\n\n *Process Manager* :  systemd  or supervisor (production)\n\n#v(1em)\n*Web Framework and Frontend*\n\n *Web Framework* :  Streamlit  1.28+\n\n *UI Components* : Built-in  Streamlit  widgets and components\n\n *File Upload* : Support for files up to 1GB\n\n *Session Management* :  Streamlit  session state\n\n *Styling* : Custom CSS and avatar integration\n\n#v(1em)\n*Database System*\n\n *Primary Database* :  PostgreSQL  13+\n\n *Vector Extension* :  pgvector  0.4.0+\n\n *Optional Extensions* :  PostGIS  for spatial data\n\n *Connection Pooling* :  pgbouncer  (production environments)\n\n#v(1em)\n*Core Python Libraries*\n\n *Document Processing* :  langchain  0.0.340,  langchain -community 0.0.12\n\n *PDF Processing* :  PyPDFLoader  for text extraction\n\n *Text Processing* :  RecursiveCharacterTextSplitter  for chunking\n\n *Vector Embeddings* : sentence-transformers 2.2.2, transformers 4.35.0\n\n *Deep Learning* : torch 2.1.0 (with CUDA support if available)\n\n *Database Integration* : psycopg2-binary 2.9.7\n\n *Data Processing* : pandas 2.1.3,  numpy  1.24.3\n\n *Visualization* :  matplotlib  3.8.2 ( Agg  backend)\n\n *Utilities* : python- dotenv  1.0.0,  hashlib ,  tempfile ,  json , re\n\n#v(1em)\n*Environment Configuration*\n\n*Required Environment Variables*\n\n *OPENAI_API_KEY* :  OpenAI  API authentication key\n\n *DB_HOST* : Database server hostname (default:  localhost )\n\n *DB_PORT* : Database port (default: 5432)\n\n *DB_NAME* : Database name (default:  pdf_chatbot )\n\n *DB_USER* : Database username (default:  postgres )\n\n *DB_PASSWORD* : Database password\n\n *MAX_FILE_SIZE* : Maximum upload size (default: 1GB)\n\n#v(1em)\n*Security Requirements*\n\n *File Validation* : SHA-256 hashing for  deduplication\n\n *Input Sanitization* : Parameterized database queries\n\n *API Security* : Environment variable management\n\n *Code Execution* : Sandboxed Python execution environment\n\n#v(1em)\n*3.2 Hardware Requirement*\n\n*Minimum System Specifications*\n\n   *CPU* : Intel Core i5-8400 or AMD  Ryzen  5 2600 (6 cores, 3.0GHz)\n\n   *RAM* : 16GB DDR4\n\n   *Storage* : 100GB SSD\n  \n   *GPU* : Optional - NVIDIA GTX 1660 with CUDA support\n\n   *Network* : Stable broadband internet connection\n\n  #v(1em)\n  *Recommended System Specifications*\n\n   *CPU* : Intel Core i7-10700K or AMD  Ryzen  7 3700X (8+ cores, 3.8GHz+)\n\n   *RAM* : 32GB DDR4\n\n   *Storage* : 500GB  NVMe  SSD\n\n   *GPU* : NVIDIA RTX 3060 or better (8GB+ VRAM)\n\n   *Network* : High-speed broadband (100+ Mbps)\n\n  #v(1em)\n*Production Environment Specifications*\n\n   *CPU* : Intel Xeon or AMD EPYC server-grade processors (16+ cores)\n\n   *RAM* : 64GB+ DDR4 ECC memory\n\n   *Storage* : 1TB+ enterprise SSD with RAID configuration\n\n   *GPU* : NVIDIA A100 or V100 for enterprise-scale processing\n\n   *Network* : Enterprise-grade infrastructure with load balancing"
  },
  {
  "input": {
    "starting_page": 12,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- IV\n\nMETHODOLOGY AND  IMPLEMENTATION\n\n4.1 Methodology\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\nOverview on RAG Technology\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\n\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\nSystem Monitoring and Management\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\nSystem Design and Workflow\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\n4.2 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig No. 4.1 Flowchart]\n)\n\n4.3 Implementation\n\nSystem Prompt Engineering for PDF  Chatbot  with Advanced RAG\n\n1. Core System Identity and Role Definition\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\n\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n2. Multi-Document Context Handling\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n3. Response Type Classification and Routing\n\nRESPONSE TYPE DETERMINATION\n\n1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n\n- Extract relevant data  from  the PDF content  for  plotting\n\n- Create meaningful charts based on available information\n\n2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):\n\n- Extract  and  structure relevant information  from  PDF content\n\n- Create tables  with  meaningful data organization\n\n- Format data  in  a clear, readable structure\n\n3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):\n\n- Provide clear, comprehensive answers\n\n- Include insights  and  analysis of the PDF content\n\n- Reference specific document sources when applicable\n\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n4. Structured JSON Output Specification\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n# Complete executable visualization code based on PDF data\\\\n# Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n# Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n# Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n5. Format Enforcement Rules\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n\n- NO markdown blocks (```\n\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n\n- Line breaks: \\\\n, Quotes: \\\\\"\n\n- Multiple images/tables allowed  and  encouraged\n\n- \" img \" type: complete  runnable  Python visualization code\n\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n\n- Be creative  in  extracting  and  presenting information  from  the PDF content\n\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n6. Context Injection and Query Processing\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\nImplementation Code Snippets\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n# Conditional prompt modification based on document mode\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\n# Construct complete system prompt with dynamic components\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\n\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \" img \":\n\nimage_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \"table\":\n\ntable_ parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n# Create a local environment for code execution\n\nlocal_env  = {\n\n' plt ':  plt ,\n\n' pd ': pd,\n\n' np ':  np ,\n\n' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"__ builtins __\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 12\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\n#v(1em)\n*Overview on RAG Technology*\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\n#v(1em)\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\n#v(1em)\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\n#v(1em)\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\n#v(1em)\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\n#v(1em)\n*System Monitoring and Management*\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\n#v(1em)\n*System Design and Workflow*\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\n#v(1em)\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\n#v(1em)\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"image001.png\", width: 70%),\n  caption: [Fig 4.1 Flowchart],\n  numbering: none\n)\n\n#text(size: 14pt, weight: \"bold\")[4.3 Implementation]\n\n*System Prompt Engineering for PDF  Chatbot  with Advanced RAG*\n\n*1. Core System Identity and Role Definition*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n*2. Multi-Document Context Handling*\n\nmulti_doc_instruction  = \"\"\"\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n*3. Response Type Classification and Routing*\n\n*RESPONSE TYPE DETERMINATION*\n\n*1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):*\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n- Extract relevant data  from  the PDF content  for  plotting\n- Create meaningful charts based on available information\n\n\n*2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):*\n\n- Extract  and  structure relevant information  from  PDF content\n- Create tables  with  meaningful data organization\n- Format data  in  a clear, readable structure\n\n\n*3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):*\n\n- Provide clear, comprehensive answers\n- Include insights  and  analysis of the PDF content\n- Reference specific document sources when applicable\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n*4. Structured JSON Output Specification*\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n Complete executable visualization code based on PDF data\\\\n Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n*5. Format Enforcement Rules*\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n- NO markdown blocks (``` ```)\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n- Line breaks: \\\\n, Quotes: \\\\\"\n- Multiple images/tables allowed  and  encouraged\n- \" img \" type: complete  runnable  Python visualization code\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n- Be creative  in  extracting  and  presenting information  from  the PDF content\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n*6. Context Injection and Query Processing*\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\n\n*Implementation Code Snippets*\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n*Conditional prompt modification based on document mode*\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\n\n#pagebreak()\n*Construct complete system prompt with dynamic components*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \" img \":\n\nimage_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \"table\":\n\ntable_parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n*Create a local environment for code execution*\n\nlocal_env  = {\n\n  ' plt ':  plt ,\n\n  ' pd ': pd,\n\n  ' np ':  np ,\n\n  ' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"_builtins_\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False"
  },
  {
  "input": {
    "starting_page": 20,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- IV\n\nMETHODOLOGY AND  IMPLEMENTATION\n\n4.1 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig No. 4.1 Flowchart]\n)\n\n4.2 Implementation\n\nSystem Prompt Engineering for PDF  Chatbot  with Advanced RAG\n\n1. Core System Identity and Role Definition\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\n\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n2. Multi-Document Context Handling\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n3. Response Type Classification and Routing\n\nRESPONSE TYPE DETERMINATION\n\n1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n\n- Extract relevant data  from  the PDF content  for  plotting\n\n- Create meaningful charts based on available information\n\n2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):\n\n- Extract  and  structure relevant information  from  PDF content\n\n- Create tables  with  meaningful data organization\n\n- Format data  in  a clear, readable structure\n\n3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):\n\n- Provide clear, comprehensive answers\n\n- Include insights  and  analysis of the PDF content\n\n- Reference specific document sources when applicable\n\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n4. Structured JSON Output Specification\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n# Complete executable visualization code based on PDF data\\\\n# Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n# Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n# Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n5. Format Enforcement Rules\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n\n- NO markdown blocks (```\n\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n\n- Line breaks: \\\\n, Quotes: \\\\\"\n\n- Multiple images/tables allowed  and  encouraged\n\n- \" img \" type: complete  runnable  Python visualization code\n\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n\n- Be creative  in  extracting  and  presenting information  from  the PDF content\n\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n6. Context Injection and Query Processing\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\nImplementation Code Snippets\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n# Conditional prompt modification based on document mode\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\n# Construct complete system prompt with dynamic components\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\n\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \" img \":\n\nimage_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \"table\":\n\ntable_ parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n# Create a local environment for code execution\n\nlocal_env  = {\n\n' plt ':  plt ,\n\n' pd ': pd,\n\n' np ':  np ,\n\n' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"__ builtins __\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False\n\n4.3 Methodology\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\nOverview on RAG Technology\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\n\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\nSystem Monitoring and Management\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\nSystem Design and Workflow\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 20\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Execution Flowchart]\n\n#figure(\n  image(\"image001.png\", width: 70%),\n  caption: [Fig 4.1 Flowchart],\n  numbering: none\n)\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[4.2 Implementation]\n\n*System Prompt Engineering for PDF  Chatbot  with Advanced RAG*\n\n*1. Core System Identity and Role Definition*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n*2. Multi-Document Context Handling*\n\nmulti_doc_instruction  = \"\"\"\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n*3. Response Type Classification and Routing*\n\n*RESPONSE TYPE DETERMINATION*\n\n*1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):*\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n- Extract relevant data  from  the PDF content  for  plotting\n- Create meaningful charts based on available information\n\n#pagebreak()\n*2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):*\n\n- Extract  and  structure relevant information  from  PDF content\n- Create tables  with  meaningful data organization\n- Format data  in  a clear, readable structure\n\n\n*3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):*\n\n- Provide clear, comprehensive answers\n- Include insights  and  analysis of the PDF content\n- Reference specific document sources when applicable\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n*4. Structured JSON Output Specification*\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n Complete executable visualization code based on PDF data\\\\n Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n*5. Format Enforcement Rules*\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n- NO markdown blocks (``` ```)\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n- Line breaks: \\\\n, Quotes: \\\\\"\n- Multiple images/tables allowed  and  encouraged\n- \" img \" type: complete  runnable  Python visualization code\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n- Be creative  in  extracting  and  presenting information  from  the PDF content\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n*6. Context Injection and Query Processing*\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\n\n*Implementation Code Snippets*\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n*Conditional prompt modification based on document mode*\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\n\n*Construct complete system prompt with dynamic components*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \" img \":\n\nimage_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \"table\":\n\ntable_parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n*Create a local environment for code execution*\n\nlocal_env  = {\n\n  ' plt ':  plt ,\n\n  ' pd ': pd,\n\n  ' np ':  np ,\n\n  ' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"_builtins_\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[4.3 Methodology]\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\n#v(1em)\n*Overview on RAG Technology*\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\n#v(1em)\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\n#v(1em)\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\n#v(1em)\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\n#v(1em)\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\n#v(1em)\n*System Monitoring and Management*\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\n#v(1em)\n*System Design and Workflow*\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\n#v(1em)\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\n#v(1em)\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions."
  },
  {
    "input": {
      "starting_page": 30,
      "academic_year": "2025-26",
      "department": "B.E/Dept of CSE/BNMIT",
      "project_title": "RAG Based PDF Editor",
      "contents_txt": "CHAPTER- IV\n\nMETHODOLOGY AND  IMPLEMENTATION\n\n4.1 Methodology\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\nOverview on RAG Technology\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\n\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\nSystem Monitoring and Management\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\nSystem Design and Workflow\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\n4.2 Implementation\n\nSystem Prompt Engineering for PDF  Chatbot  with Advanced RAG\n\n1. Core System Identity and Role Definition\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\n\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n2. Multi-Document Context Handling\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n3. Response Type Classification and Routing\n\nRESPONSE TYPE DETERMINATION\n\n1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n\n- Extract relevant data  from  the PDF content  for  plotting\n\n- Create meaningful charts based on available information\n\n2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):\n\n- Extract  and  structure relevant information  from  PDF content\n\n- Create tables  with  meaningful data organization\n\n- Format data  in  a clear, readable structure\n\n3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):\n\n- Provide clear, comprehensive answers\n\n- Include insights  and  analysis of the PDF content\n\n- Reference specific document sources when applicable\n\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n4. Structured JSON Output Specification\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n# Complete executable visualization code based on PDF data\\\\n# Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n# Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n# Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n5. Format Enforcement Rules\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n\n- NO markdown blocks (```\n\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n\n- Line breaks: \\\\n, Quotes: \\\\\"\n\n- Multiple images/tables allowed  and  encouraged\n\n- \" img \" type: complete  runnable  Python visualization code\n\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n\n- Be creative  in  extracting  and  presenting information  from  the PDF content\n\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n6. Context Injection and Query Processing\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\nImplementation Code Snippets\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n# Conditional prompt modification based on document mode\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\n# Construct complete system prompt with dynamic components\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\n\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \" img \":\n\nimage_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \"table\":\n\ntable_ parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n# Create a local environment for code execution\n\nlocal_env  = {\n\n' plt ':  plt ,\n\n' pd ': pd,\n\n' np ':  np ,\n\n' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"__ builtins __\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False\n\n4.3 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig No. 4.1 Flowchart]\n)"
    },
    "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 30\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\n#v(1em)\n*Overview on RAG Technology*\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\n#v(1em)\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\n#v(1em)\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\n#v(1em)\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\n#v(1em)\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\n#v(1em)\n*System Monitoring and Management*\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\n#v(1em)\n*System Design and Workflow*\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\n#v(1em)\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\n#v(1em)\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Implementation]\n\n*System Prompt Engineering for PDF  Chatbot  with Advanced RAG*\n\n*1. Core System Identity and Role Definition*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n#pagebreak()\n*2. Multi-Document Context Handling*\n\nmulti_doc_instruction  = \"\"\"\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n*3. Response Type Classification and Routing*\n\n*RESPONSE TYPE DETERMINATION*\n\n*1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):*\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n- Extract relevant data  from  the PDF content  for  plotting\n- Create meaningful charts based on available information\n\n\n*2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):*\n\n- Extract  and  structure relevant information  from  PDF content\n- Create tables  with  meaningful data organization\n- Format data  in  a clear, readable structure\n\n\n*3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):*\n\n- Provide clear, comprehensive answers\n- Include insights  and  analysis of the PDF content\n- Reference specific document sources when applicable\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n*4. Structured JSON Output Specification*\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n Complete executable visualization code based on PDF data\\\\n Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n#pagebreak()\n*5. Format Enforcement Rules*\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n- NO markdown blocks (``` ```)\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n- Line breaks: \\\\n, Quotes: \\\\\"\n- Multiple images/tables allowed  and  encouraged\n- \" img \" type: complete  runnable  Python visualization code\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n- Be creative  in  extracting  and  presenting information  from  the PDF content\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n*6. Context Injection and Query Processing*\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\n\n*Implementation Code Snippets*\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n*Conditional prompt modification based on document mode*\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\n\n#pagebreak()\n*Construct complete system prompt with dynamic components*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \" img \":\n\nimage_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \"table\":\n\ntable_parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n*Create a local environment for code execution*\n\nlocal_env  = {\n\n  ' plt ':  plt ,\n\n  ' pd ': pd,\n\n  ' np ':  np ,\n\n  ' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"_builtins_\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[4.3 Execution Flowchart]\n\n#figure(\n  image(\"image001.png\", width: 70%),\n  caption: [Fig 4.1 Flowchart],\n  numbering: none\n)"
  },
  {
    "input": {
      "starting_page": 52,
      "academic_year": "2025-26",
      "department": "B.E/Dept of CSE/BNMIT",
      "project_title": "RAG Based PDF Editor",
      "contents_txt": "CHAPTER- IV\n\nMETHODOLOGY AND  IMPLEMENTATION\n\n4.1 Methodology\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\nSystem Design and Workflow\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\nOverview on RAG Technology\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\n\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\nSystem Monitoring and Management\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\n4.2 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig No. 4.1 Flowchart]\n)\n\n4.3 Implementation\n\nSystem Prompt Engineering for PDF  Chatbot  with Advanced RAG\n\n1. Core System Identity and Role Definition\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\n\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n2. Multi-Document Context Handling\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n3. Response Type Classification and Routing\n\nRESPONSE TYPE DETERMINATION\n\n1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n\n- Extract relevant data  from  the PDF content  for  plotting\n\n- Create meaningful charts based on available information\n\n2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):\n\n- Extract  and  structure relevant information  from  PDF content\n\n- Create tables  with  meaningful data organization\n\n- Format data  in  a clear, readable structure\n\n3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):\n\n- Provide clear, comprehensive answers\n\n- Include insights  and  analysis of the PDF content\n\n- Reference specific document sources when applicable\n\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n4. Structured JSON Output Specification\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n# Complete executable visualization code based on PDF data\\\\n# Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n# Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n# Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n5. Format Enforcement Rules\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n\n- NO markdown blocks (```\n\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n\n- Line breaks: \\\\n, Quotes: \\\\\"\n\n- Multiple images/tables allowed  and  encouraged\n\n- \" img \" type: complete  runnable  Python visualization code\n\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n\n- Be creative  in  extracting  and  presenting information  from  the PDF content\n\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n6. Context Injection and Query Processing\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\nImplementation Code Snippets\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n# Conditional prompt modification based on document mode\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\n# Construct complete system prompt with dynamic components\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\n\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \" img \":\n\nimage_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \"table\":\n\ntable_ parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n# Create a local environment for code execution\n\nlocal_env  = {\n\n' plt ':  plt ,\n\n' pd ': pd,\n\n' np ':  np ,\n\n' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"__ builtins __\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False"
    },
    "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 52\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\n#v(1em)\n*System Design and Workflow*\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\n#v(1em)\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\n#v(1em)\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\n#v(1em)\n*Overview on RAG Technology*\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\n#v(1em)\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\n#v(1em)\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\n#v(1em)\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\n#v(1em)\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\n#v(1em)\n*System Monitoring and Management*\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"image001.png\", width: 70%),\n  caption: [Fig 4.1 Flowchart],\n  numbering: none\n)\n\n#text(size: 14pt, weight: \"bold\")[4.3 Implementation]\n\n*System Prompt Engineering for PDF  Chatbot  with Advanced RAG*\n\n*1. Core System Identity and Role Definition*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n*2. Multi-Document Context Handling*\n\nmulti_doc_instruction  = \"\"\"\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n*3. Response Type Classification and Routing*\n\n*RESPONSE TYPE DETERMINATION*\n\n*1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):*\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n- Extract relevant data  from  the PDF content  for  plotting\n- Create meaningful charts based on available information\n\n\n*2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):*\n\n- Extract  and  structure relevant information  from  PDF content\n- Create tables  with  meaningful data organization\n- Format data  in  a clear, readable structure\n\n\n*3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):*\n\n- Provide clear, comprehensive answers\n- Include insights  and  analysis of the PDF content\n- Reference specific document sources when applicable\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n*4. Structured JSON Output Specification*\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n Complete executable visualization code based on PDF data\\\\n Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n*5. Format Enforcement Rules*\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n- NO markdown blocks (``` ```)\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n- Line breaks: \\\\n, Quotes: \\\\\"\n- Multiple images/tables allowed  and  encouraged\n- \" img \" type: complete  runnable  Python visualization code\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n- Be creative  in  extracting  and  presenting information  from  the PDF content\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement .\n\n*6. Context Injection and Query Processing*\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\n\n*Implementation Code Snippets*\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n*Conditional prompt modification based on document mode*\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\n\n#pagebreak()\n*Construct complete system prompt with dynamic components*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \" img \":\n\nimage_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \"table\":\n\ntable_parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n*Create a local environment for code execution*\n\nlocal_env  = {\n\n  ' plt ':  plt ,\n\n  ' pd ': pd,\n\n  ' np ':  np ,\n\n  ' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"_builtins_\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False"
  },
  {
    "input": {
      "starting_page": 25,
      "academic_year": "2025-26",
      "department": "B.E/Dept of CSE/BNMIT",
      "project_title": "RAG Based PDF Editor",
      "contents_txt": "CHAPTER- IV\n\nMETHODOLOGY AND  IMPLEMENTATION\n\n4.1 Methodology\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\nOverview on RAG Technology\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\n\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\nSystem Monitoring and Management\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\nSystem Design and Workflow\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\n4.2 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig No. 4.1 Flowchart]\n)\n\n4.3 Implementation\n\nSystem Prompt Engineering for PDF  Chatbot  with Advanced RAG\n\n1. Multi-Document Context Handling\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n2. Core System Identity and Role Definition\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\n\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n3. Response Type Classification and Routing\n\nRESPONSE TYPE DETERMINATION\n\n1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n\n- Extract relevant data  from  the PDF content  for  plotting\n\n- Create meaningful charts based on available information\n\n2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):\n\n- Extract  and  structure relevant information  from  PDF content\n\n- Create tables  with  meaningful data organization\n\n- Format data  in  a clear, readable structure\n\n3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):\n\n- Provide clear, comprehensive answers\n\n- Include insights  and  analysis of the PDF content\n\n- Reference specific document sources when applicable\n\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n4. Structured JSON Output Specification\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n# Complete executable visualization code based on PDF data\\\\n# Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n# Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n# Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n5. Context Injection and Query Processing\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\nImplementation Code Snippets\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n# Conditional prompt modification based on document mode\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n\n- Mention which document(s) contain the relevant information\n\n- If information comes from different documents, clearly indicate the sources\n\n- Synthesize information from multiple sources when applicable\n\n- If there are contradictions between documents, mention them\n\n\"\"\"\n\n# Construct complete system prompt with dynamic components\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\n\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \" img \":\n\nimage_ parts.append ( item.get (\"content\", \"\"))\n\nelifitem.get ( \"type\") == \"table\":\n\ntable_ parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n# Create a local environment for code execution\n\nlocal_env  = {\n\n' plt ':  plt ,\n\n' pd ': pd,\n\n' np ':  np ,\n\n' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"__ builtins __\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False\n\n6. Format Enforcement Rules\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n\n- NO markdown blocks (```\n\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n\n- Line breaks: \\\\n, Quotes: \\\\\"\n\n- Multiple images/tables allowed  and  encouraged\n\n- \" img \" type: complete  runnable  Python visualization code\n\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n\n- Be creative  in  extracting  and  presenting information  from  the PDF content\n\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement ."
    },
    "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 25\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\nThe proposed PDF  chatbot  with advanced RAG capabilities consists of three major parts as shown in the system architecture. The first part is the document processing system. The hardware includes  PostgreSQL  database with  pgvector  extension,  SentenceTransformer  embedding models based on all-mpnet-base-v2 architecture, various types of text processing modules, storage systems, and processing units. The second part is the retrieval and generation service. The service is responsible for collecting incoming user queries through the web interface and processing the natural language requests. Moreover, the service is also responsible for retrieving relevant document chunks from the vector database and generating structured responses continuously. The third part of the system is the web application server built on  Streamlit  framework, to store document and conversation information, and a chat interface to interact with the PDF documents through a web browser.\n\n#v(1em)\n*Overview on RAG Technology*\n\nRAG connects document repositories through vector databases and enables intelligent information retrieval from remote document collections. A large amount of textual information in RAG systems is embedded into high-dimensional vector representations in the semantic space. Embedding models play a vital role in RAG technologies. Five RAG techniques are widely used in document intelligence products and services.\n\n#v(1em)\nDense Vector Embeddings It is used for semantic representation to capture contextual meaning of text chunks. A vector is associated with semantic field through which the similarity of any text segments are easily identified even from complex document structures.\n\n#v(1em)\nVector Similarity Search Systems with vector databases used to monitor semantic relationships and contextual conditions, when it is combined with cosine similarity metrics to better track the relevance of text chunks, their context, semantic meaning, and relationships.\n\n#v(1em)\nLanguage Model Integration Language model integration is a software layer through which developers perform communication between retrieval systems and generation models to provide contextual input and to receive structured output.\n\n#v(1em)\nStructured Response Generation The responses which are generated by language models need to get formatted and those responses have to be processed in order to generate meaningful visualizations and tables out of which better insights can be extracted for this purpose structured JSON formatting is used.\n\n#v(1em)\nMulti-Modal Output Generation In  RAG applications, system communicates with users through multiple output formats including text, tables, and visualizations. The communication should happen in such a manner that generation and rendering of content is strictly based on retrieved document context.\n\n#v(1em)\n*System Monitoring and Management*\n\nMonitoring software of the document management server is the most critical part in the PDF  chatbot  environment information processing system, which is used for dealing with user interactions and obtaining information from document repositories. The main function of monitoring software consists of document upload tracking, embedding generation  progress,  query processing, response analysis, conversation history management and status alerts for real-time operations, online document processing monitoring, real-time performance display, and remote system communication.\n\n#v(1em)\n*System Design and Workflow*\n\nInitially a document collection is established through PDF upload. The document processing module senses new file uploads through the web interface. After file validation and hash calculation, this document data is given to the text extraction controller. The controller converts it to structured text format using  PyPDFLoader  with intelligent chunking. If the document size is below processing threshold, the embedding unit will start generating vector representations. Once embeddings are complete, it will store them in the vector database. If the document size exceeds memory limits, it will start batch processing to handle large files efficiently. Once processing reaches completion, it will update the document status. In this way, document ingestion is controlled.\n\n#v(1em)\nSimilarly, query processing can be managed through the retrieval system. Initially a similarity threshold is set for vector search. The embedding model processes the user query into vector representation. If the similarity score is above the threshold limit set, the retrieval unit will return relevant chunks. Once sufficient context is gathered, it will proceed to response generation. In this way, retrieval quality is controlled.\n\n#v(1em)\nResponse generation can be managed using the language model integration. Initially response format requirements are set based on query type detection. The generation system processes retrieved context and  user intent. The structured output formatting module will ensure JSON compliance for multi-modal responses. A content validation system monitors the generated code for visualizations and tables. If the content passes validation checks, the execution unit renders the final output. Once the required response quality is achieved, the conversation state is updated. After processing text generation, visualization creation, and table formatting, it provides structured display output through the  Streamlit  interface, maintaining conversation history and enabling continued multi-turn interactions.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"image001.png\", width: 70%),\n  caption: [Fig 4.1 Flowchart],\n  numbering: none\n)\n\n#text(size: 14pt, weight: \"bold\")[4.3 Implementation]\n\n*System Prompt Engineering for PDF  Chatbot  with Advanced RAG*\n\n*1. Core System Identity and Role Definition*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\"\"\"\nPrompt Engineering Principle :  Role-based prompting  establishes clear identity and expectations. The system explicitly defines the AI as a \"document assistant\" rather than a general  chatbot , which constrains the model's behavior to document-focused tasks and improves response relevance.\n\n*2. Multi-Document Context Handling*\n\nmulti_doc_instruction  = \"\"\"\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\nPrompt Engineering Principle :  Conditional instructions  that adapt based on system state. This demonstrates  dynamic prompting  where the instruction set changes based on whether single or multiple documents are being processed, ensuring appropriate source attribution and conflict resolution.\n\n*3. Response Type Classification and Routing*\n\n*RESPONSE TYPE DETERMINATION*\n\n*1. VISUALIZATION GENERATION  (when user requests charts, graphs, plots,  or  visual analysis):*\n\n- Generate  matplotlib / seaborn  code  for  data visualization\n- Extract relevant data  from  the PDF content  for  plotting\n- Create meaningful charts based on available information\n\n\n*2. TABLE GENERATION  (when user requests tables, lists, summaries,  or  structured data):*\n\n- Extract  and  structure relevant information  from  PDF content\n- Create tables  with  meaningful data organization\n- Format data  in  a clear, readable structure\n\n\n*3. TEXT RESPONSES  ( for  explanations, insights,  and  general queries):*\n\n- Provide clear, comprehensive answers\n- Include insights  and  analysis of the PDF content\n- Reference specific document sources when applicable\nPrompt Engineering Principle :  Intent classification within prompts  that enables the model to self-route to appropriate response formats. This eliminates the need for external classification models by embedding the decision logic directly in the prompt.\n\n*4. Structured JSON Output Specification*\n\nMANDATORY JSON RESPONSE FORMAT (ABSOLUTE REQUIREMENT):\n\nENTIRE response must be valid JSON array - NO exceptions:\n\n[\n\n{\"type\": \"text\", \"content\": \"Detailed explanatory text, insights, context, assumptions, or business implications based on the PDF content\"},\n\n{\"type\": \" img \", \"content\": \"import  matplotlib.pyplot  as  plt \\\\ nimport  pandas as pd\\\\ nimportnumpy  as  np \\\\n\\\\n Complete executable visualization code based on PDF data\\\\n Extract relevant data from context and create meaningful visualizations\\\\ nplt.figure ( figsize =(12, 8))\\\\n Visualization implementation\\\\ nplt.title ('Chart Title Based on PDF Content')\\\\nplt.xlabel('X Label')\\\\nplt.ylabel('Y Label')\\\\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\\\nplt.tight_layout(pad=3.0)\\\\nplt.show()\"},\n\n{\"type\": \"table\", \"content\": \"import pandas as pd\\\\ nimportjson \\\\n\\\\n Extract and structure data from PDF content\\\\ ndata  = [\\\\n {'Item': 'Example 1', 'Value': 'Data from PDF'},\\\\n {'Item': 'Example 2', 'Value': 'More PDF data'}\\\\n]\\\\nresult_df =  pd.DataFrame (data)\\\\nprint(result_df.to_json(orient='records',  date_format =' iso '))\"},\n\n{\"type\": \"text\", \"content\": \"Additional insights, conclusions, or summary based on the PDF analysis\"}\n\n]\n\nPrompt Engineering Principle :  Strict output formatting  with  template-based generation . The prompt provides exact examples of expected output structure, ensuring consistent parsing and execution. This approach combines  few-shot prompting  (examples) with  constraint-based generation  (strict JSON format).\n\n*5. Context Injection and Query Processing*\n\ntext\n\nresponse  =  openai_client.chat.completions.create (\n\nmodel =\"gpt-4o-mini\",\n\nmessages =[\n\n{\"role\": \"system\", \"content\":  system_prompt },\n\n{\n\n\" role \": \"user\",\n\n\" content \": f\"\"\"Based on the following PDF document excerpts, please answer this question: \"{query}\"\n\nDocument Content:\n\nPlease provide a comprehensive response in the required JSON format. Extract relevant data from the PDF content to create meaningful visualizations or tables if requested.\"\"\"\n}\n\n],\n\ntemperature= 0.1,\n\nmax_tokens =2000\n\n)\n\nPrompt Engineering Principle :  Context-aware prompting  where retrieved document chunks are injected as context, followed by the specific user query. The low temperature (0.1) ensures consistent, factual responses rather than creative generation.\n\n*Implementation Code Snippets*\n\nPrompt Construction Logic\n\ntext\n\ndef   generate_response (query,  retrieved_chunks ,  is_multi_doc =False):\n\n\"\"\"Generate response using  OpenAI  API with retrieved chunks in JSON format.\"\"\"\n\n*Conditional prompt modification based on document mode*\n\nmulti_doc_instruction  = \"\"\n\nif   is_multi_doc :\n\nmulti_doc_instruction  = \"\"\"\n\nIMPORTANT: You are searching across MULTIPLE documents. When answering:\n- Mention which document(s) contain the relevant information\n- If information comes from different documents, clearly indicate the sources\n- Synthesize information from multiple sources when applicable\n- If there are contradictions between documents, mention them\n\"\"\"\n\n*Construct complete system prompt with dynamic components*\n\nsystem_prompt  = f\"\"\"You are an intelligent PDF document assistant. Your job is to analyze PDF content and provide helpful responses in a specific JSON format.\n\n{ multi_doc_instruction }\n\n[...  rest  of prompt structure ...]\n\n\"\"\"\nResponse Validation and Parsing\n\ntext\n\ndef   parse_json_response ( json_response ):\n\n\"\"\"Parse JSON response and separate different content types.\"\"\"\n\ntry :\n\nresponse_data  =  json.loads ( json_response )\n\ntext_parts  = []\n\nimage_parts  = []\n\ntable_parts  = []\n\nfor  item in  response_data :\n\nif   item.get (\"type\") == \"text\":\n\ntext_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \" img \":\n\nimage_parts.append ( item.get (\"content\", \"\"))\n\nelif item.get ( \"type\") == \"table\":\n\ntable_parts.append ( item.get (\"content\", \"\"))\n\nreturn   text_parts ,  image_parts ,  table_parts\n\nexcept   json.JSONDecodeError :\n\nreturn  [ json_response ], [], []\n\nDynamic Code Execution\n\ntext\n\ndef   execute_visualization_code (code):\n\n\"\"\"Execute visualization code and capture the plot.\"\"\"\n\ntry :\n\n*Create a local environment for code execution*\n\nlocal_env  = {\n\n  ' plt ':  plt ,\n\n  ' pd ': pd,\n\n  ' np ':  np ,\n\n  ' matplotlib ':  matplotlib\n\n}\n\nexec( code, {\"_builtins_\": {}},  local_env )\n\nreturn  True\n\nexcept  Exception as e:\n\nst.error ( f\"Error  executing visualization code: {e}\")\n\nreturn  False\n\n*6. Format Enforcement Rules*\n\nJSON FORMAT RULES (STRICTLY ENFORCED):\n\n- Valid JSON only - no text outside structure\n- NO markdown blocks (``` ```)\n- Every content piece requires JSON object  with  \"type\"  and  \"content\"\n- Line breaks: \\\\n, Quotes: \\\\\"\n- Multiple images/tables allowed  and  encouraged\n- \" img \" type: complete  runnable  Python visualization code\n- \"table\" type: code outputting JSON via  print ( result_df.to_json (orient='records',  date_format =' iso '))\n- \"text\" type: ONLY direct findings  and  business insights - NO code explanations\n- Extract actual data  from  PDF content  for  visualizations  and  tables\n- Be creative  in  extracting  and  presenting information  from  the PDF content\nPrompt Engineering Principle :  Explicit constraint specification  prevents common formatting errors and ensures executable output. This demonstrates  negative prompting  (what NOT to do) combined with  positive instruction reinforcement ."
  },
  {
  "input": {
    "starting_page": 24,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- V\n\nTESTING AND  VALIDATION\n\nAdvanced Prompt Engineering Techniques Demonstrated\n\n1. Multi-Modal Output Generation\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n2. Self-Executing Code Generation\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n3. Hierarchical Instruction Structure\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n4. Adaptive Behavior Based on Context\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\nPrompt Engineering Best Practices Demonstrated\n\n1. Specificity and Clarity\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n2. Output Format Templates\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n3. Constraint-Based Generation\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n4. Context Integration\n\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n5. Error Prevention\n\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing.\n\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 24\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Advanced Prompt Engineering Techniques Demonstrated*\n\n*1. Multi-Modal Output Generation*\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n#v(1em)\n*2. Self-Executing Code Generation*\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n#v(1em)\n*3. Hierarchical Instruction Structure*\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n#v(1em)\n*4. Adaptive Behavior Based on Context*\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\n#v(1em)\n*Prompt Engineering Best Practices Demonstrated*\n\n*1. Specificity and Clarity*\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n#v(1em)\n*2. Output Format Templates*\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n#pagebreak()\n*3. Constraint-Based Generation*\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n#v(1em)\n*4. Context Integration*\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n#v(1em)\n*5. Error Prevention*\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing.\n\n#v(1em)\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system.\n"
  },
  {
  "input": {
    "starting_page": 32,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- V\n\nTESTING AND  VALIDATION\n\nAdvanced Prompt Engineering Techniques Demonstrated\n\n1. Adaptive Behavior Based on Context\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\n2. Self-Executing Code Generation\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n3. Hierarchical Instruction Structure\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n4. Multi-Modal Output Generation\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\nPrompt Engineering Best Practices Demonstrated\n\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system.\n\n1. Specificity and Clarity\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n2. Output Format Templates\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n3. Constraint-Based Generation\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n4. Context Integration\n\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n5. Error Prevention\n\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 32\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Advanced Prompt Engineering Techniques Demonstrated*\n\n*1. Multi-Modal Output Generation*\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n#v(1em)\n*2. Self-Executing Code Generation*\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n#v(1em)\n*3. Hierarchical Instruction Structure*\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n#v(1em)\n*4. Adaptive Behavior Based on Context*\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\n#v(1em)\n*Prompt Engineering Best Practices Demonstrated*\n\n*1. Specificity and Clarity*\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n#v(1em)\n*2. Output Format Templates*\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n#pagebreak()\n*3. Constraint-Based Generation*\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n#v(1em)\n*4. Context Integration*\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n#v(1em)\n*5. Error Prevention*\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing.\n\n#v(1em)\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system.\n"
  },
  {
  "input": {
    "starting_page": 54,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- V\n\nTESTING AND  VALIDATION\n\nPrompt Engineering Best Practices Demonstrated\n\n1. Specificity and Clarity\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n2. Output Format Templates\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n3. Constraint-Based Generation\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n4. Context Integration\n\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n5. Error Prevention\n\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing.\n\nAdvanced Prompt Engineering Techniques Demonstrated\n\n1. Multi-Modal Output Generation\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n2. Self-Executing Code Generation\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n3. Hierarchical Instruction Structure\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n4. Adaptive Behavior Based on Context\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 54\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n\n*Prompt Engineering Best Practices Demonstrated*\n\n*1. Specificity and Clarity*\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n#v(1em)\n*2. Output Format Templates*\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n#v(1em)\n*3. Constraint-Based Generation*\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n#v(1em)\n*4. Context Integration*\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n#v(1em)\n*5. Error Prevention*\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing.\n\n#v(1em)\n*Advanced Prompt Engineering Techniques Demonstrated*\n\n*1. Multi-Modal Output Generation*\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n#v(1em)\n*2. Self-Executing Code Generation*\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n#pagebreak()\n*3. Hierarchical Instruction Structure*\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n#v(1em)\n*4. Adaptive Behavior Based on Context*\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\n#v(1em)\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system.\n"
  },
  {
  "input": {
    "starting_page": 24,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt": "CHAPTER- V\n\nTESTING AND  VALIDATION\n\nAdvanced Prompt Engineering Techniques Demonstrated\n\n1. Multi-Modal Output Generation\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n2. Self-Executing Code Generation\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n3. Hierarchical Instruction Structure\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n4. Adaptive Behavior Based on Context\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\nPrompt Engineering Best Practices Demonstrated\n\n1. Constraint-Based Generation\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n2. Specificity and Clarity\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n3. Output Format Templates\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n4. Context Integration\n\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n5. Error Prevention\n\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing.\n\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 24\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Advanced Prompt Engineering Techniques Demonstrated*\n\n*1. Multi-Modal Output Generation*\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n#v(1em)\n*2. Self-Executing Code Generation*\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n#v(1em)\n*3. Hierarchical Instruction Structure*\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n#v(1em)\n*4. Adaptive Behavior Based on Context*\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\n#v(1em)\n*Prompt Engineering Best Practices Demonstrated*\n\n*1. Constraint-Based Generation*\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n#v(1em)\n*2. Specificity and Clarity*\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n#pagebreak()\n*3. Output Format Templates*\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n#v(1em)\n*4. Context Integration*\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n#v(1em)\n*5. Error Prevention*\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing.\n\n#v(1em)\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system.\n"
  },
  {
  "input": {
    "starting_page": 40,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- V\n\nTESTING AND  VALIDATION\n\nPrompt Engineering Best Practices Demonstrated\n\n1. Multi-Modal Output Generation\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n2. Self-Executing Code Generation\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n3. Hierarchical Instruction Structure\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n4. Adaptive Behavior Based on Context\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system.\n\nAdvanced Prompt Engineering Techniques Demonstrated\n\n1. Specificity and Clarity\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n2. Output Format Templates\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n3. Constraint-Based Generation\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n4. Context Integration\n\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n5. Error Prevention\n\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing."
  },
  "output": "#page(\n paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n header: none,\n   footer: none,\n  numbering: none\n)[\n   #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n#v(1fr)\n\n#text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n #v(0.5em)\n#text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n#v(1fr)\n ]\n\n#let starting_page = 40\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n paper: \"a4\",\n margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n numbering: \"1\",\n header: context [\n#if counter(page).get().first() >= starting_page+2 [\n#set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n#block(\n width: 100%,\ninset: (top: 5pt, bottom: 8pt),\n)[\n #stack(\n dir: ttb,\n project_title,\n v(0.3em),\n spacing: 1.5pt,\n line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n v(0.3em),\n line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n)\n]\n]\n],\n footer: context [\n#set text(size: 10pt, font: \"Times New Roman\")\n#block(\n width: 100%,\n inset: (top: 8pt, bottom: 5pt),\n)[\n#stack(\n dir: ttb,\n spacing: 1.5pt, \n line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\nv(0.3em),line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n)\n#grid(\n columns: (1fr, 1fr, 1fr),\n align: (left, center, right),\n)[\n#department\n][\nPage #counter(page).display()\n][\n#academic_year\n]\n]\n]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n *Advanced Prompt Engineering Techniques Demonstrated*\n\n*1. Multi-Modal Output Generation*\n\nThe system prompt enables a single conversation turn to produce text, executable code, and structured data simultaneously, representing advanced  multi-modal prompting .\n\n*2. Self-Executing Code Generation*\n\nThe prompt instructs the model to generate executable Python code that will be run in a sandboxed environment, demonstrating  code-as-output prompting .\n\n*3. Hierarchical Instruction Structure*\n\nThe prompt uses nested instruction levels (main categories, sub-instructions, formatting rules)  showing  hierarchical prompt architecture .\n\n*4. Adaptive Behavior Based on Context*\n\nThe conditional multi-document instructions show  context-sensitive prompting  that adapts behavior based on system state.\n\nThis system prompt represents a sophisticated example of  production-level prompt engineering  that combines multiple advanced techniques to achieve reliable, multi-modal output generation from a conversational AI system.\n\n*Prompt Engineering Best Practices Demonstrated*\n\n*1. Specificity and Clarity*\n\nEvery instruction is explicit and unambiguous, reducing model confusion and improving output consistency.\n\n*2. Output Format Templates*\n\nProviding exact JSON structure examples ensures the model understands expected output format precisely.\n\n*3. Constraint-Based Generation*\n\nClear rules about what to include/exclude prevent common errors and ensure executable output.\n\n*4. Context Integration*\n\nSeamless integration of retrieved document content with user queries maintains relevance and accuracy.\n\n*5. Error Prevention*\n\nExplicit instructions about formatting, escaping, and structure prevent parsing errors in downstream processing."
  },
  {
  "input": {
    "starting_page": 24,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VI\n\nRESULTS AND  DISCUSSIONS\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Figure 6.1 Main Interface]\n)\n\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Figure 6.2 Generated Queries]\n)\n\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Figure 6.3 Generated Graphs]\n)\n\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 24\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"images/image002.png\", width: 70%),\n  caption: [Fig 6.1 Main Interface],\n  numbering: none\n)\n\n#v(1em)\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"images/image003.png\", width: 70%),\n  caption: [Fig 6.2 Generated queries],\n  numbering: none\n)\n\n#v(1em)\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"images/image004.png\", width: 70%),\n  caption: [Fig 6.3 Generated Graphs],\n  numbering: none\n)\n\n#v(1em)\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations.\n"
  },
  {
  "input": {
    "starting_page": 11,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VI\n\nRESULTS AND  DISCUSSIONS\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Figure 6.2 Generated Queries]\n)\n\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Figure 6.1 Main Interface]\n)\n\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Figure 6.3 Generated Graphs]\n)\n\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 11\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"images\\image003.png\", width: 70%),\n  caption: [Fig 6.2 Generated Queries],\n  numbering: none\n)\n\n#v(1em)\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"images\\image002.png\", width: 70%),\n  caption: [Fig 6.1 Main Interface],\n  numbering: none\n)\n\n#v(1em)\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"images\\image004.png\", width: 70%),\n  caption: [Fig 6.3 Generated Graphs],\n  numbering: none\n)\n\n#v(1em)\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations.\n"
  },
  {
  "input": {
    "starting_page": 76,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VI\n\nRESULTS AND  DISCUSSIONS\n\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Figure 6.3 Generated Graphs]\n)\n\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Figure 6.1 Main Interface]\n)\n\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Figure 6.2 Generated Queries]\n)"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 76\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"images\\image004.png\", width: 70%),\n  caption: [Fig 6.3 Generated Graphs],\n  numbering: none\n)\n#v(1em)\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations.\n\n#v(1em)\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"images\\image002.png\", width: 70%),\n  caption: [Fig 6.1 Main Interface],\n  numbering: none\n)\n\n#v(1em)\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"images\\image003.png\", width: 70%),\n  caption: [Fig 6.2 Generated Queries],\n  numbering: none\n)"
  },
  {
  "input": {
    "starting_page": 24,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VI\n\nRESULTS AND  DISCUSSIONS\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Figure 6.1 Main Interface]\n)\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Figure 6.3 Generated Graphs]\n)\n\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations\n\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Figure 6.2 Generated Queries]\n)\n\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 24\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#figure(\n  image(\"images\\image002.png\", width: 70%),\n  caption: [Fig 6.1 Main Interface],\n  numbering: none\n)\n\n#figure(\n  image(\"images\\image004.png\", width: 70%),\n  caption: [Fig 6.3 Generated Graphs],\n  numbering: none\n)\n#v(1em)\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations.\n\n#v(1em)\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"images\\image003.png\", width: 70%),\n  caption: [Fig 6.2 Generated Queries],\n  numbering: none\n)\n\n#v(1em)\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes."
  },
  {
  "input": {
    "starting_page": 19,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VI\n\nRESULTS AND  DISCUSSIONS\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Figure 6.3 Generated Graphs]\n)\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Figure 6.2 Generated Queries]\n)\n\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Figure 6.1 Main Interface]\n)\n\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 19\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe PDF  Chatbot  with Advanced RAG Capabilities successfully demonstrated complete functionality with effective database initialization using all-mpnet-base-v2 embeddings and seamless document processing capabilities. The system achieved robust performance in conversational AI interactions, multi-modal output generation, and intelligent information extraction from uploaded PDF documents.\n\n#v(1em)\nThe main interface shows successful database connection with  PostgreSQL  and  pgvector  extension, displaying clean document management features and intuitive chat functionality. The system correctly processes uploaded PDF files and maintains proper session state with clear visual feedback for users.\n\n#figure(\n  image(\"images\\image002.png\", width: 70%),\n  caption: [Fig 6.1 Main Interface],\n  numbering: none\n)\n\n#figure(\n  image(\"images\\image003.png\", width: 70%),\n  caption: [Fig 6.2 Generated Queries],\n  numbering: none\n)\n\n#v(1em)\nWhen queried about  Abhigna's  Python projects, the system accurately identified two specific projects: Dynamic PDFs with  SQLite  integration and MEME Generation for  WhatsApp . The table generation feature successfully extracted and formatted  Abhigna's  strengths into a structured table showing technical skills, expertise areas, and professional attributes.\n\n#figure(\n  image(\"images\\image004.png\", width: 70%),\n  caption: [Fig 6.3 Generated Graphs],\n  numbering: none\n)\n\n#v(1em)\nThe system automatically generated a professional horizontal bar chart displaying \"Certifications Obtained by  Abhigna  C B\" with comprehensive data including Harvard leadership courses and technical certifications. The visualization demonstrates effective data extraction and  matplotlib  integration, transforming unstructured PDF text into meaningful visual representations."
  },
  {
  "input": {
    "starting_page": 27,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VII\n\nCONCLUSION\n\n.\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation. The successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 27\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation. The successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world.\n"
  },
  {
  "input": {
    "starting_page": 53,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VII\n\nCONCLUSION\n\nThe successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world.\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 53\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world.\n\n#v(1em)\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation. \n\n"
  },
  {
  "input": {
    "starting_page": 90,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VII\n\nCONCLUSION\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation reviewThe successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 90\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation reviewThe successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world.\n"
  },
  {
  "input": {
    "starting_page": 47,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VII\n\nCONCLUSION\n\nThe system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review.\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation. The successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 47\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review.\n\n#v(1em)\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation. The successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world.\n\n"
  },
  {
  "input": {
    "starting_page": 27,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "RAG Based PDF Editor",
    "contents_txt":"CHAPTER- VII\n\nCONCLUSION\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world.\n\nThe project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 27\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"RAG Based PDF Editor\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nIn conclusion, this PDF  Chatbot  with Advanced RAG Capabilities represents a significant advancement in document-based conversational AI systems, successfully demonstrating how modern natural language processing technologies can transform static document interaction into dynamic, intelligent knowledge extraction workflows. The successful integration of retrieval-augmented generation principles ensures that responses remain grounded in actual document content rather than relying solely on pre-trained model knowledge, thereby minimizing hallucinations and maximizing factual accuracy, while the system's modular architecture and comprehensive error handling mechanisms provide a robust foundation for future enhancements and scalability improvements. This project ultimately bridges the gap between traditional document management systems and modern conversational AI, offering a powerful tool that transforms how individuals and organizations interact with their document collections, extract insights from complex information sources, and leverage artificial intelligence to enhance productivity and decision-making processes in an increasingly data-driven world.\n\n#v(1em)\nThe project has successfully integrated cutting-edge technologies including the all-mpnet-base-v2 embedding model for semantic understanding,  PostgreSQL  with  pgvector  extension for efficient vector similarity search, and  OpenAI's  GPT-4o-mini for intelligent response generation, creating a comprehensive system that not only answers questions but also generates visualizations, extracts structured data, and provides multi-modal analytical insights from uploaded PDF documents. The system's ability to handle documents up to 1GB in size, perform cross-document analysis, and generate executable Python code for charts and tables demonstrates its practical utility for real-world applications across diverse professional domains including business intelligence, academic research, legal analysis, and technical documentation review. The implementation of sophisticated features such as duplicate detection through SHA-256 hashing, intelligent text chunking with context preservation, batch processing for scalability, and JSON-structured responses for multi-modal output generation showcases the technical depth and innovation embedded within the system architecture. Furthermore, the project's emphasis on user experience through intuitive  Streamlit   interface design, real-time progress tracking, custom avatar integration, and conversational chat functionality makes advanced AI capabilities accessible to non-technical users while maintaining the sophisticated backend processing required for accurate information retrieval and generation. \n\n\n"
  },
  {
  "input": {
    "starting_page": 1,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   I  -  INTRODUCTION\n\n1.1 Statement of the Problem\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application.\n\n1.2. Objective of the Project\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n To store and manage pose metadata in a scalable, real-time cloud database(Firestore)\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n To integrate semantic vector search using Vertex AI for intelligent pose retrieval\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n To enable multimodal user interaction through text and voice input\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n To provide real-time posture analysis using webcam input and pose detection libraries\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n To generate voice-based output using text-to-speech for guided yoga practice\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 1\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Statement of the Problem]\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\n#v(1em)\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Objective of the Project]\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n* To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0*\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n#v(1em)\n* To store and manage pose metadata in a scalable, real-time cloud database(Firestore)*\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n#v(1em)\n* To integrate semantic vector search using Vertex AI for intelligent pose retrieval*\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n#v(1em)\n* To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency*\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n#pagebreak()\n* To enable multimodal user interaction through text and voice input*\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n#v(1em)\n* To provide real-time posture analysis using webcam input and pose detection libraries*\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n#v(1em)\n* To generate voice-based output using text-to-speech for guided yoga practice*\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus."
  },
  {
  "input": {
    "starting_page": 20,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   I  -  INTRODUCTION\n\n1.1 Objective of the Project\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application.\n\n1.2. Statement of the Problem\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n To store and manage pose metadata in a scalable, real-time cloud database(Firestore)\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n To integrate semantic vector search using Vertex AI for intelligent pose retrieval\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n To enable multimodal user interaction through text and voice input\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n To provide real-time posture analysis using webcam input and pose detection libraries\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n To generate voice-based output using text-to-speech for guided yoga practice\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 20\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Objective of the Project]\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\n#v(1em)\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Statement of the Problem]\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n* To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0*\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n#v(1em)\n* To store and manage pose metadata in a scalable, real-time cloud database(Firestore)*\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n#v(1em)\n* To integrate semantic vector search using Vertex AI for intelligent pose retrieval*\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n#v(1em)\n* To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency*\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n#pagebreak()\n* To enable multimodal user interaction through text and voice input*\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n#v(1em)\n* To provide real-time posture analysis using webcam input and pose detection libraries*\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n#v(1em)\n* To generate voice-based output using text-to-speech for guided yoga practice*\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus."
  },
  {
  "input": {
    "starting_page": 34,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   I  -  INTRODUCTION\n\n1.1 Objective of the Project\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n To store and manage pose metadata in a scalable, real-time cloud database(Firestore)\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n To integrate semantic vector search using Vertex AI for intelligent pose retrieval\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n To enable multimodal user interaction through text and voice input\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n To provide real-time posture analysis using webcam input and pose detection libraries\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n To generate voice-based output using text-to-speech for guided yoga practice\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus.\n\n1.2. Statement of the Problem\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 34\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Objective of the Project]\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n* To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0*\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n#v(1em)\n* To store and manage pose metadata in a scalable, real-time cloud database(Firestore)*\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n#v(1em)\n* To integrate semantic vector search using Vertex AI for intelligent pose retrieval*\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n#pagebreak()\n* To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency*\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n#v(1em)\n* To enable multimodal user interaction through text and voice input*\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n#v(1em)\n* To provide real-time posture analysis using webcam input and pose detection libraries*\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n#v(1em)\n* To generate voice-based output using text-to-speech for guided yoga practice*\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Statement of the Problem]\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\n#v(1em)\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application."
  },
  {
  "input": {
    "starting_page": 22,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   I  -  INTRODUCTION\n\n1.1 Objective of the Project\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n To store and manage pose metadata in a scalable, real-time cloud database(Firestore)\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n To integrate semantic vector search using Vertex AI for intelligent pose retrieval\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n To enable multimodal user interaction through text and voice input\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n To provide real-time posture analysis using webcam input and pose detection libraries\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n To generate voice-based output using text-to-speech for guided yoga practice\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus.\n\n1.2. Statement of the Problem\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 22\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Objective of the Project]\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n* To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0*\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n#v(1em)\n* To store and manage pose metadata in a scalable, real-time cloud database(Firestore)*\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n#v(1em)\n* To integrate semantic vector search using Vertex AI for intelligent pose retrieval*\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n#pagebreak()\n* To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency*\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n#v(1em)\n* To enable multimodal user interaction through text and voice input*\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n#v(1em)\n* To provide real-time posture analysis using webcam input and pose detection libraries*\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n#v(1em)\n* To generate voice-based output using text-to-speech for guided yoga practice*\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Statement of the Problem]\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\n#v(1em)\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application."
  },
  {
  "input": {
    "starting_page": 3,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   I  -  INTRODUCTION\n\n1.1. Objective of the Project\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n To store and manage pose metadata in a scalable, real-time cloud database(Firestore)\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n To integrate semantic vector search using Vertex AI for intelligent pose retrieval\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n To enable multimodal user interaction through text and voice input\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n To provide real-time posture analysis using webcam input and pose detection libraries\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n To generate voice-based output using text-to-speech for guided yoga practice\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus.\n\n1.2 Statement of the Problem\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 3\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Objective of the Project]\n\nThe primary objective of the Yoga Pose Recommender System is to develop an intelligent, cloud-based application that offers personalized yoga guidance tailored to individual needs, preferences, and physical conditions. By integrating advanced AI models, cloud infrastructure, and computer vision techniques, the project seeks to address the limitations of traditional yoga resources and provide a modern, accessible solution that promotes holistic health. The following specific objectives outline the core goals of the system:\n\n#v(1em)\n* To store and manage pose metadata in a scalable, real-time cloud database(Firestore)*\n\nFirestore is used to store essential pose information, such as difficulty level, targeted body parts, benefits, and contraindications. It enables efficient real-time retrieval and updates, ensuring the recommendation engine has immediate access to the latest metadata.\n\n#v(1em)\n* To integrate semantic vector search using Vertex AI for intelligent pose retrieval*\n\nVector Search allows the system to compare user input embeddings with yoga pose embeddings, ensuring that recommendations are semantically relevant rather than relying on exact keyword matches. This improves the systems ability to understand nuanced user needs.\n\n#v(1em)\n* To build a context-aware yoga pose recommendation engine using Vertex AI  and Gemini 2.0*\n\nThis objective involves using a fine-tuned large language model capable of understanding natural language input to recommend suitable yoga poses. The model processes user queries such as pain areas or emotional states and maps them to the most appropriate yoga practices.\n\n#pagebreak()\n* To deploy a cloud-native backend using Google Cloud Run for scalability and efficiency*\n\nBy deploying the backend as a serverless container on Cloud Run, the application benefits from automatic scaling, low maintenance, and high availability. This also simplifies the development pipeline and reduces operational overhead.\n\n#v(1em)\n* To enable multimodal user interaction through text and voice input*\n\nThe system supports both typing and speech via the Web Speech API, allowing users to choose their preferred method of interaction. This improves accessibility for users who may have visual impairments or prefer hands-free usage.\n\n#v(1em)\n* To provide real-time posture analysis using webcam input and pose detection libraries*\n\nA Python-based module uses  MediaPipe  and OpenCV to detect and overlay skeletal landmarks on the users video feed. This provides immediate visual feedback to help users align their posture with the recommended yoga poses.\n\n#v(1em)\n* To generate voice-based output using text-to-speech for guided yoga practice*\n\nThe application uses text-to-speech conversion to vocalize the recommended pose instructions, making it easier for users to follow along without constantly referring to the screen, thus enhancing user convenience and focus.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Statement of the Problem]\n\nIn today's fast-paced world, physical and mental health challenges such as back pain, stress, anxiety, and poor posture have become increasingly prevalent due to sedentary lifestyles, long screen time hours, and lack of physical activity. While yoga is widely recognized as an effective holistic remedy for many of these issues, a major barrier to adoption remains the lack of personalized guidance and accessibility. Beginners often struggle to find poses that match their needs, conditions, or limitations, and many users lack the expertise to select appropriate poses safely. Additionally, conventional yoga resourcessuch as videos or booksdo not offer real-time feedback or adapt to individual pain points and physical limitations. This creates a gap between users and effective yoga practice, especially for those who cannot access professional instructors or personalized training. Furthermore, individuals with disabilities, chronic pain, or limited mobility often find it difficult to engage in standardized yoga sessions that dont take their specific context into account.\n\n#v(1em)\nThere is also a significant lack of intelligent systems that can dynamically respond to a user's real-time posture and interpret natural language inputs like I have neck pain or I need relaxation. Most existing digital yoga solutions fail to combine interactivity, personalization, and context-awareness, which are essential for safe and effective practice. The need, therefore, is for a smart, adaptive system that not only understands the users physical condition and intent but also provides meaningful pose recommendations tailored to those inputs. Moreover, such a system must be inclusive supporting both textual and speech input, offering hands-free interaction, and delivering real-time feedback through pose correction. Solving this problem requires an intelligent integration of artificial intelligence and cloud computing technologies to bridge the gap between static content and personalized experiences. This project aims to fill that gap by building a cloud-based Yoga Pose Recommender System that empowers users with customized yoga guidance through an accessible, intelligent, and real-time application."
  },
  {
    "input": {
      "starting_page": 5,
      "academic_year": "2025-26",
      "department": "B.E/Dept of CSE/BNMIT",
      "project_title": "Yoga Pose Recommender System",
      "contents_txt": "CHAPTER II   -  LITERATURE SURVEY\n\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\n1] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n2]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n3] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n4] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains.\n\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization."
    },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 5\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"YOGA POSE RECOMMENDER SYSTEM\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\n#v(1em)\n*1\\] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020*\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n#v(1em)\n*2\\]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021*\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n#v(1em)\n*3\\] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019*\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n#v(1em)\n*4\\] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020*\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains.\n\n#v(1em)\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization.\n\n"
  },
  {
  "input": {
    "starting_page": 14,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER II   -  LITERATURE SURVEY\n\n1] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n2]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n3] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n4] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains.\n\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 14\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"YOGA POSE RECOMMENDER SYSTEM\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\n*1\\] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020*\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n#v(1em)\n*2\\]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021*\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n#v(1em)\n*3\\] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019*\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n#v(1em)\n*4\\] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020*\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains.\n\n#v(1em)\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\n#v(1em)\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization.\n\n"
  },
  {
  "input": {
    "starting_page": 35,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER II   -  LITERATURE SURVEY\n\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization.\n\n1] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n2]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n3] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n4] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 35\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"YOGA POSE RECOMMENDER SYSTEM\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\n#v(1em)\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization.\n\n#v(1em)\n*1\\] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020*\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n#v(1em)\n*2\\]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021*\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n#v(1em)\n*3\\] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019*\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n#v(1em)\n*4\\] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020*\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains.\n"
  },
  {
  "input": {
    "starting_page": 18,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER II   -  LITERATURE SURVEY\n\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\n1] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n2] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n3]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n4] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains.\n\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 18\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"YOGA POSE RECOMMENDER SYSTEM\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\nTo build a robust and effective Yoga Pose Recommender System, it is essential to understand the existing research and technological advancements in areas such as pose estimation, natural language understanding, AI-based recommendation systems, and cloud-based deployments. A literature survey helps identify the strengths, limitations, and opportunities in prior work and guides the development of a system that is both innovative and grounded in proven methodologies. The  following reviewed papers and techniques form the foundation of this projects approach.\n\n#v(1em)\n*1\\] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020*\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n#v(1em)\n*2\\] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019*\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n#v(1em)\n*3\\]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021*\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n#v(1em)\n*4\\] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020*\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains.\n\n#v(1em)\nIn conclusion, the reviewed literature provides a strong foundation for the development of the Yoga Pose Recommender System by highlighting the advancements and limitations in pose estimation, NLP-based recommendation engines, and cloud-enabled healthcare applications. The studies collectively underscore the importance of accurate human pose recognition, context-aware  recommendations, and the scalability benefits of cloud-based deployments. By integrating insights from these works, this project builds upon established methodologies while introducing innovations such as real-time skeletal feedback, multimodal user interaction, and AI-powered personalization."
  },
  {
  "input": {
    "starting_page": 1,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER II   -  LITERATURE SURVEY\n\n1] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n2]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n3] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n4] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 1\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"YOGA POSE RECOMMENDER SYSTEM\"\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em)\n\n*1\\] Pose2Yoga: Learning Pose Embeddings for Yoga Pose Correction  IEEE, 2020*\n\nThis paper proposes a novel framework for identifying and correcting yoga poses using pose embeddings generated through deep learning. The researchers used a combination of convolutional neural networks (CNNs) and a  large annotated  yoga pose dataset to learn fine-grained differences between correct and incorrect poses. This work is foundational to the real-time posture correction component of our project, as it demonstrates the effectiveness of embedding-based comparisons in identifying pose discrepancies.\n\n#v(1em)\n*2\\]  MoveNet :  Ultra Fast  and Accurate Human Pose Estimation  Google AI Blog, 2021*\n\nMoveNet  is a state-of-the-art pose detection model developed by Google that enables real-time estimation of human skeletal  keypoints . It achieves high accuracy and speed on edge devices using TensorFlow Lite. While our project primarily uses  MediaPipe  for skeletal tracking, the principles behind  MoveNet  informed  our  selection of lightweight, real-time pose estimation tools suitable for web integration, especially for webcam-based feedback.\n\n#v(1em)\n*3\\] Towards Personalized Yoga Recommendation Using Natural Language Processing  Springer, 2019*\n\nThis paper explores how NLP techniques can be applied to interpret user intent from textual inputs to recommend suitable yoga poses. It introduces a rule-based recommendation engine that uses keyword extraction and context mapping. Though limited in scalability and flexibility, this work provides an early blueprint for understanding the potential of NLP in personalized wellness applications. Our project builds on this concept by using large language models like Gemini 2.0 for more nuanced and contextual understanding.\n\n#v(1em)\n*4\\] A Survey on Cloud-Based Healthcare Services and Recommendation Systems  Elsevier, 2020*\n\nThis comprehensive review discusses various cloud computing models used in healthcare applications, focusing on scalability, availability, and security. It highlights the benefits of deploying intelligent health services using platforms like Google Cloud and AWS. The survey supports our decision to use Firestore, Cloud Run, and Vertex AI, emphasizing how cloud infrastructure can facilitate real-time recommendations, user data management, and AI model hosting in healthcare and wellness domains."
  },
  {
  "input": {
    "starting_page": 10,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   III  -  System Requirement Specification\n\n3.1 Software Requirements\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n3.1.1 Tools used:\n\n Google Cloud Services\n\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n Python\n\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n HTML/CSS (Frontend)\n\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n JavaScript and Web Speech API\n\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n3.1.2 Technology used:\n\n Gemini API\n\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n Vector Search (Vertex AI)\n\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n Vertex AI (Machine Learning Models)\n\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n Firestore (Database)\n\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n  MediaPipe  and OpenCV (Pose Detection)\n\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n Web Speech API\n\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\n3.2 Hardware Requirements\n\n1. Client-Side Device (Users Computer/Smartphone)\n\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n2. Server-Side Infrastructure (Cloud-Based)\n\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n3. Storage Device (Cloud Storage)\n\n The system uses Google Firestore to store pose metadata and  Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n4. Graphics Processing Unit (GPU)\n\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n5. Internet Connection\n\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure  more  responsive user experience, especially for real-time pose feedback."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 10\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Software Requirements]\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\n#v(1em)\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n*3.1.1 Tools used:*\n\n* Google Cloud Services*\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n#v(1em)\n* Python*\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n#v(1em)\n* HTML/CSS (Frontend)*\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n#v(1em)\n* JavaScript and Web Speech API*\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n#v(1em)\n*3.1.2 Technology used:*\n\n* Gemini API*\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n#v(1em)\n* Vector Search (Vertex AI)*\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n#v(1em)\n* Vertex AI (Machine Learning Models)*\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n#v(1em)\n* Firestore (Database)*\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n#v(1em)\n*  MediaPipe  and OpenCV (Pose Detection)*\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n#v(1em)\n* Web Speech API*\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Hardware Requirements]\n\n*1. Client-Side Device (Users Computer/Smartphone)*\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n#v(1em)\n*2. Server-Side Infrastructure (Cloud-Based)*\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n#v(1em)\n*3. Storage Device (Cloud Storage)*\n The system uses Google Firestore to store pose metadata and Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n#v(1em)\n*4. Graphics Processing Unit (GPU)*\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n#v(1em)\n*5. Internet Connection*\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure more responsive user experience, especially for real-time pose feedback."
  },
  {
  "input": {
    "starting_page": 18,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   III  -  System Requirement Specification\n\n3.1 Software Requirements\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n3.1.1 Technology used:\n\n Gemini API\n\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n Vector Search (Vertex AI)\n\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n Vertex AI (Machine Learning Models)\n\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n Firestore (Database)\n\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n  MediaPipe  and OpenCV (Pose Detection)\n\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n Web Speech API\n\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\n3.1.2 Tools used:\n\n Google Cloud Services\n\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n Python\n\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n HTML/CSS (Frontend)\n\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n JavaScript and Web Speech API\n\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n3.2 Hardware Requirements\n\n1. Client-Side Device (Users Computer/Smartphone)\n\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n2. Server-Side Infrastructure (Cloud-Based)\n\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n3. Storage Device (Cloud Storage)\n\n The system uses Google Firestore to store pose metadata and  Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n4. Graphics Processing Unit (GPU)\n\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n5. Internet Connection\n\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure  more  responsive user experience, especially for real-time pose feedback."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 18\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Software Requirements]\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\n#v(1em)\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n#v(1em)\n*3.1.1 Technology used:*\n\n* Gemini API*\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n#v(1em)\n* Vector Search (Vertex AI)*\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n#v(1em)\n* Vertex AI (Machine Learning Models)*\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n#v(1em)\n* Firestore (Database)*\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n#v(1em)\n*  MediaPipe  and OpenCV (Pose Detection)*\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n#v(1em)\n* Web Speech API*\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\n#v(1em)\n*3.1.2 Tools used:*\n\n* Google Cloud Services*\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n#v(1em)\n* Python*\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n#v(1em)\n* HTML/CSS (Frontend)*\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n#v(1em)\n* JavaScript and Web Speech API*\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Hardware Requirements]\n\n*1. Client-Side Device (Users Computer/Smartphone)*\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n#v(1em)\n*2. Server-Side Infrastructure (Cloud-Based)*\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n#v(1em)\n*3. Storage Device (Cloud Storage)*\n The system uses Google Firestore to store pose metadata and Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n#v(1em)\n*4. Graphics Processing Unit (GPU)*\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n#v(1em)\n*5. Internet Connection*\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure more responsive user experience, especially for real-time pose feedback."
  },
  {
  "input": {
    "starting_page": 28,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   III  -  System Requirement Specification\n\n3.1 Software Requirements\n\n3.1.1 Tools used:\n\n Google Cloud Services\n\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n Python\n\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n HTML/CSS (Frontend)\n\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n JavaScript and Web Speech API\n\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n3.1.2 Technology used:\n\n Gemini API\n\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n Vector Search (Vertex AI)\n\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n Vertex AI (Machine Learning Models)\n\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n Firestore (Database)\n\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n  MediaPipe  and OpenCV (Pose Detection)\n\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n Web Speech API\n\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n3.2 Hardware Requirements\n\n1. Client-Side Device (Users Computer/Smartphone)\n\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n2. Server-Side Infrastructure (Cloud-Based)\n\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n3. Storage Device (Cloud Storage)\n\n The system uses Google Firestore to store pose metadata and  Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n4. Graphics Processing Unit (GPU)\n\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n5. Internet Connection\n\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure  more  responsive user experience, especially for real-time pose feedback."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 28\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Software Requirements]\n\n*3.1.1 Tools used:*\n\n* Google Cloud Services*\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n#v(1em)\n* Python*\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n#v(1em)\n* HTML/CSS (Frontend)*\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n#v(1em)\n* JavaScript and Web Speech API*\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n#v(1em)\n*3.1.2 Technology used:*\n\n* Gemini API*\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n#v(1em)\n* Vector Search (Vertex AI)*\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n#v(1em)\n* Vertex AI (Machine Learning Models)*\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n#v(1em)\n* Firestore (Database)*\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n#v(1em)\n*  MediaPipe  and OpenCV (Pose Detection)*\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n#v(1em)\n* Web Speech API*\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\n#v(1em)\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\n#v(1em)\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Hardware Requirements]\n\n*1. Client-Side Device (Users Computer/Smartphone)*\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n#v(1em)\n*2. Server-Side Infrastructure (Cloud-Based)*\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n#v(1em)\n*3. Storage Device (Cloud Storage)*\n The system uses Google Firestore to store pose metadata and Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n#v(1em)\n*4. Graphics Processing Unit (GPU)*\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n#v(1em)\n*5. Internet Connection*\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure more responsive user experience, especially for real-time pose feedback."
  },
  {
  "input": {
    "starting_page": 22,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   III  -  System Requirement Specification\n\n3.1 Hardware Requirements\n\n1. Client-Side Device (Users Computer/Smartphone)\n\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n2. Server-Side Infrastructure (Cloud-Based)\n\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n3. Storage Device (Cloud Storage)\n\n The system uses Google Firestore to store pose metadata and  Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n4. Graphics Processing Unit (GPU)\n\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n5. Internet Connection\n\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure  more  responsive user experience, especially for real-time pose feedback.\n\n3.2 Software Requirements\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n3.1.1 Tools used:\n\n Google Cloud Services\n\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n Python\n\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n HTML/CSS (Frontend)\n\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n JavaScript and Web Speech API\n\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n3.1.2 Technology used:\n\n Gemini API\n\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n Vector Search (Vertex AI)\n\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n Vertex AI (Machine Learning Models)\n\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n Firestore (Database)\n\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n  MediaPipe  and OpenCV (Pose Detection)\n\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n Web Speech API\n\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 22\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Hardware Requirements]\n\n*1. Client-Side Device (Users Computer/Smartphone)*\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n#v(1em)\n*2. Server-Side Infrastructure (Cloud-Based)*\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n#v(1em)\n*3. Storage Device (Cloud Storage)*\n The system uses Google Firestore to store pose metadata and Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n#v(1em)\n*4. Graphics Processing Unit (GPU)*\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n#v(1em)\n*5. Internet Connection*\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure more responsive user experience, especially for real-time pose feedback.\n\n#text(size: 14pt, weight: \"bold\")[3.2 Software Requirements]\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\n#v(1em)\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n*3.1.1 Tools used:*\n\n* Google Cloud Services*\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n#v(1em)\n* Python*\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n#v(1em)\n* HTML/CSS (Frontend)*\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n#v(1em)\n* JavaScript and Web Speech API*\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n#v(1em)\n*3.1.2 Technology used:*\n\n* Gemini API*\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n#v(1em)\n* Vector Search (Vertex AI)*\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n#v(1em)\n* Vertex AI (Machine Learning Models)*\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n#v(1em)\n* Firestore (Database)*\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n#v(1em)\n*  MediaPipe  and OpenCV (Pose Detection)*\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n#v(1em)\n* Web Speech API*\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input."
  },
  {
  "input": {
    "starting_page": 37,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt":  "CHAPTER   III  -  System Requirement Specification\n\n3.1 Hardware Requirements\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n3.1.1 Technology used:\n\n Google Cloud Services\n\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n Python\n\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n HTML/CSS (Frontend)\n\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n JavaScript and Web Speech API\n\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n3.1.2 Tools used:\n\n Gemini API\n\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n Vector Search (Vertex AI)\n\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n Vertex AI (Machine Learning Models)\n\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n Firestore (Database)\n\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n  MediaPipe  and OpenCV (Pose Detection)\n\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n Web Speech API\n\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\n3.2 Software Requirements\n\n1. Client-Side Device (Users Computer/Smartphone)\n\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n2. Server-Side Infrastructure (Cloud-Based)\n\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n3. Storage Device (Cloud Storage)\n\n The system uses Google Firestore to store pose metadata and  Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n4. Graphics Processing Unit (GPU)\n\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n5. Internet Connection\n\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure  more  responsive user experience, especially for real-time pose feedback."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 37\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Hardware Requirements]\n\nThe development of the Yoga Pose Recommender System relies on a carefully selected software stack that enables seamless integration of cloud services, artificial intelligence, and real-time user interaction. The software requirements define the necessary tools, platforms, libraries, and frameworks needed to design, develop, deploy, and maintain the application efficiently. Given the cloud-centric nature of the system, the selection of software focuses heavily on compatibility with Google Cloud services such as Vertex AI, Firestore, and Cloud Run, ensuring that all components work in harmony within a scalable and serverless environment. The use of advanced AI models and real-time pose estimation demands support for machine learning frameworks and lightweight processing libraries that can run efficiently on both cloud infrastructure and client-side devices.\n\n#v(1em)\nFurthermore, the software requirements are driven by the systems need to support multimodal input (text and speech), dynamic UI interaction, and webcam-based pose detection. On the frontend, modern JavaScript frameworks and browser APIs such as the Web Speech API are essential for voice-based interaction and rendering real-time video overlays. On the backend, Java and Python are chosen for their robust ecosystem and compatibility with AI and computer vision workloads. Integration of tools such as  MediaPipe  and OpenCV allows the system to analyze skeletal movements from webcam feeds, while cloud-based deployment tools ensure continuous integration and delivery. Together, these software components form the backbone of a responsive, intelligent, and accessible wellness platform.\n\n#v(1em)\n*3.1.1 Technology used:*\n\n* Gemini API*\nThe Gemini API, a large language model from Googles Vertex AI, is central to the Yoga Pose Recommender System's functionality. Fine-tuned to interpret natural language input, Gemini 2.0  is capable of understanding  a variety of user queries related to pain points, physical conditions, and emotional states. It processes these inputs, such as \"lower back pain\" or \"stress relief,\" and generates intelligent, context-aware recommendations for yoga poses. By leveraging the capabilities of Gemini, the system can accurately match user needs to relevant poses, making the recommendation process more personalized and dynamic. This AI-powered approach enables the system to handle diverse user queries with high accuracy and adaptability, far beyond simple keyword matching.\n\n#v(1em)\n* Vector Search (Vertex AI)*\nVertex AI's Vector Search enables efficient semantic search capabilities within the system by indexing pose data as high-dimensional vectors. When a user submits a query, it is transformed into an embedding, and Vertex AI compares this embedding against the indexed pose data. This vector-based search mechanism ensures that the system not only performs traditional keyword-based matching but also understands the underlying intent of the user, providing more accurate and meaningful recommendations.\n\n#v(1em)\n* Vertex AI (Machine Learning Models)*\nVertex AI is a comprehensive machine learning platform that provides the infrastructure for building, deploying, and managing AI models at scale. In this system, Vertex AI hosts  fine -tuned models that process user input and deliver pose recommendations based on real-time queries. The integration with Gemini 2.0 allows for highly contextualized responses to user inputs. Vertex AI provides a seamless environment for deploying machine learning models, ensuring that the system scales efficiently with user demand. By using this platform, the Yoga Pose Recommender System benefits from Google's robust infrastructure, enabling it to handle complex machine learning tasks while maintaining high performance and reliability.\n\n#v(1em)\n* Firestore (Database)*\nFirestore is a NoSQL cloud database service provided by Google that plays a pivotal role in storing and managing metadata for the yoga poses in the system. Firestore is ideal for this project because of its real-time synchronization capabilities, which ensures that any updates to the pose database are instantly reflected across all user sessions. The metadata stored in Firestore includes detailed information about each yoga pose, such as difficulty, target body areas, step-by-step instructions, benefits, and contraindications. This allows the system  to retrieve relevant pose data quickly and efficiently when generating recommendations, ensuring a smooth and responsive user experience.\n\n#v(1em)\n*  MediaPipe  and OpenCV (Pose Detection)*\nMediaPipe  and OpenCV are used for real-time pose detection in the Yoga Pose Recommender System.  MediaPipe  is a cross-platform framework that offers efficient solutions for human body pose estimation, enabling the system to identify key skeletal landmarks on the user's body from webcam input. OpenCV, a popular computer vision library, is used to process and display these landmarks on the video feed, overlaying skeletal markers on the user's live image. This feature provides immediate visual feedback, allowing users to compare their posture with the recommended poses in real time. Together,  MediaPipe  and OpenCV enable accurate and responsive pose tracking, which is a critical aspect of the system's interactive features.\n\n#v(1em)\n* Web Speech API*\nThe Web Speech API is integrated into the frontend to support multimodal interaction between the user and the Yoga Pose Recommender System. This API enables speech-to-text functionality, allowing users to interact with the system using voice commands, such as describing pain points (\"I have neck pain\"). In addition, the text-to-speech feature of the Web Speech API is used to convert the system's pose recommendations into spoken instructions, guiding the user through the poses. By incorporating the Web Speech API, the system enhances accessibility for users with visual impairments or those who prefer voice-based interactions over traditional text input.\n\n#v(1em)\n*3.1.2 Tools used:*\n\n* Google Cloud Services*\nThe Yoga Pose Recommender System leverages several Google Cloud services to ensure a scalable, reliable, and efficient backend. Vertex AI is at the core of the AI  driven  engine. By using pre-trained and fine-tuned models like Gemini 2.0, Vertex AI provides the system with the ability to process and interpret user inputs (both text and speech), enabling the recommendation of suitable yoga poses based on the user's specific needs. Firestore, a real-time NoSQL database, stores metadata about various yoga poses, including descriptions, benefits, and contraindications, and allows the system to retrieve this data quickly and  efficiently during the recommendation process. Google Cloud Run is employed for serverless deployment, which enables automatic scaling and low-latency performance without the need for complex infrastructure management. This choice ensures that the system can handle varying user traffic seamlessly, providing high availability while reducing operational costs.\n\n#v(1em)\n* Python*\nPython serves as the primary programming language for implementing backend functionalities, particularly the pose detection and analysis components of the system. Using libraries such as  MediaPipe  and OpenCV, Python enables real-time  webcam based  pose estimation by detecting key skeletal landmarks and providing immediate feedback on posture accuracy.  MediaPipe  offers efficient solutions for body pose tracking, while OpenCV helps in processing and overlaying skeletal data on live video feeds. Additionally, Python is used to interface with Google Cloud services, handling API calls for recommendations and data retrieval. Its versatility, ease of integration with machine learning models, and strong ecosystem of libraries make it the ideal choice for this projects backend.\n\n#v(1em)\n* HTML/CSS (Frontend)*\nFor the  frontend , HTML and CSS are employed to create a user-friendly, responsive interface. HTML provides the structure of the web pages, ensuring that users can easily navigate through the application and interact with the system. CSS is used to style the interface, ensuring a clean and visually appealing design. Given the projects emphasis on accessibility, special attention is paid to designing an intuitive layout that supports various input methods, including voice and text. CSS ensures the layout is adaptable to different screen sizes, improving usability on both desktops and mobile devices. Together, HTML and CSS form the backbone of a frontend that is both functional and easy to navigate for users of all backgrounds.\n\n#v(1em)\n* JavaScript and Web Speech API*\nJavaScript is used to add interactivity to the frontend, handling user interactions and managing data flow between the user interface and backend services. The Web Speech API is particularly important for enabling voice-based input and text-to speech functionality. This API allows users to dictate their symptoms or requests (such as neck pain or stress relief) and receive spoken yoga instructions in return. The integration of the Web Speech API enhances accessibility, allowing the app to support hands-free operation, which is essential for users with disabilities or those who prefer voice interactions over typing.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Software Requirements]\n\n*1. Client-Side Device (Users Computer/Smartphone)*\n Minimum specifications include a processor (CPU) capable of handling video input (e.g., Intel i3 or equivalent), at least 4 GB of RAM, and a webcam for pose detection. A high-definition (HD) webcam is recommended for optimal accuracy in skeletal tracking.\n\n#v(1em)\n*2. Server-Side Infrastructure (Cloud-Based)*\n The application is hosted entirely on Google Cloud Platform (GCP), so physical server hardware is abstracted away. However, for scalability and performance, it requires sufficient cloud computing resources such as virtual machine instances on Google Cloud Compute Engine for supporting backend operations, data processing, and API management.\n\n#v(1em)\n*3. Storage Device (Cloud Storage)*\n The system uses Google Firestore to store pose metadata and Google Cloud Storage for storing large datasets (e.g., pose images, videos, and machine learning model checkpoints). While physical storage hardware is managed by Google Cloud, ensuring fast read/write speeds and high availability is critical for real-time data access and smooth user experience.\n\n#v(1em)\n*4. Graphics Processing Unit (GPU)*\n For faster processing of pose detection and real-time video overlay, the system can benefit from using GPU resources, especially when processing multiple concurrent video streams. While basic pose detection can run on CPU, leveraging a NVIDIA GPU (or equivalent) in the cloud can significantly accelerate pose analysis, particularly in machine learning training and inference tasks. This is essential for providing low-latency feedback in real-time pose tracking.\n\n#v(1em)\n*5. Internet Connection*\n A stable and high-speed internet connection (at least 5 Mbps for smooth video streaming and cloud communication) is required for seamless interaction with the system, particularly for the live webcam feed, voice input, and cloud-based services. Faster connections (10 Mbps or above) will ensure more responsive user experience, especially for real-time pose feedback."
  },
  {
  "input": {
    "starting_page": 25,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   IV  -  METHODOLOGY AND IMPLEMENTATION\n\n4.1 METHODOLOGY\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\nUser Input and Interaction Module\n\n Input Methods: The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n Voice Feedback: The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to  refer back  to the screen.\n\nNatural Language Processing (NLP) Module\n\n Processing Input: The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n Recommendation Logic: The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\nPose Recommendation Engine Module\n\n Vector Search: The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n Database Access: The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\nReal-Time Pose Detection Module\n\n Webcam Input: Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n Pose Overlay: Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\nCloud Deployment and Scalability Module\n\n Backend Deployment: The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n Machine Learning Infrastructure: The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\nData Storage and Management Module\n\n Firestore: All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n Cloud Storage: In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\nUser Interface and Visualization Module\n\n Frontend Development: The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile)\n\n Pose Visualization: The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\nFeedback and Continuous Learning Module\n\n User Feedback: After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n Continuous Learning: The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n4.2 EXECUTION FLOWCHART\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.1: Work Flowchart]\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 25\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\n*User Input and Interaction Module*\n* Input Methods:* The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n* Voice Feedback:* The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to refer back to the screen.\n\n*Natural Language Processing (NLP) Module*\n* Processing Input:* The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n* Recommendation Logic:* The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\n*Pose Recommendation Engine Module*\n* Vector Search:* The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n* Database Access:* The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\n*Real-Time Pose Detection Module*\n* Webcam Input:* Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n* Pose Overlay:* Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\n*Cloud Deployment and Scalability Module*\n* Backend Deployment:* The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n* Machine Learning Infrastructure:* The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\n\n*Data Storage and Management Module*\n* Firestore:* All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n* Cloud Storage:* In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\n*User Interface and Visualization Module*\n* Frontend Development:* The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile).\n\n* Pose Visualization:* The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\n*Feedback and Continuous Learning Module*\n* User Feedback:* After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n* Continuous Learning:* The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"images\\image001.png\", width: 70%),\n  caption: [Fig 4.1 Work Flowchart],\n  numbering: none\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services."
  },
  {
    "input": {
      "starting_page": 2,
      "academic_year": "2025-26",
      "department": "B.E/Dept of CSE/BNMIT",
      "project_title": "Yoga Pose Recommender System",
      "contents_txt": "CHAPTER   IV  -  METHODOLOGY AND IMPLEMENTATION\n\n4.1 EXECUTION FLOWCHART\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.1: Work Flowchart]\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services.\n\n4.2 METHODOLOGY\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\nUser Input and Interaction Module\n\n Input Methods: The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n Voice Feedback: The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to  refer back  to the screen.\n\nNatural Language Processing (NLP) Module\n\n Processing Input: The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n Recommendation Logic: The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\nPose Recommendation Engine Module\n\n Vector Search: The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n Database Access: The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\nReal-Time Pose Detection Module\n\n Webcam Input: Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n Pose Overlay: Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\nCloud Deployment and Scalability Module\n\n Backend Deployment: The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n Machine Learning Infrastructure: The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\nData Storage and Management Module\n\n Firestore: All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n Cloud Storage: In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\nUser Interface and Visualization Module\n\n Frontend Development: The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile)\n\n Pose Visualization: The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\nFeedback and Continuous Learning Module\n\n User Feedback: After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n Continuous Learning: The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions."
    },
    "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 2\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Execution Flowchart]\n\n#figure(\n  image(\"images/image001.png\", width: 70%),\n  caption: [Fig 4.1 Work Flowchart],\n  numbering: none\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[4.2 Methodology]\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\n*User Input and Interaction Module*\n* Input Methods:* The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n* Voice Feedback:* The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to refer back to the screen.\n\n*Natural Language Processing (NLP) Module*\n* Processing Input:* The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n* Recommendation Logic:* The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\n*Pose Recommendation Engine Module*\n* Vector Search:* The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n* Database Access:* The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\n*Real-Time Pose Detection Module*\n* Webcam Input:* Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n* Pose Overlay:* Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\n*Cloud Deployment and Scalability Module*\n* Backend Deployment:* The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n* Machine Learning Infrastructure:* The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\n\n*Data Storage and Management Module*\n* Firestore:* All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n* Cloud Storage:* In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\n*User Interface and Visualization Module*\n* Frontend Development:* The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile).\n\n* Pose Visualization:* The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\n*Feedback and Continuous Learning Module*\n* User Feedback:* After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n* Continuous Learning:* The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions."
  },
  {
  "input": {
    "starting_page": 35,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   IV  -  METHODOLOGY AND IMPLEMENTATION\n\n4.1 METHODOLOGY\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\nNatural Language Processing (NLP) Module\n\n Processing Input: The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n Recommendation Logic: The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\nPose Recommendation Engine Module\n\n Vector Search: The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n Database Access: The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\nReal-Time Pose Detection Module\n\n Webcam Input: Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n Pose Overlay: Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\nCloud Deployment and Scalability Module\n\n Backend Deployment: The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n Machine Learning Infrastructure: The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\nUser Input and Interaction Module\n\n Input Methods: The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n Voice Feedback: The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to  refer back  to the screen.\n\nData Storage and Management Module\n\n Firestore: All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n Cloud Storage: In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\nUser Interface and Visualization Module\n\n Frontend Development: The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile)\n\n Pose Visualization: The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\nFeedback and Continuous Learning Module\n\n User Feedback: After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n Continuous Learning: The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n4.2 EXECUTION FLOWCHART\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.1: Work Flowchart]\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 35\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\n*Natural Language Processing (NLP) Module*\n* Processing Input:* The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n* Recommendation Logic:* The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\n*Pose Recommendation Engine Module*\n* Vector Search:* The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n* Database Access:* The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\n*Real-Time Pose Detection Module*\n* Webcam Input:* Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n* Pose Overlay:* Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\n*Cloud Deployment and Scalability Module*\n* Backend Deployment:* The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n* Machine Learning Infrastructure:* The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\n*User Input and Interaction Module*\n* Input Methods:* The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n* Voice Feedback:* The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to refer back to the screen.\n\n*Data Storage and Management Module*\n* Firestore:* All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n* Cloud Storage:* In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\n*User Interface and Visualization Module*\n* Frontend Development:* The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile).\n\n* Pose Visualization:* The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\n*Feedback and Continuous Learning Module*\n* User Feedback:* After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n* Continuous Learning:* The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"images/image001.png\", width: 70%),\n  caption: [Fig 4.1 Work Flowchart],\n  numbering: none\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services."
  },
  {
  "input": {
    "starting_page": 10,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   IV  -  METHODOLOGY AND IMPLEMENTATION\n\n4.1 EXECUTION FLOWCHART\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\nUser Input and Interaction Module\n\n Input Methods: The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n Voice Feedback: The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to  refer back  to the screen.\n\nNatural Language Processing (NLP) Module\n\n Processing Input: The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n Recommendation Logic: The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\nPose Recommendation Engine Module\n\n Vector Search: The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n Database Access: The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\nReal-Time Pose Detection Module\n\n Webcam Input: Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n Pose Overlay: Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\nCloud Deployment and Scalability Module\n\n Backend Deployment: The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n Machine Learning Infrastructure: The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\nData Storage and Management Module\n\n Firestore: All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n Cloud Storage: In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\nUser Interface and Visualization Module\n\n Frontend Development: The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile)\n\n Pose Visualization: The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\nFeedback and Continuous Learning Module\n\n User Feedback: After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n Continuous Learning: The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n4.2 METHODOLOGY\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.1: Work Flowchart]\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 10\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Execution Flowchart]\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\n*User Input and Interaction Module*\n* Input Methods:* The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n* Voice Feedback:* The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to refer back to the screen.\n\n*Natural Language Processing (NLP) Module*\n* Processing Input:* The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n* Recommendation Logic:* The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\n*Pose Recommendation Engine Module*\n* Vector Search:* The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n* Database Access:* The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\n*Real-Time Pose Detection Module*\n* Webcam Input:* Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n* Pose Overlay:* Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\n*Cloud Deployment and Scalability Module*\n* Backend Deployment:* The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n* Machine Learning Infrastructure:* The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\n\n*Data Storage and Management Module*\n* Firestore:* All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n* Cloud Storage:* In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\n*User Interface and Visualization Module*\n* Frontend Development:* The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile).\n\n* Pose Visualization:* The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\n*Feedback and Continuous Learning Module*\n* User Feedback:* After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n* Continuous Learning:* The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Methodology]\n\n#figure(\n  image(\"images/image001.png\", width: 70%),\n  caption: [Fig 4.1 Work Flowchart],\n  numbering: none\n)\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services."
  },
  {
  "input": {
    "starting_page": 46,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   IV  -  METHODOLOGY AND IMPLEMENTATION\n\n4.1 METHODOLOGY\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\nUser Input and Interaction Module\n\n Input Methods: The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n Voice Feedback: The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to  refer back  to the screen.\n\nNatural Language Processing (NLP) Module\n\n Processing Input: The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n Recommendation Logic: The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\nPose Recommendation Engine Module\n\n Vector Search: The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n Database Access: The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\nReal-Time Pose Detection Module\n\n Webcam Input: Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n Pose Overlay: Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\nCloud Deployment and Scalability Module\n\n Backend Deployment: The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n Machine Learning Infrastructure: The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\nData Storage and Management Module\n\n Firestore: All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n Cloud Storage: In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\nUser Interface and Visualization Module\n\n Frontend Development: The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile)\n\n Pose Visualization: The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\nFeedback and Continuous Learning Module\n\n User Feedback: After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n Continuous Learning: The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n4.2 EXECUTION FLOWCHART\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services.\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.1: Work Flowchart]\n)"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 46\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\nThe methodology of the Yoga Pose Recommender System involves multiple modules that work in tandem to deliver real-time, personalized yoga recommendations. Each module contributes to the overall functionality of the system, ensuring an efficient and user-friendly experience. Below is a breakdown of the methodology, divided into the key modules of the system:\n\n*User Input and Interaction Module*\n* Input Methods:* The user can provide input through two primary methods: text and voice. Text input is entered via a standard web form, where users can describe their conditions (e.g., \"I have back pain\") or select preferences from predefined options. Voice input is captured using the Web Speech API, which converts speech to text in real-time. This enables hands-free interaction for users, making the system more accessible, especially for those with disabilities.\n\n* Voice Feedback:* The system also converts pose instructions and recommendations into speech using the text-to-speech feature of the Web Speech API. This ensures the user can follow along with the recommended yoga poses without needing to refer back to the screen.\n\n*Natural Language Processing (NLP) Module*\n* Processing Input:* The input text (either typed or spoken) is processed by the Gemini 2.0 API, which is fine-tuned to understand user intent. The NLP engine analyzes user queries to detect pain points, physical conditions, or desired outcomes (e.g., \"I need a relaxing pose\" or \"I have shoulder pain\").\n\n* Recommendation Logic:* The processed query is used to generate a list of recommended yoga poses based on the user's context. The system considers factors like the users current physical state (e.g., pain or stress), pose difficulty, and contraindications to generate the most suitable pose recommendations.\n\n*Pose Recommendation Engine Module*\n* Vector Search:* The pose recommendation system is powered by Vertex AIs Vector Search. Yoga pose data is represented as vector embeddings that capture the semantic properties of each pose (e.g., target body parts, difficulty level, benefits). When the system receives a query, the user input is also transformed into an embedding using NLP techniques, and a similarity search is conducted against the stored pose vectors in Google Firestore.\n\n* Database Access:* The pose metadata, such as pose names, descriptions, and images, are stored in Firestore. The pose recommendation engine queries this database in real time to retrieve relevant pose details and presents them to the user in a structured format.\n\n*Real-Time Pose Detection Module*\n* Webcam Input:* Using  MediaPipe  and OpenCV, the system analyzes the users live webcam feed to detect skeletal landmarks on the users body in real time. This is the core of the systems pose detection functionality, providing immediate feedback on the users posture.\n\n* Pose Overlay:* Once the skeletal landmarks are detected, the system overlays these points on the live video feed, giving visual feedback to the user. The user can compare their posture with the recommended yoga poses, helping them align correctly with minimal effort.\n\n*Cloud Deployment and Scalability Module*\n* Backend Deployment:* The systems backend, responsible for handling user requests, pose recommendations, and data retrieval, is deployed on Google Cloud Run. This serverless deployment platform ensures that the system can scale automatically based on traffic, offering high availability and low-latency performance.\n\n* Machine Learning Infrastructure:* The Vertex AI platform is used for managing machine learning models, including the fine-tuning of Gemini 2.0 for understanding user queries. The system can scale its model deployment as needed, ensuring efficient handling of large user bases.\n\n\n*Data Storage and Management Module*\n* Firestore:* All metadata related to  the yoga  poses, such as names, instructions, images, and benefits, are stored in Google Firestore, a cloud-native NoSQL database.  Firestores  real-time synchronization ensures that any updates to the pose database are reflected instantly for all users, providing accurate and up- to date  recommendations.\n\n* Cloud Storage:* In addition to Firestore, Google Cloud Storage is used to store larger datasets like high-resolution images, videos, and training models for pose estimation. Cloud Storages scalability ensures that the system can efficiently handle large volumes of data.\n\n*User Interface and Visualization Module*\n* Frontend Development:* The frontend of the system is built using HTML, CSS, and JavaScript. The user interface is designed to be responsive, ensuring that the system works seamlessly across devices (desktop, tablet, and mobile).\n\n* Pose Visualization:* The interface also integrates real-time video feeds, displaying skeletal landmarks overlaid on the users live video to guide them in performing the yoga poses correctly. This module leverages WebRTC for real time video streaming and Canvas for rendering the skeletal overlay on the users screen.\n\n*Feedback and Continuous Learning Module*\n* User Feedback:* After completing a yoga session, the system may prompt users to rate their experience or provide feedback about the effectiveness of the recommended poses. This data can be used to continuously improve the recommendation algorithm and user experience.\n\n* Continuous Learning:* The system can be enhanced by collecting user data over time to refine pose suggestions, improve personalization, and adjust recommendations based on individual progress or changes in user conditions.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\nThis execution flowchart outlines a system designed to recommend yoga poses based on user input and evaluate posture accuracy through skeletal structure detection. The process starts with a user interface that accepts both text and speech input. If the input is spoken, it is converted to text using Google Cloud Speech-to-Text. The processed text is then passed to Gemini 2.0 (hosted on Google Cloud Vertex AI), which generates semantic embeddings. These embeddings are used by a vector search engine to match relevant yoga pose recommendations. The metadata for these poses is stored and retrieved from Google Cloud Firestore to assist the search process. Once the recommendation is made, the user receives a suggested yoga pose. Afterward, a live webcam feed is activated to monitor the user's performance. The system then applies skeletal structure detection to evaluate whether the user is correctly performing the recommended pose. This allows for real-time feedback and improvement, enhancing the overall user experience and ensuring accuracy in yoga practice. This flow demonstrates the integration of AI and cloud services.\n\n#figure(\n  image(\"images/image001.png\", width: 70%),\n  caption: [Fig 4.1 Work Flowchart],\n  numbering: none\n)"
  },
  {
  "input": {
    "starting_page": 22,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   V  -  TESTING AND VALIDATION\n\n#table(\n  columns: (auto, 1fr, 1fr, 1fr, 1fr, 1fr),\n  [No.], [Test Scenario], [Test input], [Expected Results], [Actual Results], [Pass/Fail],\n  [1], [User has sustained a neck injury from an accident], [Give poses for neck pain (Text input)], [3 recommended yoga poses for neck pain, skeletal overlay near neck], [As  expected], [Pass],\n  [2], [User wants relaxing poses after work], [I need yoga for stress relief (Voice input via Web Speech API)], [3 calming poses, audio instructions, key point mapping on torso], [As  expected], [Pass],\n  [3], [User types a vague or unsupported query], [I feel kind of ugh], [Prompt user for clearer input], [System returns irrelevant or no poses], [Fail],\n  [4], [User provides a valid, clear input after initial failure], [Suggest yoga for anxiety], [3 relevant calming poses with benefits], [As  expected], [Pass],\n  [5], [Users webcam is on, but pose detection fails], [Poses for shoulder pain], [Pose suggestions + real-time skeletal overlay on shoulders], [Pose suggestions appear but webcam overlay does not load], [Fail],\n  [6], [User refreshes webcam after failure], [Give poses for shoulder pain], [Pose suggestions and visible shoulder landmarks], [As  expected], [Pass]\n)\n\nTable 5.1 Test cases\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n5.1 Testing Strategies\n\n5.1.1 Unit Testing\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n Google Cloud Speech-to-Text: Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n Text Processing Module: Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n Gemini 2.0 Embedding Generation: Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n Vector Search Engine: Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n Skeletal Detection Algorithm: Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n5.1.2 Integration Testing\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n5.1.3 End-to-End System Testing\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n5.2 Validation Approaches\n\n5.2.1 Functional Validation\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n5.2.2 User Feedback Validation\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n5.2.3 Performance and Load Testing\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries)."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 22\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#table(\n  columns: (0.5fr, 2fr, 2fr, 2fr, 2fr, 1fr),\n  align: horizon,\n  [*No.*], [*Test Scenario*], [*Test Input*], [*Expected Results*],[*Actual Results*], [*Pass/Fail*],\n\n  [1],\n  [User has sustained a neck injury from an accident], \n  [Give poses for neck pain (Text input)], \n  [3 recommended yoga poses for neck pain, skeletal overlay near neck],\n  [As Expected],\n  [Pass],\n\n  [2],\n  [User wants relaxing poses after work], \n  [I need yoga for stress relief (Voice input via Web Speech API)], \n  [3 calming poses, audio instructions, key point mapping on torso],\n  [As Expected],\n  [Pass],\n\n  [3],\n  [User types a vague or unsupported query], \n  [I feel kind of ugh], \n  [Prompt user for clearer input],\n  [System returns irrelevant or no poses],\n  [Fail],\n\n  [4],\n  [User provides a valid, clear input after initial failure], \n  [Suggest yoga for anxiety], \n  [3 relevant calming poses with benefits],\n  [As Expected],\n  [Pass],\n\n   [5],\n  [Users webcam is on, but pose detection fails], \n  [Poses for shoulder pain], \n  [Pose suggestions + real-time skeletal overlay on shoulders],\n  [Pose suggestions appear but webcam overlay does not load],\n  [Fail],\n\n  [6],\n  [User refreshes webcam after failure], \n  [Give poses for shoulder pain], \n  [Pose suggestions and visible shoulder landmarks],\n  [As Expected],\n  [Pass]\n)\n#text(size: 12pt, weight: \"bold\")[\n#align(center)[Table 5.1 Test Cases]]\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n#text(size: 14pt, weight: \"bold\")[5.1 Testing strategies]\n\n*5.1.1 Unit Testing*\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n* Google Cloud Speech-to-Text:* Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n* Text Processing Module:* Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n* Gemini 2.0 Embedding Generation:* Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n* Vector Search Engine:* Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n* Skeletal Detection Algorithm:* Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n*5.1.2 Integration Testing*\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n*5.1.3 End-to-End System Testing*\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n#text(size: 14pt, weight: \"bold\")[5.2 Validation Approaches]\n\n*5.2.1 Functional Validation*\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n*5.2.2 User Feedback Validation*\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n*5.2.3 Performance and Load Testing*\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries)."
  },
  {
  "input": {
    "starting_page": 44,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   V  -  TESTING AND VALIDATION\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n5.1 Testing Strategies\n\n5.1.1 Unit Testing\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n Google Cloud Speech-to-Text: Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n Text Processing Module: Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n Gemini 2.0 Embedding Generation: Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n Vector Search Engine: Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n Skeletal Detection Algorithm: Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n5.1.2 Integration Testing\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n5.1.3 End-to-End System Testing\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n5.2 Validation Approaches\n\n5.2.1 Functional Validation\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n5.2.2 User Feedback Validation\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n5.2.3 Performance and Load Testing\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries).\n\n#table(\n  columns: (auto, 1fr, 1fr, 1fr, 1fr, 1fr),\n  [No.], [Test Scenario], [Test input], [Expected Results], [Actual Results], [Pass/Fail],\n  [1], [User has sustained a neck injury from an accident], [Give poses for neck pain (Text input)], [3 recommended yoga poses for neck pain, skeletal overlay near neck], [As  expected], [Pass],\n  [2], [User wants relaxing poses after work], [I need yoga for stress relief (Voice input via Web Speech API)], [3 calming poses, audio instructions, key point mapping on torso], [As  expected], [Pass],\n  [3], [User types a vague or unsupported query], [I feel kind of ugh], [Prompt user for clearer input], [System returns irrelevant or no poses], [Fail],\n  [4], [User provides a valid, clear input after initial failure], [Suggest yoga for anxiety], [3 relevant calming poses with benefits], [As  expected], [Pass],\n  [5], [Users webcam is on, but pose detection fails], [Poses for shoulder pain], [Pose suggestions + real-time skeletal overlay on shoulders], [Pose suggestions appear but webcam overlay does not load], [Fail],\n  [6], [User refreshes webcam after failure], [Give poses for shoulder pain], [Pose suggestions and visible shoulder landmarks], [As  expected], [Pass]\n)\n\nTable 5.1 Test cases"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 44\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n#text(size: 14pt, weight: \"bold\")[5.1 Testing strategies]\n\n*5.1.1 Unit Testing*\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n* Google Cloud Speech-to-Text:* Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n* Text Processing Module:* Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n* Gemini 2.0 Embedding Generation:* Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n* Vector Search Engine:* Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n* Skeletal Detection Algorithm:* Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n*5.1.2 Integration Testing*\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n*5.1.3 End-to-End System Testing*\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n#text(size: 14pt, weight: \"bold\")[5.2 Validation Approaches]\n\n*5.2.1 Functional Validation*\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n*5.2.2 User Feedback Validation*\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n*5.2.3 Performance and Load Testing*\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries).\n\n#table(\n  columns: (0.5fr, 2fr, 2fr, 2fr, 2fr, 1fr),\n  align: horizon,\n  [*No.*], [*Test Scenario*], [*Test Input*], [*Expected Results*],[*Actual Results*], [*Pass/Fail*],\n\n  [1],\n  [User has sustained a neck injury from an accident], \n  [Give poses for neck pain (Text input)], \n  [3 recommended yoga poses for neck pain, skeletal overlay near neck],\n  [As Expected],\n  [Pass],\n\n  [2],\n  [User wants relaxing poses after work], \n  [I need yoga for stress relief (Voice input via Web Speech API)], \n  [3 calming poses, audio instructions, key point mapping on torso],\n  [As Expected],\n  [Pass],\n\n  [3],\n  [User types a vague or unsupported query], \n  [I feel kind of ugh], \n  [Prompt user for clearer input],\n  [System returns irrelevant or no poses],\n  [Fail],\n\n  [4],\n  [User provides a valid, clear input after initial failure], \n  [Suggest yoga for anxiety], \n  [3 relevant calming poses with benefits],\n  [As Expected],\n  [Pass],\n\n   [5],\n  [Users webcam is on, but pose detection fails], \n  [Poses for shoulder pain], \n  [Pose suggestions + real-time skeletal overlay on shoulders],\n  [Pose suggestions appear but webcam overlay does not load],\n  [Fail],\n\n  [6],\n  [User refreshes webcam after failure], \n  [Give poses for shoulder pain], \n  [Pose suggestions and visible shoulder landmarks],\n  [As Expected],\n  [Pass]\n)\n#text(size: 12pt, weight: \"bold\")[\n#align(center)[Table 5.1 Test Cases]]"
  },
  {
  "input": {
    "starting_page": 19,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   V  -  TESTING AND VALIDATION\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n5.1 Validation Approaches\n\n5.1.1 Unit Testing\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n Google Cloud Speech-to-Text: Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n Text Processing Module: Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n Gemini 2.0 Embedding Generation: Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n Vector Search Engine: Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n Skeletal Detection Algorithm: Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n5.1.2 Integration Testing\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n5.1.3 End-to-End System Testing\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n5.2 Testing Strategies\n\n5.2.1 Functional Validation\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n5.2.2 User Feedback Validation\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n5.2.3 Performance and Load Testing\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries).\n\n#table(\n  columns: (auto, 1fr, 1fr, 1fr, 1fr, 1fr),\n  [No.], [Test Scenario], [Test input], [Expected Results], [Actual Results], [Pass/Fail],\n  [1], [User has sustained a neck injury from an accident], [Give poses for neck pain (Text input)], [3 recommended yoga poses for neck pain, skeletal overlay near neck], [As  expected], [Pass],\n  [2], [User wants relaxing poses after work], [I need yoga for stress relief (Voice input via Web Speech API)], [3 calming poses, audio instructions, key point mapping on torso], [As  expected], [Pass],\n  [3], [User types a vague or unsupported query], [I feel kind of ugh], [Prompt user for clearer input], [System returns irrelevant or no poses], [Fail],\n  [4], [User provides a valid, clear input after initial failure], [Suggest yoga for anxiety], [3 relevant calming poses with benefits], [As  expected], [Pass],\n  [5], [Users webcam is on, but pose detection fails], [Poses for shoulder pain], [Pose suggestions + real-time skeletal overlay on shoulders], [Pose suggestions appear but webcam overlay does not load], [Fail],\n  [6], [User refreshes webcam after failure], [Give poses for shoulder pain], [Pose suggestions and visible shoulder landmarks], [As  expected], [Pass]\n)\n\nTable 5.1 Test cases"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 19\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n#text(size: 14pt, weight: \"bold\")[5.1 Validation Approaches]\n\n*5.1.1 Unit Testing*\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n* Google Cloud Speech-to-Text:* Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n* Text Processing Module:* Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n* Gemini 2.0 Embedding Generation:* Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n* Vector Search Engine:* Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n* Skeletal Detection Algorithm:* Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n*5.1.2 Integration Testing*\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n*5.1.3 End-to-End System Testing*\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n#text(size: 14pt, weight: \"bold\")[5.2 Testing Strategies]\n\n*5.2.1 Functional Validation*\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n*5.2.2 User Feedback Validation*\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n*5.2.3 Performance and Load Testing*\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries).\n\n#table(\n  columns: (0.5fr, 2fr, 2fr, 2fr, 2fr, 1fr),\n  align: horizon,\n  [*No.*], [*Test Scenario*], [*Test Input*], [*Expected Results*],[*Actual Results*], [*Pass/Fail*],\n\n  [1],\n  [User has sustained a neck injury from an accident], \n  [Give poses for neck pain (Text input)], \n  [3 recommended yoga poses for neck pain, skeletal overlay near neck],\n  [As Expected],\n  [Pass],\n\n  [2],\n  [User wants relaxing poses after work], \n  [I need yoga for stress relief (Voice input via Web Speech API)], \n  [3 calming poses, audio instructions, key point mapping on torso],\n  [As Expected],\n  [Pass],\n\n  [3],\n  [User types a vague or unsupported query], \n  [I feel kind of ugh], \n  [Prompt user for clearer input],\n  [System returns irrelevant or no poses],\n  [Fail],\n\n  [4],\n  [User provides a valid, clear input after initial failure], \n  [Suggest yoga for anxiety], \n  [3 relevant calming poses with benefits],\n  [As Expected],\n  [Pass],\n\n   [5],\n  [Users webcam is on, but pose detection fails], \n  [Poses for shoulder pain], \n  [Pose suggestions + real-time skeletal overlay on shoulders],\n  [Pose suggestions appear but webcam overlay does not load],\n  [Fail],\n\n  [6],\n  [User refreshes webcam after failure], \n  [Give poses for shoulder pain], \n  [Pose suggestions and visible shoulder landmarks],\n  [As Expected],\n  [Pass]\n)\n#text(size: 12pt, weight: \"bold\")[\n#align(center)[Table 5.1 Test Cases]]"
  },
  {
  "input": {
    "starting_page": 50,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   V  -  TESTING AND VALIDATION\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n5.1 Testing Strategies\n\n5.1.1 Integration Testing\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n Google Cloud Speech-to-Text: Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n Text Processing Module: Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n Gemini 2.0 Embedding Generation: Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n Vector Search Engine: Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n Skeletal Detection Algorithm: Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n5.1.2 Unit Testing\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n5.1.3 End-to-End System Testing\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n5.2 Validation Approaches\n\n5.2.1 User Feedback Validation\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n5.2.2 Functional Validation\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n5.2.3 Performance and Load Testing\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries).\n\n#table(\n  columns: (auto, 1fr, 1fr, 1fr, 1fr, 1fr),\n  [No.], [Test Scenario], [Test input], [Expected Results], [Actual Results], [Pass/Fail],\n  [1], [User has sustained a neck injury from an accident], [Give poses for neck pain (Text input)], [3 recommended yoga poses for neck pain, skeletal overlay near neck], [As  expected], [Pass],\n  [2], [User wants relaxing poses after work], [I need yoga for stress relief (Voice input via Web Speech API)], [3 calming poses, audio instructions, key point mapping on torso], [As  expected], [Pass],\n  [3], [User types a vague or unsupported query], [I feel kind of ugh], [Prompt user for clearer input], [System returns irrelevant or no poses], [Fail],\n  [4], [User provides a valid, clear input after initial failure], [Suggest yoga for anxiety], [3 relevant calming poses with benefits], [As  expected], [Pass],\n  [5], [Users webcam is on, but pose detection fails], [Poses for shoulder pain], [Pose suggestions + real-time skeletal overlay on shoulders], [Pose suggestions appear but webcam overlay does not load], [Fail],\n  [6], [User refreshes webcam after failure], [Give poses for shoulder pain], [Pose suggestions and visible shoulder landmarks], [As  expected], [Pass]\n)\n\nTable 5.1 Test cases"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 50\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n#text(size: 14pt, weight: \"bold\")[5.1 Testing strategies]\n\n*5.1.1 Integration Testing*\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n*5.1.2 Unit Testing*\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n* Google Cloud Speech-to-Text:* Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n* Text Processing Module:* Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n* Gemini 2.0 Embedding Generation:* Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n* Vector Search Engine:* Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n* Skeletal Detection Algorithm:* Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n*5.1.3 End-to-End System Testing*\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n#text(size: 14pt, weight: \"bold\")[5.2 Validation Approaches]\n\n*5.2.1 User Feedback Validation*\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n*5.2.1 Functional Validation*\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n*5.2.3 Performance and Load Testing*\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries).\n\n#table(\n  columns: (0.5fr, 2fr, 2fr, 2fr, 2fr, 1fr),\n  align: horizon,\n  [*No.*], [*Test Scenario*], [*Test Input*], [*Expected Results*],[*Actual Results*], [*Pass/Fail*],\n\n  [1],\n  [User has sustained a neck injury from an accident], \n  [Give poses for neck pain (Text input)], \n  [3 recommended yoga poses for neck pain, skeletal overlay near neck],\n  [As Expected],\n  [Pass],\n\n  [2],\n  [User wants relaxing poses after work], \n  [I need yoga for stress relief (Voice input via Web Speech API)], \n  [3 calming poses, audio instructions, key point mapping on torso],\n  [As Expected],\n  [Pass],\n\n  [3],\n  [User types a vague or unsupported query], \n  [I feel kind of ugh], \n  [Prompt user for clearer input],\n  [System returns irrelevant or no poses],\n  [Fail],\n\n  [4],\n  [User provides a valid, clear input after initial failure], \n  [Suggest yoga for anxiety], \n  [3 relevant calming poses with benefits],\n  [As Expected],\n  [Pass],\n\n   [5],\n  [Users webcam is on, but pose detection fails], \n  [Poses for shoulder pain], \n  [Pose suggestions + real-time skeletal overlay on shoulders],\n  [Pose suggestions appear but webcam overlay does not load],\n  [Fail],\n\n  [6],\n  [User refreshes webcam after failure], \n  [Give poses for shoulder pain], \n  [Pose suggestions and visible shoulder landmarks],\n  [As Expected],\n  [Pass]\n)\n#text(size: 12pt, weight: \"bold\")[\n#align(center)[Table 5.1 Test Cases]]"
  },
  {
  "input": {
    "starting_page": 5,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   V  -  TESTING AND VALIDATION\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n5.1 Testing Strategies\n\n#table(\n  columns: (auto, 1fr, 1fr, 1fr, 1fr, 1fr),\n  [No.], [Test Scenario], [Test input], [Expected Results], [Actual Results], [Pass/Fail],\n  [1], [User has sustained a neck injury from an accident], [Give poses for neck pain (Text input)], [3 recommended yoga poses for neck pain, skeletal overlay near neck], [As  expected], [Pass],\n  [2], [User wants relaxing poses after work], [I need yoga for stress relief (Voice input via Web Speech API)], [3 calming poses, audio instructions, key point mapping on torso], [As  expected], [Pass],\n  [3], [User types a vague or unsupported query], [I feel kind of ugh], [Prompt user for clearer input], [System returns irrelevant or no poses], [Fail],\n  [4], [User provides a valid, clear input after initial failure], [Suggest yoga for anxiety], [3 relevant calming poses with benefits], [As  expected], [Pass],\n  [5], [Users webcam is on, but pose detection fails], [Poses for shoulder pain], [Pose suggestions + real-time skeletal overlay on shoulders], [Pose suggestions appear but webcam overlay does not load], [Fail],\n  [6], [User refreshes webcam after failure], [Give poses for shoulder pain], [Pose suggestions and visible shoulder landmarks], [As  expected], [Pass]\n)\n\nTable 5.1 Test cases\n\n5.1.1 Unit Testing\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n Google Cloud Speech-to-Text: Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n Text Processing Module: Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n Gemini 2.0 Embedding Generation: Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n Vector Search Engine: Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n Skeletal Detection Algorithm: Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n5.1.2 Integration Testing\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n5.1.3 End-to-End System Testing\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n5.2 Validation Approaches\n\n5.2.1 Functional Validation\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n5.2.2 User Feedback Validation\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n5.2.3 Performance and Load Testing\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries)."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 5\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nTesting and validation are critical to ensuring the reliability, accuracy, and overall performance of the yoga recommendation and posture detection system. Given the architecture that includes natural language processing, vector-based search,  metadata handling, and computer vision components, a multi-layered testing strategy must be adopted. This includes unit testing, integration testing, user interface testing, system validation, and real-world performance evaluation under variable conditions.\n\n#text(size: 14pt, weight: \"bold\")[5.1 Testing strategies]\n\n#table(\n  columns: (0.5fr, 2fr, 2fr, 2fr, 2fr, 1fr),\n  align: horizon,\n  [*No.*], [*Test Scenario*], [*Test Input*], [*Expected Results*],[*Actual Results*], [*Pass/Fail*],\n\n  [1],\n  [User has sustained a neck injury from an accident], \n  [Give poses for neck pain (Text input)], \n  [3 recommended yoga poses for neck pain, skeletal overlay near neck],\n  [As Expected],\n  [Pass],\n\n  [2],\n  [User wants relaxing poses after work], \n  [I need yoga for stress relief (Voice input via Web Speech API)], \n  [3 calming poses, audio instructions, key point mapping on torso],\n  [As Expected],\n  [Pass],\n\n  [3],\n  [User types a vague or unsupported query], \n  [I feel kind of ugh], \n  [Prompt user for clearer input],\n  [System returns irrelevant or no poses],\n  [Fail],\n\n  [4],\n  [User provides a valid, clear input after initial failure], \n  [Suggest yoga for anxiety], \n  [3 relevant calming poses with benefits],\n  [As Expected],\n  [Pass],\n\n   [5],\n  [Users webcam is on, but pose detection fails], \n  [Poses for shoulder pain], \n  [Pose suggestions + real-time skeletal overlay on shoulders],\n  [Pose suggestions appear but webcam overlay does not load],\n  [Fail],\n\n  [6],\n  [User refreshes webcam after failure], \n  [Give poses for shoulder pain], \n  [Pose suggestions and visible shoulder landmarks],\n  [As Expected],\n  [Pass]\n)\n#text(size: 12pt, weight: \"bold\")[\n#align(center)[Table 5.1 Test Cases]]\n\n*5.1.1 Unit Testing*\n\nUnit testing will be applied to each individual module to ensure that they perform as expected in isolation.\n\n* Google Cloud Speech-to-Text:* Tested using pre-recorded speech samples with varying accents, speeds, and background noise to validate transcription accuracy.\n\n* Text Processing Module:* Ensures consistent formatting and cleaning of input (removing noise like fillers or punctuation).\n\n* Gemini 2.0 Embedding Generation:* Validate whether different textual inputs produce embeddings that correlate correctly in the semantic space.\n\n* Vector Search Engine:* Verify that similar queries retrieve consistent and relevant pose metadata from the Firestore backend.\n\n* Skeletal Detection Algorithm:* Validated using test datasets (e.g., MPII Human Pose Dataset or COCO) to ensure key points are accurately detected.\n\n*5.1.2 Integration Testing*\n\nThis involves testing the interaction between modules:\n\n From input capture (text/speech) through embedding and vector search to pose retrieval.\n\n From recommendation output to webcam feed capture and skeletal detection.\n\n Test scenarios simulate real-time input flows, ensuring that communication across APIs and cloud services is seamless and  fault-tolerant .\n\n*5.1.3 End-to-End System Testing*\n\n Full workflow testing using real user inputs:\n\n User gives a speech/text command such as I want a beginner yoga pose to relax.\n\n The system processes it, returns a recommendation.\n\n The user performs the pose; the webcam activates and evaluates their performance.\n\n Validation is performed by comparing system outputs against expected outcomes (e.g., was the pose relevant, was the posture analysis accurate, etc.).\n\n#text(size: 14pt, weight: \"bold\")[5.2 Validation Approaches]\n\n*5.2.1 Functional Validation*\n\nEach component is validated against its expected functionality:\n\n Speech-to-text must meet an accuracy threshold of 90%+ in ideal conditions.\n\n Gemini-generated embeddings must map semantically similar inputs close in vector space.\n\n Pose recommendations should be contextually relevant to user needs (e.g., level, goal, duration).\n\n*5.2.2 User Feedback Validation*\n\nA group of beta testers (e.g., yoga practitioners or instructors) is involved  in :\n\n Evaluate the relevance and quality of the recommendations.\n\n Test the skeletal detection accuracy during actual performance.\n\n Quantitative feedback (via rating scales) and qualitative feedback (written suggestions) are collected for iterative improvement.\n\n*5.2.3 Performance and Load Testing*\n\n System performance under stress is tested:\n\n Concurrent user inputs (simulating multiple users).\n\n Webcam frame rates and latency during live analysis.\n\n Google Cloud services response times (Speech-to-Text, Firestore queries)."
  },
  {
  "input": {
    "starting_page": 22,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VI  -  RESULT AND DISCUSSION\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\nYoga Pose Recommendation Engine\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 6.1 Search for Yoga pose]\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\nReal-Time Pose Detection\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 6.2 Yoga pose performance and detection]\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 22\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\n*Yoga Pose Recommendation Engine*\n\n#figure(\n  image(\"images/image002.jpg\", width: 70%),\n  caption: [Fig 6.1 Search for Yoga pose],\n  numbering: none\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\n#pagebreak()\n*Real-Time Pose Detection*\n\n#figure(\n  image(\"images/image003.jpg\", width: 70%),\n  caption: [Fig 6.2 Yoga pose performance and detection],\n  numbering: none\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  {
  "input": {
    "starting_page": 2,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VI  -  RESULT AND DISCUSSION\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\nYoga Pose Recommendation Engine\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 6.2 Yoga pose performance and detection]\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\nReal-Time Pose Detection\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 6.1 Search for Yoga pose]\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 2\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\n*Yoga Pose Recommendation Engine*\n\n#figure(\n  image(\"images/image003.jpg\", width: 70%),\n  caption: [Fig 6.2 Yoga pose performance and detection],\n  numbering: none\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\n#pagebreak()\n*Real-Time Pose Detection*\n\n#figure(\n  image(\"images/image002.jpg\", width: 70%),\n  caption: [Fig 6.1 Search for Yoga pose],\n  numbering: none\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  {
  "input": {
    "starting_page": 15,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VI  -  RESULT AND DISCUSSION\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\nYoga Pose Recommendation Engine\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 6.1 Search for Yoga pose]\n)\n\nReal-Time Pose Detection\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 6.2 Yoga pose performance and detection]\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 15\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\n*Yoga Pose Recommendation Engine*\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\n#figure(\n  image(\"images/image002.jpg\", width: 70%),\n  caption: [Fig 6.1 Search for Yoga pose],\n  numbering: none\n)\n\n#pagebreak()\n*Real-Time Pose Detection*\n\n#figure(\n  image(\"images/image003.jpg\", width: 70%),\n  caption: [Fig 6.2 Yoga pose performance and detection],\n  numbering: none\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  {
  "input": {
    "starting_page": 30,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VI  -  RESULT AND DISCUSSION\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\nYoga Pose Recommendation Engine\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 6.1 Search for Yoga pose]\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\nReal-Time Pose Detection\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame.\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 6.2 Yoga pose performance and detection]\n)"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 30\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\n*Yoga Pose Recommendation Engine*\n\n#figure(\n  image(\"images/image002.jpg\", width: 70%),\n  caption: [Fig 6.1 Search for Yoga pose],\n  numbering: none\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\n#pagebreak()\n*Real-Time Pose Detection*\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame.\n\n#figure(\n  image(\"images/image003.jpg\", width: 70%),\n  caption: [Fig 6.2 Yoga pose performance and detection],\n  numbering: none\n)"
  },
  {
  "input": {
    "starting_page": 38,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VI  -  RESULT AND DISCUSSION\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\nReal-Time Pose Detection\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 6.1 Search for Yoga pose]\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\nYoga Pose Recommendation Engine\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 6.2 Yoga pose performance and detection]\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 38\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe Yoga Pose Recommender web application was successfully implemented and tested. The system consists of two primary modules: a real-time pose detection system and an intelligent yoga pose recommendation engine. These modules were integrated into a single user-friendly interface, which was deployed as a responsive web app.\n\n*Real-Time Pose Detection*\n\n#figure(\n  image(\"images/image002.jpg\", width: 70%),\n  caption: [Fig 6.1 Search for Yoga pose],\n  numbering: none\n)\n\nIn Fig 6.1, the second feature of the application is illustrated  a search-based yoga pose recommendation. The user inputs natural language queries such as \"Which pose helps to strengthen my legs?\" The application interprets the intent using an NLP model and returns a list of yoga poses relevant to the query. In this case, it returned poses like Warrior II, Lunge Pose, and Low Lunge Pose, which are commonly used to build leg strength and balance. The recommendation engine functions by mapping keywords and semantic meanings from the users input to a curated database of yoga poses with associated benefits. The responses are displayed clearly with pose names and visual thumbnails, making it easier for users to understand and select poses based on their fitness goals.\n\n#pagebreak()\n*Yoga Pose Recommendation Engine*\n#figure(\n  image(\"images/image003.jpg\", width: 70%),\n  caption: [Fig 6.2 Yoga pose performance and detection],\n  numbering: none\n)\n\nAs shown in Screenshot 1, the application captures live video from the user's webcam and overlays skeletal  keypoints  and connections using a pose detection model. The system effectively identifies the users posture and compares it against pre-defined yoga poses. When a user performs a pose such as Warrior II, the application detects the alignment of limbs and body angles and confirms the match if the  keypoints  closely resemble the reference pose. The visual overlay of joints and limbs helps the user understand their pose accuracy in real-time. This component demonstrated high responsiveness and worked well under good lighting conditions with the subject clearly visible. However, minor inaccuracies were observed when body parts were  occluded  or the subject was partially out of frame."
  },
  {
  "input": {
    "starting_page": 7,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VII  -  CONCLUSION AND FUTURE SCOPE\n\n7.1 Conclusion\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space.\n\n7.2 Future Scope\n\n1. Multi-language Support: Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n2. Real-time Feedback with Pose Correction Tips: Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n3. Personalized Practice Plans: Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n4. Integration with Wearables: Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]\n\n  #v(1fr)\n]\n\n#let starting_page = 7\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[7.1 Conclusion]\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space.\n\n#text(size: 14pt, weight: \"bold\")[7.2. Future Scope]\n\n*1. Multi-language Support:* Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n#v(1em)\n*2. Real-time Feedback with Pose Correction Tips:* Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n#v(1em)\n*3. Personalized Practice Plans:* Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n#v(1em)\n*4. Integration with Wearables:* Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience."
  },
  {
  "input": {
    "starting_page": 30,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VII  -  CONCLUSION AND FUTURE SCOPE\n\n7.1 Future Scope\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space.\n\n7.2 Conclusion\n\n1. Multi-language Support: Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n2. Real-time Feedback with Pose Correction Tips: Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n3. Personalized Practice Plans: Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n4. Integration with Wearables: Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]\n\n  #v(1fr)\n]\n\n#let starting_page = 30\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[7.1 Conclusion]\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space.\n\n#text(size: 14pt, weight: \"bold\")[7.2. Future Scope]\n\n*1. Multi-language Support:* Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n#v(1em)\n*2. Real-time Feedback with Pose Correction Tips:* Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n#v(1em)\n*3. Personalized Practice Plans:* Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n#v(1em)\n*4. Integration with Wearables:* Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience.\n"
  },
  {
  "input": {
    "starting_page": 11,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VII  -  CONCLUSION AND FUTURE SCOPE\n\n7.1 Future Scope\n\n1. Multi-language Support: Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n2. Real-time Feedback with Pose Correction Tips: Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n3. Personalized Practice Plans: Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n4. Integration with Wearables: Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience.\n\n7.2 Conclusion\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]\n\n  #v(1fr)\n]\n\n#let starting_page = 11\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[7.1 Future Scope]\n\n*1. Multi-language Support:* Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n#v(1em)\n*2. Real-time Feedback with Pose Correction Tips:* Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n#v(1em)\n*3. Personalized Practice Plans:* Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n#v(1em)\n*4. Integration with Wearables:* Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience.\n\n#text(size: 14pt, weight: \"bold\")[7.2. Conclusion]\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space."
  },
  {
  "input": {
    "starting_page": 56,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VII  -  CONCLUSION AND FUTURE SCOPE\n\n7.1 Conclusion\n\n1. Multi-language Support: Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n2. Real-time Feedback with Pose Correction Tips: Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n3. Personalized Practice Plans: Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n4. Integration with Wearables: Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience.\n\n7.2 Future Scope\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]\n\n  #v(1fr)\n]\n\n#let starting_page = 56\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[7.1 Conclusion]\n\n*1. Multi-language Support:* Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n#v(1em)\n*2. Real-time Feedback with Pose Correction Tips:* Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n#v(1em)\n*3. Personalized Practice Plans:* Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation.\n\n#v(1em)\n*4. Integration with Wearables:* Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience.\n\n#text(size: 14pt, weight: \"bold\")[7.2. Future Scope]\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space."
  },
  {
  "input": {
    "starting_page": 25,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Yoga Pose Recommender System",
    "contents_txt": "CHAPTER   VII  -  CONCLUSION AND FUTURE SCOPE\n\n7.1 Conclusion\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space.\n\n7.2 Future Scope\n\n1. Multi-language Support: Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n2. Integration with Wearables: Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience.\n\n3. Real-time Feedback with Pose Correction Tips: Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n4. Personalized Practice Plans: Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]\n\n  #v(1fr)\n]\n\n#let starting_page = 25\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Yoga Pose Recommender System\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION AND FUTURE SCOPE]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[7.1 Conclusion]\n\nThe yoga pose recommendation and feedback system leverages a powerful combination of natural language processing, semantic search, cloud infrastructure, and real-time computer vision to deliver an interactive wellness experience. Through rigorous testing and validation methods  including unit, integration, system, and real-world scenario testing  the system has demonstrated reliability and accuracy in translating user intent into meaningful pose recommendations and assessing posture via skeletal analysis. Furthermore, the integration of cloud-based AI services like Google Cloud Speech-to-Text, Gemini 2.0, and Firestore ensures the system is scalable and adaptable to different  users and environments. By continuously validating the system against real user feedback and standardized benchmarks, the platform remains robust, user-centered, and performance optimized. This foundation sets the stage for future enhancements and broader applications in  the digital  fitness and health tech space.\n\n#text(size: 14pt, weight: \"bold\")[7.2. Future Scope]\n\n*1. Multi-language Support:* Incorporate multilingual capabilities to accept and process speech/text inputs in various languages, expanding accessibility across diverse user bases globally.\n\n#v(1em)\n*2. Integration with Wearables:* Connect with wearable fitness devices (e.g., smartwatches, posture trackers) to enhance feedback accuracy and provide biometric monitoring during yoga sessions. The outlined future enhancements aim to make the system more intelligent, accessible, and engaging for a wider audience.\n\n#v(1em)\n*3. Real-time Feedback with Pose Correction Tips:* Enhance the skeletal detection module to not only identify incorrect posture but also provide actionable visual or audio feedback for correcting the pose in real time.\n\n#v(1em)\n*4. Personalized Practice Plans:* Use user history, preferences, and performance data to generate customized yoga routines or weekly plans tailored to individual goals such as flexibility, strength, or relaxation."
  },
  {
  "input": {
    "starting_page": 1,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":  "CHAPTER- I  :  Introduction\n\n1.1. Statement of the Problem\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n1.2. Objective of the Project\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\nPredict Sleep  Apnea :\n\nAnalyze  Sleep Data: Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea .\n\nContinuous Monitoring: Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies.\n\nSOS Alert System for Heart Rate Drops:\n\nHeart Rate Monitoring: Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\nThreshold-Based Alerts: Establish threshold values for heart rate drops that would trigger an emergency alert.\n\nAutomated SOS Alerts: Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 1\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Statement of the Problem]\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Objective of the Project]\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\n*Predict Sleep  Apnea :*\n\n*Analyze  Sleep Data:* Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea.\n\n*Continuous Monitoring:* Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies.\n\n*SOS Alert System for Heart Rate Drops:*\n\n*Heart Rate Monitoring:* Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\n*Threshold-Based Alerts:* Establish threshold values for heart rate drops that would trigger an emergency alert.\n\n*Automated SOS Alerts:* Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events."
  },
  {
  "input": {
    "starting_page": 43,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- I  :  Introduction\n\n1.1. Objective of the Project\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n1.1. Statement of the Problem\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\nPredict Sleep  Apnea :\n\nAnalyze  Sleep Data: Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea .\n\nContinuous Monitoring: Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies.\n\nSOS Alert System for Heart Rate Drops:\n\nHeart Rate Monitoring: Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\nThreshold-Based Alerts: Establish threshold values for heart rate drops that would trigger an emergency alert.\n\nAutomated SOS Alerts: Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 43\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Objective of the Project]\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Statement of the Problem]\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\n*Predict Sleep  Apnea :*\n\n*Analyze  Sleep Data:* Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea.\n\n*Continuous Monitoring:* Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies.\n\n*SOS Alert System for Heart Rate Drops:*\n\n*Heart Rate Monitoring:* Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\n*Threshold-Based Alerts:* Establish threshold values for heart rate drops that would trigger an emergency alert.\n\n*Automated SOS Alerts:* Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events."
  },
  {
  "input": {
    "starting_page": 32,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- I  :  Introduction\n\n1.1. Objective of the Project\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\nPredict Sleep  Apnea :\n\nAnalyze  Sleep Data: Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea .\n\nContinuous Monitoring: Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies.\n\nSOS Alert System for Heart Rate Drops:\n\nHeart Rate Monitoring: Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\nThreshold-Based Alerts: Establish threshold values for heart rate drops that would trigger an emergency alert.\n\nAutomated SOS Alerts: Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events.\n\n1.2. Statement of the Problem\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 32\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Objective of the Project]\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\n*Predict Sleep  Apnea :*\n\n*Analyze  Sleep Data:* Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea.\n\n*Continuous Monitoring:* Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies.\n\n*SOS Alert System for Heart Rate Drops:*\n\n*Heart Rate Monitoring:* Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\n*Threshold-Based Alerts:* Establish threshold values for heart rate drops that would trigger an emergency alert.\n\n*Automated SOS Alerts:* Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[1.2 Statement of the Problem]\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops."
  },
  {
  "input": {
    "starting_page": 50,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- I  :  Introduction\n\n1.1. Statement of the Problem\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n1.2. Objective of the Project\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\nSOS Alert System for Heart Rate Drops:\n\nHeart Rate Monitoring: Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\nThreshold-Based Alerts: Establish threshold values for heart rate drops that would trigger an emergency alert.\n\nAutomated SOS Alerts: Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events.\n\nPredict Sleep  Apnea :\n\nAnalyze  Sleep Data: Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea .\n\nContinuous Monitoring: Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 50\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Statement of the Problem]\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Objective of the Project]\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\n*SOS Alert System for Heart Rate Drops:*\n\n*Heart Rate Monitoring:* Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\n*Threshold-Based Alerts:* Establish threshold values for heart rate drops that would trigger an emergency alert.\n\n*Automated SOS Alerts:* Develop a mechanism to automatically send SOS alerts to pre-defined emergency contacts (such as family members or healthcare providers) when a critical heart rate drop is detected.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events.\n\n*Predict Sleep  Apnea :*\n\n*Analyze  Sleep Data:* Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea.\n\n*Continuous Monitoring:* Enable continuous and non-intrusive monitoring through wearable devices, providing real-time analysis and prediction of sleep  apnea  without the need for clinical sleep studies."
  },
  {
  "input": {
    "starting_page": 23,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- I  :  Introduction\n\n1.1. Statement of the Problem\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n1.2. Objective of the Project\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\nPredict Sleep  Apnea :\n\nAnalyze  Sleep Data: Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea .\n\nSOS Alert System for Heart Rate Drops:\n\nHeart Rate Monitoring: Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\nThreshold-Based Alerts: Establish threshold values for heart rate drops that would trigger an emergency alert.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- I]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[INTRODUCTION]\n\n  #v(1fr)\n]\n\n#let starting_page = 23\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - I]\n#align(center)[#text(size: 16pt, weight: \"bold\")[INTRODUCTION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[1.1 Statement of the Problem]\n\nSleep  apnea  is a common sleep disorder characterized by repeated interruptions in breathing during sleep, leading to poor sleep quality and various health issues. Traditional methods for diagnosing sleep  apnea  often require expensive and uncomfortable overnight sleep studies in a clinical setting. Additionally, sudden drops in heart rate during sleep can lead to heart attacks, necessitating timely intervention. There is a need for a cost-effective, non-intrusive, and continuous monitoring system that can predict sleep  apnea  and alert emergency contacts in case of critical heart rate drops.\n\n#text(size: 14pt, weight: \"bold\")[1.2. Objective of the Project]\n\nThe primary objective of this project is to develop an intelligent system that can predict sleep  apnea  and prevent heart attacks by leveraging data collected from smart watches. This system aims to address the following specific goals:\n\n*Predict Sleep  Apnea :*\n\n*Analyze  Sleep Data:* Utilize sleep data, particularly focusing on sleep awakenings and sleep cycles, to identify patterns that may indicate sleep  apnea.\n\n*SOS Alert System for Heart Rate Drops:*\n\n*Heart Rate Monitoring:* Continuously monitor the heart rate of the user during sleep to detect any significant drops that may indicate a potential heart attack.\n\n*Threshold-Based Alerts:* Establish threshold values for heart rate drops that would trigger an emergency alert.\n\nBy achieving these objectives, the project aims to provide a cost-effective, non-intrusive solution for early detection of sleep  apnea  and timely intervention for potential heart attacks. This can significantly improve the quality of life for individuals suffering from sleep disorders and reduce the risk of life-threatening cardiac events."
  },
  {
  "input": {
    "starting_page": 6,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- II  :  Literature  Survey\n\nWearable devices in medical Internet of Things: Scientific research and commercially available devices\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring.\n\nAutomated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\nHeart rate variability analysis with photoplethysmography in critical care medicine\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\nReal-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\nWearable sensors for remote health monitoring\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\nEvaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\nWearable Heart Rate Sensors: Technologies, Applications, and Opportunities\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 6\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Wearable devices in medical Internet of Things: Scientific research and commercially available devices*\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring.\n\n*Automated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms*\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\n*Heart rate variability analysis with photoplethysmography in critical care medicine*\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\n*Real-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device*\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\n#pagebreak()\n*Wearable sensors for remote health monitoring*\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\n*Evaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection*\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\n*Wearable Heart Rate Sensors: Technologies, Applications, and Opportunities*\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring."
  },
  {
  "input": {
    "starting_page": 16,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- II  :  Literature  Survey\n\nAutomated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\nHeart rate variability analysis with photoplethysmography in critical care medicine\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\nReal-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\nWearable sensors for remote health monitoring\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\nEvaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\nWearable Heart Rate Sensors: Technologies, Applications, and Opportunities\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring.\n\nWearable devices in medical Internet of Things: Scientific research and commercially available devices\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 16\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Automated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms*\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\n*Heart rate variability analysis with photoplethysmography in critical care medicine*\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\n*Real-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device*\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\n*Wearable sensors for remote health monitoring*\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\n#pagebreak()\n*Evaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection*\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\n*Wearable Heart Rate Sensors: Technologies, Applications, and Opportunities*\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring.\n\n*Wearable devices in medical Internet of Things: Scientific research and commercially available devices*\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring."
  },
  {
  "input": {
    "starting_page": 26,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- II  :  Literature  Survey\n\nWearable devices in medical Internet of Things: Scientific research and commercially available devices\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring.\n\nAutomated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\nHeart rate variability analysis with photoplethysmography in critical care medicine\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\nReal-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\nWearable sensors for remote health monitoring\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\nEvaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 26\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Wearable devices in medical Internet of Things: Scientific research and commercially available devices*\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring.\n\n*Automated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms*\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\n*Heart rate variability analysis with photoplethysmography in critical care medicine*\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\n*Real-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device*\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\n#pagebreak()\n*Wearable sensors for remote health monitoring*\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\n*Evaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection*\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening."
  },
  {
  "input": {
    "starting_page": 36,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- II  :  Literature  Survey\n\nWearable devices in medical Internet of Things: Scientific research and commercially available devices\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring.\n\nAutomated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\nReal-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\nWearable sensors for remote health monitoring\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\nEvaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\nWearable Heart Rate Sensors: Technologies, Applications, and Opportunities\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring.\n\nHeart rate variability analysis with photoplethysmography in critical care medicine\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 36\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Wearable devices in medical Internet of Things: Scientific research and commercially available devices*\n\nAuthors: S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak\n\nGist: This paper reviews the landscape of wearable devices within the medical IoT framework, discussing both research advancements and commercial products. It highlights challenges and future directions in integrating these devices for health monitoring.\n\n*Automated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms*\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\n*Real-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device*\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\n*Wearable sensors for remote health monitoring*\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\n#pagebreak()\n*Evaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection*\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\n*Wearable Heart Rate Sensors: Technologies, Applications, and Opportunities*\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring.\n\n*Heart rate variability analysis with photoplethysmography in critical care medicine*\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management."
  },
  {
  "input": {
    "starting_page": 46,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- II  :  Literature  Survey\n\nAutomated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\nHeart rate variability analysis with photoplethysmography in critical care medicine\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\nReal-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\nWearable sensors for remote health monitoring\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\nEvaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\nWearable Heart Rate Sensors: Technologies, Applications, and Opportunities\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- II]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[LITERATURE SURVEY]\n\n  #v(1fr)\n]\n\n#let starting_page = 46\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - II]\n#align(center)[#text(size: 16pt, weight: \"bold\")[LITERATURE SURVEY]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Automated detection of obstructive sleep  apnea  using a smart watch and machine learning algorithms*\n\nAuthors: Y. Zhang, J. Ren, Y. Liu, and M. Guo\n\nGist: This study introduces a machine learning-based system for detecting sleep  apnea  using smart watch data. The results demonstrate high accuracy in identifying  apnea  events, showcasing the potential of smart watches in sleep disorder diagnostics.\n\n*Heart rate variability analysis with photoplethysmography in critical care medicine*\n\nAuthors: A.  Elgendi , R. Norton, D. Brearley, R.  Abolmaesumi , and M. Menon\n\nGist: This research evaluates photoplethysmography (PPG) for heart rate variability (HRV) analysis in critical care settings. The study supports the reliability of PPG in providing continuous HRV monitoring, crucial for patient management.\n\n*Real-time heart rate variability monitoring using a wrist-type photo  plethysmographic  device*\n\nAuthors: F. Karlen, K. Kobayashi, M. D. Christian, T. J. Tenhunen, and J. J. Smith\n\nGist: This study explores the use of wrist-worn PPG devices for real-time HRV monitoring. It confirms the feasibility of using such devices for continuous cardiac health assessment outside clinical environments.\n\n*Wearable sensors for remote health monitoring*\n\nAuthors: B. Gyselinckx, R. Jafari, C. Van Hoof\n\nGist: This paper reviews the advancements in wearable sensor technology for remote health monitoring. It covers design challenges, potential applications, and the impact of these technologies on personalized healthcare.\n\n#pagebreak()\n*Evaluation of a Consumer Wearable Sleep Tracker for Sleep  Apnea  Detection*\n\nAuthors: A. Walch, D. B. Eichstaedt, M. Lachmann, S. Unger\n\nGist: This study evaluates the effectiveness of a consumer-grade wearable device in detecting sleep  apnea . The results indicate that certain wearable devices can provide reliable sleep  apnea  screening.\n\n*Wearable Heart Rate Sensors: Technologies, Applications, and Opportunities*\n\nAuthors: D. Di Rienzo, L. Rizzo, P.  Parati\n\nGist: This review paper discusses the various technologies and applications of wearable heart rate sensors. It highlights the opportunities and challenges in integrating these sensors for continuous health monitoring."
  },
  {
  "input": {
    "starting_page": 8,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- III System Requirement  Specification\n\n3.1 Software Requirement\n\n3.1.1 Visual Studio  Code\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n3.1.2 Google Sheets\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n3.1.3 Python Modules\n\nPandas\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\n\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\nNumPy\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\n\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\nMatplotlib/Seaborn\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\n\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\nsmtplib\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\n\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\n\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\nemail.mime  ( email.mime.multipart  and  email.mime.text )\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\nStreamlit\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\n\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n3.1.4  Data set for Analysis\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n\n3.2 Hardware  Requirement\n\nOperating System: Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\nProcessor (CPU): A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\nLibraries and Frameworks: If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\nGraphical User Interfaces (GUIs): If your program has a graphical interface, it might require slightly better hardware for smooth performance"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 8\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Software Requirement]\n\n*3.1.1 Visual Studio  Code*\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n*3.1.2 Google Sheets*\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n*3.1.3 Python Modules*\n\n*Pandas*\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\n*NumPy*\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\n*Matplotlib/Seaborn*\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\n*smtplib*\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\n*email.mime  ( email.mime.multipart  and  email.mime.text )*\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\n*Streamlit*\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n*3.1.4  Data set for Analysis*\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Hardware  Requirement]\n\n*Operating System:* Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\n*Processor (CPU):* A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\n*Libraries and Frameworks:* If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\n*Graphical User Interfaces (GUIs):* If your program has a graphical interface, it might require slightly better hardware for smooth performance\n"
  },
  {
    "input": {
      "starting_page": 18,
      "academic_year": "2025-26",
      "department": "B.E/Dept of CSE/BNMIT",
      "project_title": "Sleep Apnea Detection",
      "contents_txt":"CHAPTER- III System Requirement  Specification\n\n3.1 Hardware  Requirement\n\nOperating System: Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\nProcessor (CPU): A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\nLibraries and Frameworks: If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\nGraphical User Interfaces (GUIs): If your program has a graphical interface, it might require slightly better hardware for smooth performance\n\n3.2 Software Requirement\n\n3.2.1 Visual Studio  Code\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n3.2.2 Google Sheets\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n3.2.3 Python Modules\n\nPandas\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\n\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\nNumPy\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\n\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\nMatplotlib/Seaborn\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\n\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\nsmtplib\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\n\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\n\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\nemail.mime  ( email.mime.multipart  and  email.mime.text )\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\nStreamlit\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\n\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n3.2.4  Data set for Analysis\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies"
    },
    "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 18\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Hardware  Requirement]\n\n*Operating System:* Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\n*Processor (CPU):* A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\n*Libraries and Frameworks:* If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\n*Graphical User Interfaces (GUIs):* If your program has a graphical interface, it might require slightly better hardware for smooth performance\n\n#text(size: 14pt, weight: \"bold\")[3.2 Software Requirement]\n\n*3.1.1 Visual Studio  Code*\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n*3.1.2 Google Sheets*\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n*3.1.3 Python Modules*\n\n*Pandas*\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\n*NumPy*\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\n*Matplotlib/Seaborn*\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\n*smtplib*\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\n*email.mime  ( email.mime.multipart  and  email.mime.text )*\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\n#pagebreak()\n*Streamlit*\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n*3.1.4  Data set for Analysis*\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n"
  },
  {
  "input": {
    "starting_page": 28,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- III System Requirement  Specification\n\n3.1 Software Requirement\n\n3.1.1 Google Sheets\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n3.1.2 Python Modules\n\nPandas\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\n\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\nNumPy\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\n\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\nMatplotlib/Seaborn\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\n\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\nsmtplib\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\n\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\n\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\nemail.mime  ( email.mime.multipart  and  email.mime.text )\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\nStreamlit\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\n\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n3.1.3 Visual Studio  Code\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n3.1.4  Data set for Analysis\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n\n3.2 Hardware  Requirement\n\nOperating System: Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\nProcessor (CPU): A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\nLibraries and Frameworks: If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\nGraphical User Interfaces (GUIs): If your program has a graphical interface, it might require slightly better hardware for smooth performance"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 28\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Software Requirement]\n\n*3.1.1 Google Sheets*\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n*3.1.2 Python Modules*\n\n*Pandas*\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\n*NumPy*\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\n*Matplotlib/Seaborn*\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\n*smtplib*\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\n*email.mime  ( email.mime.multipart  and  email.mime.text )*\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\n*Streamlit*\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n*3.1.3 Visual Studio  Code*\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n*3.1.4  Data set for Analysis*\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Hardware  Requirement]\n\n*Operating System:* Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\n*Processor (CPU):* A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\n*Libraries and Frameworks:* If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\n*Graphical User Interfaces (GUIs):* If your program has a graphical interface, it might require slightly better hardware for smooth performance\n"
  },
  {
  "input": {
    "starting_page": 38,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- III System Requirement  Specification\n\n3.1 Software Requirement\n\n3.1.1 Visual Studio  Code\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n3.1.2 Python Modules\n\nPandas\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\n\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\nNumPy\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\n\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\nMatplotlib/Seaborn\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\n\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\nsmtplib\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\n\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\n\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\nemail.mime  ( email.mime.multipart  and  email.mime.text )\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\nStreamlit\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\n\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n3.1.3  Data set for Analysis\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n\n3.1.4 Google Sheets\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n3.2 Hardware  Requirement\n\nOperating System: Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\nProcessor (CPU): A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\nLibraries and Frameworks: If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\nGraphical User Interfaces (GUIs): If your program has a graphical interface, it might require slightly better hardware for smooth performance"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 38\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Software Requirement]\n\n*3.1.1 Visual Studio  Code*\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n*3.1.2 Python Modules*\n\n*Pandas*\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\n*NumPy*\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\n*Matplotlib/Seaborn*\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\n*smtplib*\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\n*email.mime  ( email.mime.multipart  and  email.mime.text )*\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\n*Streamlit*\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n*3.1.3  Data set for Analysis*\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies.\n\n*3.1.4 Google Sheets*\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Hardware  Requirement]\n\n*Operating System:* Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\n*Processor (CPU):* A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\n*Libraries and Frameworks:* If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\n*Graphical User Interfaces (GUIs):* If your program has a graphical interface, it might require slightly better hardware for smooth performance\n"
  },
  {
  "input": {
    "starting_page": 48,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- III System Requirement  Specification\n\n3.1 Hardware  Requirement\n\n3.1.1 Visual Studio  Code\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n3.1.2 Google Sheets\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\nPython Modules\n\nPandas\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\n\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\nNumPy\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\n\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\nMatplotlib/Seaborn\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\n\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\nsmtplib\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\n\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\n\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\nemail.mime  ( email.mime.multipart  and  email.mime.text )\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\nStreamlit\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\n\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n3.1.4  Data set for Analysis\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n\n3.2 Software Requirement\n\nOperating System: Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\nProcessor (CPU): A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\nLibraries and Frameworks: If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\nGraphical User Interfaces (GUIs): If your program has a graphical interface, it might require slightly better hardware for smooth performance"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- III]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 48\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - III]\n#align(center)[#text(size: 16pt, weight: \"bold\")[SYSTEM REQUIREMENT SPECIFICATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[3.1 Hardware Requirement]\n\n*3.1.1 Visual Studio  Code*\n\nVisual Studio Code, also commonly referred to as VS Code, is a source-code editor developed by Microsoft for Windows, Linux, macOS and web browser\n\n*3.1.2 Google Sheets*\n\nA platform for storing and sharing data, which can be integrated with Python for data analysis.\n\n*3.1.3 Python Modules*\n\n*Pandas*\n\nDescription: Pandas is an open-source data manipulation and analysis library for Python. It provides data structures like  DataFrame  and Series, which are powerful and flexible for handling structured data.\nUsage in Project: In this project, Pandas is used for loading, cleaning, and preprocessing the dataset. It allows for efficient data manipulation, such as filtering, merging, and aggregating data. Pandas is also utilized to extract features like sleep awakenings and sleep cycles from the ra w data.\n\n*NumPy*\n\nDescription: NumPy is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\nUsage in Project: NumPy is used for numerical computations and handling large datasets. It is particularly useful for performing mathematical  operationson  the dataset, such as calculating statistics (mean, standard deviation) and normalizing data.\n\n*Matplotlib/Seaborn*\n\nDescription: Matplotlib is a plotting library for Python, and Seaborn is built on top of Matplotlib, providing a high- level interface for drawing attractive and informative statistical graphics.\nUsage in Project: These libraries are used for data visualization. Matplotlib and Seaborn help in creating plots, charts, and graphs to visualize the distribution of sleep data, the relationship between different features, and the results of the sleep  apnea  predictions. Visualization aids in understanding data patterns and communicating results effectively.\n\n*smtplib*\n\nDescription:smtplib  is a Python library for sending emails using the Simple Mail Transfer Protocol (SMTP).\nUsage in  Project:smtplib  is used to send email alerts as part of the SOS alert system. When a critical heart rate drop is detected, an email alert can be sent to emergency\ncontacts. This provides an additional communication channel to ensure that alerts are received promptly.\n\n*email.mime  ( email.mime.multipart  and  email.mime.text )*\n\nDescription: The  email.mime  module is part of Pythons standard library for managing MIME (Multipurpose Internet Mail Extensions) email messages. It allows the creation of complex email messages with multiple parts, including text and attachments.\n\nUsage in Project: The  email.mime  modules are used to construct the content of the email alerts. This includes setting up the email's subject, body, and any additional parts, such as attachments or HTML content. These  modules work in conjunction with  smtplib  to send well-formatted email alerts.\n\n*Streamlit*\n\nDescription:   Streamlit  lets you turn regular Python scripts into interactive web apps.\nUsage in Project: It used to design a user-friendly GUI of the website to display the analysis and take the user input.\n\n*3.1.4  Data set for Analysis*\n\nA dataset containing sleep data (including sleep awakenings and sleep cycles) and heart rate data collected from smart watches. This dataset is crucial for training the machine learning models and testing the system's accuracy in predicting sleep apnea and detecting heart rate anomalies\n\n#pagebreak()\n#text(size: 14pt, weight: \"bold\")[3.2 Software Requirement]\n\n*Operating System:* Any modern operating system like Windows 10 (or later), macOS (or later), or a Linux distribution will work.\n\n*Processor (CPU):* A modern processor (e.g., Intel Core i3 or AMD Ryzen 3) is sufficient for most basic Python programs.\n\n*Libraries and Frameworks:* If your program uses external libraries or frameworks, they might have their own hardware requirements. Check the documentation for those specific tools.\n\n*Graphical User Interfaces (GUIs):* If your program has a graphical interface, it might require slightly better hardware for smooth performance\n"
  },
  {
  "input": {
    "starting_page": 16,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- IV   :  Methodology and  Implementation\n\n4.1 Methodology\n\n1. Data Collection\n\n1.1  Identify Datasets:\n\nSearch for or obtain permissions to access datasets containing:\n\nSleep Data:  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\nLifestyle Factors:  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\nHeart Rate Data:  Continuous heart rate measurements from smart watches or wearable devices.\n\n1.2  Data Preprocessing:\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n2. Feature Engineering\n\n2.1 Extract Relevant Features:\n\nCalculate sleep metrics: Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% )\n\nConvert lifestyle factors into quantifiable features: Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\nIncorporate heart rate features: Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n2.2  Feature Selection:\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n3. Model Development\n\n3.1  Interpretation of Results:\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n4. Reasoning for Sleep Disturbances and Heart Rate Anomalies\n\n4.1  Statistical Analysis:\n\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n4.2  Hypothesis Testing:\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n5. Evaluation and Validation\n\n5.1  Validation of Findings:\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n6. SOS System Integration\n\n6.1  Design SOS Functionality:\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n6.2   Integration with Sleep Data Analysis:\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n4.2 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.2.1 Flow chart]\n)\n\nThe process flow diagram illustrates three main stages: 1) Load and prepare data by loading, cleaning, and formatting datasets. 2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. 3) Compile findings and results, including visualizations, to present the final results\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 4.2.2 Block Diagram]\n)\n\nT he smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 16\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\n*1. Data Collection*\n\n*1.1  Identify Datasets:*\n\nSearch for or obtain permissions to access datasets containing:\n\n*Sleep Data:*  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\n*Lifestyle Factors:*  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\n*Heart Rate Data:*  Continuous heart rate measurements from smart watches or wearable devices.\n\n*1.2  Data Preprocessing:*\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n*2. Feature Engineering*\n\n*2.1 Extract Relevant Features:*\n\n*Calculate sleep metrics:* Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% ).\n\n*Convert lifestyle factors into quantifiable features:* Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\n*Incorporate heart rate features:* Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n*2.2  Feature Selection:*\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n*3. Model Development*\n\n*3.1  Interpretation of Results:*\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n*4. Reasoning for Sleep Disturbances and Heart Rate Anomalies*\n\n*4.1  Statistical Analysis:*\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n*4.2  Hypothesis Testing:*\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability.\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n*5. Evaluation and Validation*\n\n*5.1  Validation of Findings:*\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n*6. SOS System Integration*\n\n*6.1  Design SOS Functionality:*\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n*6.2   Integration with Sleep Data Analysis:*\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"images/image001(2).png\", width: 70%),\n  caption: [Fig 4.2.1 Flow Chart],\n  numbering: none\n)\n\nThe process flow diagram illustrates three main stages: \n\n1) Load and prepare data by loading, cleaning, and formatting datasets. \n\n2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. \n\n3) Compile findings and results, including visualizations, to present the final results.\n\n#figure(\n  image(\"images/image002(2).jpg\", width: 70%),\n  caption: [Fig 4.2.2 Block Diagram],\n  numbering: none\n)\n\nThe smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage."
  },
  {
  "input": {
    "starting_page": 30,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":  "CHAPTER- IV   :  Methodology and  Implementation\n\n4.1 Methodology\n\n1. Data Collection\n\n1.1  Identify Datasets:\n\nSearch for or obtain permissions to access datasets containing:\n\nSleep Data:  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\nLifestyle Factors:  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\nHeart Rate Data:  Continuous heart rate measurements from smart watches or wearable devices.\n\n1.2  Data Preprocessing:\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n2. Feature Engineering\n\n2.1 Extract Relevant Features:\n\nCalculate sleep metrics: Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% )\n\nConvert lifestyle factors into quantifiable features: Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\nIncorporate heart rate features: Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n2.2  Feature Selection:\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n3. Model Development\n\n3.1  Interpretation of Results:\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n4. Reasoning for Sleep Disturbances and Heart Rate Anomalies\n\n4.1  Statistical Analysis:\n\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n4.2  Hypothesis Testing:\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n5. Evaluation and Validation\n\n5.1  Validation of Findings:\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n6. SOS System Integration\n\n6.1  Design SOS Functionality:\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n6.2   Integration with Sleep Data Analysis:\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n4.2 Execution Flowchart\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 4.2.2 Block Diagram]\n)\n\nThe process flow diagram illustrates three main stages: 1) Load and prepare data by loading, cleaning, and formatting datasets. 2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. 3) Compile findings and results, including visualizations, to present the final results\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.2.1 Flow chart]\n)\n\nT he smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 30\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\n*1. Data Collection*\n\n*1.1  Identify Datasets:*\n\nSearch for or obtain permissions to access datasets containing:\n\n*Sleep Data:*  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\n*Lifestyle Factors:*  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\n*Heart Rate Data:*  Continuous heart rate measurements from smart watches or wearable devices.\n\n*1.2  Data Preprocessing:*\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n*2. Feature Engineering*\n\n*2.1 Extract Relevant Features:*\n\n*Calculate sleep metrics:* Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% ).\n\n*Convert lifestyle factors into quantifiable features:* Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\n*Incorporate heart rate features:* Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n*2.2  Feature Selection:*\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n*3. Model Development*\n\n*3.1  Interpretation of Results:*\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n*4. Reasoning for Sleep Disturbances and Heart Rate Anomalies*\n\n*4.1  Statistical Analysis:*\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n*4.2  Hypothesis Testing:*\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability.\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n*5. Evaluation and Validation*\n\n*5.1  Validation of Findings:*\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n*6. SOS System Integration*\n\n*6.1  Design SOS Functionality:*\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n*6.2   Integration with Sleep Data Analysis:*\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"images/image002(2).jpg\", width: 70%),\n  caption: [Fig 4.2.2 Block Diagram],\n  numbering: none\n)\n\nThe process flow diagram illustrates three main stages: \n\n1) Load and prepare data by loading, cleaning, and formatting datasets. \n\n2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. \n\n3) Compile findings and results, including visualizations, to present the final results.\n\n#figure(\n  image(\"images/image001(2).png\", width: 70%),\n  caption: [Fig 4.2.1 Flow Chart],\n  numbering: none\n)\n\nThe smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage."
  },
  {
  "input": {
    "starting_page": 3,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":  "CHAPTER- IV   :  Methodology and  Implementation\n\n4.1 Methodology\n\n1. Data Collection\n\n1.1  Identify Datasets:\n\nSearch for or obtain permissions to access datasets containing:\n\nSleep Data:  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\nLifestyle Factors:  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\nHeart Rate Data:  Continuous heart rate measurements from smart watches or wearable devices.\n\n1.2  Data Preprocessing:\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n2. Feature Engineering\n\n2.1 Extract Relevant Features:\n\nCalculate sleep metrics: Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% )\n\nConvert lifestyle factors into quantifiable features: Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\nIncorporate heart rate features: Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n2.2  Feature Selection:\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n3. Model Development\n\n3.1  Interpretation of Results:\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n4. Reasoning for Sleep Disturbances and Heart Rate Anomalies\n\n4.1  Statistical Analysis:\n\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n4.2  Hypothesis Testing:\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n5. Evaluation and Validation\n\n5.1  Validation of Findings:\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n6. SOS System Integration\n\n6.1  Design SOS Functionality:\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n6.2   Integration with Sleep Data Analysis:\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n4.2 Execution Flowchart\n\nThe process flow diagram illustrates three main stages: 1) Load and prepare data by loading, cleaning, and formatting datasets. 2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. 3) Compile findings and results, including visualizations, to present the final results\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 4.2.2 Block Diagram]\n)\n\nT he smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.2.1 Flow chart]\n)"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 3\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\n*1. Data Collection*\n\n*1.1  Identify Datasets:*\n\nSearch for or obtain permissions to access datasets containing:\n\n*Sleep Data:*  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\n*Lifestyle Factors:*  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\n*Heart Rate Data:*  Continuous heart rate measurements from smart watches or wearable devices.\n\n*1.2  Data Preprocessing:*\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n*2. Feature Engineering*\n\n*2.1 Extract Relevant Features:*\n\n*Calculate sleep metrics:* Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% ).\n\n*Convert lifestyle factors into quantifiable features:* Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\n*Incorporate heart rate features:* Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n*2.2  Feature Selection:*\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n*3. Model Development*\n\n*3.1  Interpretation of Results:*\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n*4. Reasoning for Sleep Disturbances and Heart Rate Anomalies*\n\n*4.1  Statistical Analysis:*\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n*4.2  Hypothesis Testing:*\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability.\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n*5. Evaluation and Validation*\n\n*5.1  Validation of Findings:*\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n*6. SOS System Integration*\n\n*6.1  Design SOS Functionality:*\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n*6.2   Integration with Sleep Data Analysis:*\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\nThe process flow diagram illustrates three main stages: \n\n1) Load and prepare data by loading, cleaning, and formatting datasets. \n\n2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. \n\n3) Compile findings and results, including visualizations, to present the final results.\n\n#figure(\n  image(\"images/image002(2).jpg\", width: 70%),\n  caption: [Fig 4.2.2 Block Diagram],\n  numbering: none\n)\n\nThe smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage.\n\n#figure(\n  image(\"images/image001(2).png\", width: 70%),\n  caption: [Fig 4.2.1 Flow Chart],\n  numbering: none\n)"
  },
  {
  "input": {
    "starting_page": 47,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- IV   :  Methodology and  Implementation\n\n4.1 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.2.1 Flow chart]\n)\n\nThe process flow diagram illustrates three main stages: 1) Load and prepare data by loading, cleaning, and formatting datasets. 2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. 3) Compile findings and results, including visualizations, to present the final results\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 4.2.2 Block Diagram]\n)\n\nT he smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage\n\n4.2 Methodology\n\n1. Data Collection\n\n1.1  Identify Datasets:\n\nSearch for or obtain permissions to access datasets containing:\n\nSleep Data:  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\nLifestyle Factors:  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\nHeart Rate Data:  Continuous heart rate measurements from smart watches or wearable devices.\n\n1.2  Data Preprocessing:\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n2. Feature Engineering\n\n2.1 Extract Relevant Features:\n\nCalculate sleep metrics: Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% )\n\nConvert lifestyle factors into quantifiable features: Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\nIncorporate heart rate features: Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n2.2  Feature Selection:\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n3. Model Development\n\n3.1  Interpretation of Results:\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n4. Reasoning for Sleep Disturbances and Heart Rate Anomalies\n\n4.1  Statistical Analysis:\n\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n4.2  Hypothesis Testing:\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n5. Evaluation and Validation\n\n5.1  Validation of Findings:\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n6. SOS System Integration\n\n6.1  Design SOS Functionality:\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n6.2   Integration with Sleep Data Analysis:\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 47\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Execution Flowchart]\n\n#figure(\n  image(\"images/image001(2).png\", width: 70%),\n  caption: [Fig 4.2.1 Flow Chart],\n  numbering: none\n)\n\nThe process flow diagram illustrates three main stages: 1) Load and prepare data by loading, cleaning, and formatting datasets. 2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. 3) Compile findings and results, including visualizations, to present the final results.\n\n#figure(\n  image(\"images/image002(2).jpg\", width: 70%),\n  caption: [Fig 4.2.2 Block Diagram],\n  numbering: none\n)\n\nThe smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Methodology]\n\n*1. Data Collection*\n\n*1.1  Identify Datasets:*\n\nSearch for or obtain permissions to access datasets containing:\n\n*Sleep Data:*  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\n*Lifestyle Factors:*  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\n*Heart Rate Data:*  Continuous heart rate measurements from smart watches or wearable devices.\n\n*1.2  Data Preprocessing:*\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n*2. Feature Engineering*\n\n*2.1 Extract Relevant Features:*\n\n*Calculate sleep metrics:* Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% ).\n\n*Convert lifestyle factors into quantifiable features:* Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\n*Incorporate heart rate features:* Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n*2.2  Feature Selection:*\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n*3. Model Development*\n\n*3.1  Interpretation of Results:*\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n*4. Reasoning for Sleep Disturbances and Heart Rate Anomalies*\n\n*4.1  Statistical Analysis:*\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n*4.2  Hypothesis Testing:*\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability.\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n#pagebreak()\n*5. Evaluation and Validation*\n\n*5.1  Validation of Findings:*\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n*6. SOS System Integration*\n\n*6.1  Design SOS Functionality:*\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n*6.2   Integration with Sleep Data Analysis:*\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep."
  },
  {
  "input": {
    "starting_page": 28,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- IV   :  Methodology and  Implementation\n\n4.1 Methodology\n\n1. Feature Engineering\n\n1.1 Extract Relevant Features:\n\nCalculate sleep metrics: Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% )\n\nConvert lifestyle factors into quantifiable features: Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\nIncorporate heart rate features: Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n1.2  Feature Selection:\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n2. Data Collection\n\n2.1  Identify Datasets:\n\nSearch for or obtain permissions to access datasets containing:\n\nSleep Data:  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\nLifestyle Factors:  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\nHeart Rate Data:  Continuous heart rate measurements from smart watches or wearable devices.\n\n2.2  Data Preprocessing:\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n3. Model Development\n\n3.1  Interpretation of Results:\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n4. Reasoning for Sleep Disturbances and Heart Rate Anomalies\n\n4.1  Statistical Analysis:\n\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n4.2  Hypothesis Testing:\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n5. Evaluation and Validation\n\n5.1  Validation of Findings:\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n6. SOS System Integration\n\n6.1  Design SOS Functionality:\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n6.2   Integration with Sleep Data Analysis:\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n4.2 Execution Flowchart\n\n#figure(\n  image(\"image001.jpg\", width: 50%),\n  caption: [Fig 4.2.1 Flow chart]\n)\n\nThe process flow diagram illustrates three main stages: 1) Load and prepare data by loading, cleaning, and formatting datasets. 2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. 3) Compile findings and results, including visualizations, to present the final results\n\nT he smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage\n\n#figure(\n  image(\"image002.jpg\", width: 50%),\n  caption: [Fig 4.2.2 Block Diagram]\n)"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- IV]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 28\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - IV]\n#align(center)[#text(size: 16pt, weight: \"bold\")[METHODOLOGY AND IMPLEMENTATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#text(size: 14pt, weight: \"bold\")[4.1 Methodology]\n\n*1. Feature Engineering*\n\n*1.1 Extract Relevant Features:*\n\n*Calculate sleep metrics:* Total sleep time, time spent in each sleep stage (REM, NREM), number of awakenings per night, sleep efficiency (% ).\n\n*Convert lifestyle factors into quantifiable features:* Binary indicators (e.g., caffeine consumption yes/no), categorical variables (e.g., alcohol consumption frequency), or numerical values (e.g., average daily caffeine intake).\n\n*Incorporate heart rate features:* Mean heart rate, variability, and detect patterns indicative of heart rate drops.\n\n*1.2  Feature Selection:*\n\nSelect the most relevant features for predicting sleep apnea and detecting heart rate anomalies.\n\n*2. Data Collection*\n\n*2.1  Identify Datasets:*\n\nSearch for or obtain permissions to access datasets containing:\n\n*Sleep Data:*  Metrics such as sleep stages (REM, NREM), awakenings, sleep duration, and sleep efficiency.\n\n*Lifestyle Factors:*  Information on caffeine and alcohol consumption, exercise patterns, and other relevant lifestyle choices.\n\n*Heart Rate Data:*  Continuous heart rate measurements from smart watches or wearable devices.\n\n*2.2  Data Preprocessing:*\n\nClean and preprocess the data to ensure consistency and quality.\n\nMerge datasets if necessary to combine sleep data with lifestyle factors and heart rate data.\n\n*3. Model Development*\n\n*3.1  Interpretation of Results:*\n\nAnalyze model outputs to understand which features (sleep metrics, lifestyle factors, heart rate data) contribute most to predicting sleep apnea.\n\nInvestigate correlations between lifestyle factors (e.g., caffeine, alcohol) and sleep disturbances, and heart rate drops.\n\n*4. Reasoning for Sleep Disturbances and Heart Rate Anomalies*\n\n*4.1  Statistical Analysis:*\nConduct statistical tests (e.g., regression analysis, correlation tests) to identify relationships between lifestyle factors, sleep patterns, and heart rate anomalies.\n\n*4.2  Hypothesis Testing:*\n\nFormulate hypotheses about the impact of lifestyle factors (caffeine, alcohol) on sleep quality and heart rate variability.\n\nTest hypotheses using data-driven methods to validate or refute assumptions about their effects.\n\n*5. Evaluation and Validation*\n\n*5.1  Validation of Findings:*\n\nValidate findings related to lifestyle factors, sleep disturbances, and heart rate anomalies against existing literature and clinical studies.\n\nEnsure conclusions drawn are scientifically supported and statistically significant.\n\n*6. SOS System Integration*\n\n*6.1  Design SOS Functionality:*\n\nDevelop algorithms to monitor heart rate data in real-time.\n\nDefine thresholds and conditions for triggering SOS alerts when heart rate drops below a critical level, potentially indicating a health emergency.\n\n*6.2   Integration with Sleep Data Analysis:*\n\nIntegrate the SOS system with the sleep apnea prediction model and heart rate monitoring.\n\nImplement logic to send immediate SOS alerts to emergency contacts when abnormal heart rate patterns are detected during sleep.\n\n#text(size: 14pt, weight: \"bold\")[4.2 Execution Flowchart]\n\n#figure(\n  image(\"images/image001(2).png\", width: 70%),\n  caption: [Fig 4.2.1 Flow Chart],\n  numbering: none\n)\n\nThe process flow diagram illustrates three main stages: \n\n1) Load and prepare data by loading, cleaning, and formatting datasets. \n\n2) Perform data analysis by calculating sleep metrics, descriptive statistics, and formulating hypotheses. \n\n3) Compile findings and results, including visualizations, to present the final results.\n\nThe smart watch data analyzer includes four key functions: visualizing data analysis with NumPy, Matplotlib, and Seaborn; predicting sleep apnea by analyzing sleep parameters; implementing an SOS system to alert on heart rate drops and predict heart attacks; and providing lifestyle change suggestions to manage.\n\n#figure(\n  image(\"images/image002(2).jpg\", width: 70%),\n  caption: [Fig 4.2.2 Block Diagram],\n  numbering: none\n)"
  },
  {
  "input": {
    "starting_page": 30,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- V  :  Testing and  Validation\n\nTest Case 1 : Testing of mail delivering\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 5.1 Error 1]\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\nTest Case 2 : Testing of mail delivering\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Fig 5.2 Fixing of Error 1]\n)\n\nTest case passed.\n\nTest Case 3 : Sleep apnea prediction\n\n#figure(\n  image(\"image005.jpg\", width: 50%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle]\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\nTest Case 4 : Sleep apnea prediction\n\n#figure(\n  image(\"image006.jpg\", width: 50%),\n  caption: [Fig 5.4 Sleep apnea prediction]\n)\n\nTest case has passed.\n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 30\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Test Case 1 : Testing of mail delivering*\n#figure(\n  image(\"images/image003(2).png\", width: 70%),\n  caption: [Fig 5.1 Error 1],\n  numbering: none\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\n*Test Case 2 : Testing of mail delivering*\n#figure(\n  image(\"images/image004(1).png\", width: 70%),\n  caption: [Fig 5.2 Fixing of Error 1],\n  numbering: none\n)\n\nTest case passed.\n\n#pagebreak()\n*Test Case 3 : Sleep apnea prediction*\n#figure(\n  image(\"images/image005.png\", width: 70%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle],\n  numbering: none\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\n#pagebreak()\n*Test Case 4 : Sleep apnea prediction*\n#figure(\n  image(\"images/image006.png\", width: 70%),\n  caption: [Fig 5.4 Sleep apnea prediction],\n  numbering: none\n)\n\nTest case has passed. \n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia."
  },
  {
  "input": {
    "starting_page": 1,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- V  :  Testing and  Validation\n\nTest Case 1 : Testing of mail delivering\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 5.1 Error 1]\n)\n\nTest Case 2 : Testing of mail delivering\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Fig 5.2 Fixing of Error 1]\n)\n\nTest case passed.\n\nTest Case 3 : Sleep apnea prediction\n\n#figure(\n  image(\"image005.jpg\", width: 50%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle]\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\nTest Case 4 : Sleep apnea prediction\n\nTest case has passed.\n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia.\n\n#figure(\n  image(\"image006.jpg\", width: 50%),\n  caption: [Fig 5.4 Sleep apnea prediction]\n)"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 1\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\n*Test Case 1 : Testing of mail delivering*\n#figure(\n  image(\"images/image003(2).png\", width: 70%),\n  caption: [Fig 5.1 Error 1],\n  numbering: none\n)\n\n*Test Case 2 : Testing of mail delivering*\n#figure(\n  image(\"images/image004(1).png\", width: 70%),\n  caption: [Fig 5.2 Fixing of Error 1],\n  numbering: none\n)\n\nTest case passed.\n\n#pagebreak()\n*Test Case 3 : Sleep apnea prediction*\n#figure(\n  image(\"images/image005.png\", width: 70%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle],\n  numbering: none\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\nTest case has passed. \n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia.\n\n*Test Case 4 : Sleep apnea prediction*\n#figure(\n  image(\"images/image006.png\", width: 70%),\n  caption: [Fig 5.4 Sleep apnea prediction],\n  numbering: none\n)"
  },
  {
  "input": {
    "starting_page": 22,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- V  :  Testing and  Validation\n\nTest Case 1 : Testing of mail delivering\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 5.1 Error 1]\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\nTest Case 2 : Testing of mail delivering\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Fig 5.2 Fixing of Error 1]\n)\n\nTest case passed.\n\nTest Case 3 : Sleep apnea prediction\n\n#figure(\n  image(\"image005.jpg\", width: 50%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle]\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 22\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Test Case 1 : Testing of mail delivering*\n#figure(\n  image(\"images/image003(2).png\", width: 70%),\n  caption: [Fig 5.1 Error 1],\n  numbering: none\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\n*Test Case 2 : Testing of mail delivering*\n#figure(\n  image(\"images/image004(1).png\", width: 70%),\n  caption: [Fig 5.2 Fixing of Error 1],\n  numbering: none\n)\n\nTest case passed.\n\n#pagebreak()\n*Test Case 3 : Sleep apnea prediction*\n#figure(\n  image(\"images/image005.png\", width: 70%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle],\n  numbering: none\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal."
  },
  {
  "input": {
    "starting_page": 49,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- V  :  Testing and  Validation\n\nTest Case 1 : Testing of mail delivering\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 5.1 Error 1]\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\nTest Case 2 : Sleep apnea prediction\n\n#figure(\n  image(\"image005.jpg\", width: 50%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle]\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\nTest Case 3 : Sleep apnea prediction\n\n#figure(\n  image(\"image006.jpg\", width: 50%),\n  caption: [Fig 5.4 Sleep apnea prediction]\n)\n\nTest case has passed.\n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia.\n\nTest Case 2 : Testing of mail delivering\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Fig 5.2 Fixing of Error 1]\n)\n\nTest case passed."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 49\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Test Case 1 : Testing of mail delivering*\n#figure(\n  image(\"images/image003(2).png\", width: 70%),\n  caption: [Fig 5.1 Error 1],\n  numbering: none\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\n#pagebreak()\n*Test Case 3 : Sleep apnea prediction*\n#figure(\n  image(\"images/image005.png\", width: 70%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle],\n  numbering: none\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\n#pagebreak()\n*Test Case 4 : Sleep apnea prediction*\n#figure(\n  image(\"images/image006.png\", width: 70%),\n  caption: [Fig 5.4 Sleep apnea prediction],\n  numbering: none\n)\n\nTest case has passed. \n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia.\n\n#pagebreak()\n*Test Case 2 : Testing of mail delivering*\n#figure(\n  image(\"images/image004(1).png\", width: 70%),\n  caption: [Fig 5.2 Fixing of Error 1],\n  numbering: none\n)\n\nTest case passed."
  },
  {
  "input": {
    "starting_page": 12,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- V  :  Testing and  Validation\n\nTest Case 1 : Testing of mail delivering\n\n#figure(\n  image(\"image003.jpg\", width: 50%),\n  caption: [Fig 5.1 Error 1]\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\nTest Case 2 : Testing of mail delivering\n\n#figure(\n  image(\"image004.jpg\", width: 50%),\n  caption: [Fig 5.2 Fixing of Error 1]\n)\n\nTest case passed.\n\nTest Case 3 : Sleep apnea prediction\n\n#figure(\n  image(\"image005.jpg\", width: 50%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle]\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\nTest Case 4 : Sleep apnea prediction\n\n#figure(\n  image(\"image006.jpg\", width: 50%),\n  caption: [Fig 5.4 Sleep apnea prediction]\n)\n\nTest case has passed.\n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- V]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[TESTING AND VALIDATION]\n\n  #v(1fr)\n]\n\n#let starting_page = 12\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - V]\n#align(center)[#text(size: 16pt, weight: \"bold\")[TESTING AND VALIDATION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Test Case 1 : Testing of mail delivering*\n#figure(\n  image(\"images/image003(2).png\", width: 70%),\n  caption: [Fig 5.1 Error 1],\n  numbering: none\n)\n\nTest case failed. The  try-except-else  block is incorrectly indented, violating Python's syntax rules, leading to a parsing error and preventing code execution.\n\n*Test Case 2 : Testing of mail delivering*\n#figure(\n  image(\"images/image004(1).png\", width: 70%),\n  caption: [Fig 5.2 Fixing of Error 1],\n  numbering: none\n)\n\nTest case passed.\n\n#pagebreak()\n*Test Case 3 : Sleep apnea prediction*\n#figure(\n  image(\"images/image005.png\", width: 70%),\n  caption: [Fig 5.3 Checking if sleep apnea prediction is right for lower REM cycle],\n  numbering: none\n)\n\nTest case has passed. Based on the sleep and lifestyle data entered and the subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's REM cycle duration is significantly lower than normal.\n\n#pagebreak()\n*Test Case 4 : Sleep apnea prediction*\n#figure(\n  image(\"images/image006.png\", width: 70%),\n  caption: [Fig 5.4 Sleep apnea prediction],\n  numbering: none\n)\n\nTest case has passed. \n\nBased on the sleep and lifestyle data entered and the   subsequent prediction analysis, it has been determined that the individual may have sleep apnea. This conclusion is drawn from the observation that the person's awakenings in more. So we have suggested that the person should consume caffeine at least 2 hours before bedtime to lessen your chance of having insomnia."
  },
  {
  "input": {
    "starting_page": 16,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- VI  :  Results and  Discussions\n\n#figure(\n  image(\"image007.jpg\", width: 50%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week]\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"image008.jpg\", width: 50%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution]\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"image009.jpg\", width: 50%),\n  caption: [Fig 6. 3  Sleep apnea]\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"image010.jpg\", width: 50%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gend er]\n)\n\n#figure(\n  image(\"image011.jpg\", width: 50%),\n  caption: [Fig 6. 5  Data Analysis]\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"image012.jpg\", width: 50%),\n  caption: [Fig  6. 6   Sleep Apnea prediction]\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"image013.jpg\", width: 50%),\n  caption: [Fig 6. 7  User Data Analysis]\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"image014.jpg\", width: 50%),\n  caption: [Fig 6. 8  Email system]\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\nDiscussions:\n\nAccuracy and Reliability:\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\nClinical Relevance:\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\nEthical and Privacy Considerations:\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\nFuture Directions:\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\nChallenges in Data Collection and Validation:\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   Impact on Personalized Healthcare:\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\nIntegration with Healthcare Systems:\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\nLongitudinal Monitoring and Predictive Maintenance:\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\nPublic Health Implications:\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\n\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\nRegulatory and Policy Considerations:\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\nUser Education and Engagement:\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\nLimitations and Future Research Directions:\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]\n\n  #v(1fr)\n]\n\n#let starting_page = 16\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#figure(\n  image(\"images/image007.png\", width: 70%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week],\n  numbering: none\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"images/image008.png\", width: 70%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution],\n  numbering: none\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"images/image009.png\", width: 70%),\n  caption: [Fig 6. 3  Sleep apnea],\n  numbering: none\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"images/image010.png\", width: 70%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gender],\n  numbering: none\n)\n\n#figure(\n  image(\"images/image011.png\", width: 70%),\n  caption: [Fig 6. 5  Data Analysis],\n  numbering: none\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"images/image012.png\", width: 70%),\n  caption: [Fig  6. 6   Sleep Apnea prediction],\n  numbering: none\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"images/image013.png\", width: 70%),\n  caption: [Fig 6. 7  User Data Analysis],\n  numbering: none\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"images/image014.png\", width: 70%),\n  caption: [Fig 6. 8  Email system],\n  numbering: none\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\n*Discussions:*\n\n*Accuracy and Reliability:*\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\n*Clinical Relevance:*\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\n*Ethical and Privacy Considerations:*\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\n*Future Directions:*\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\n*Challenges in Data Collection and Validation:*\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   \n\n*Impact on Personalized Healthcare:*\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\n*Integration with Healthcare Systems:*\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\n*Longitudinal Monitoring and Predictive Maintenance:*\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\n#pagebreak()\n*Public Health Implications:*\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\n*Regulatory and Policy Considerations:*\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\n*User Education and Engagement:*\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\n*Limitations and Future Research Directions:*\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  {
  "input": {
    "starting_page": 34,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- VI  :  Results and  Discussions\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"image007.jpg\", width: 50%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week]\n)\n\n#figure(\n  image(\"image008.jpg\", width: 50%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution]\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"image009.jpg\", width: 50%),\n  caption: [Fig 6. 3  Sleep apnea]\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"image010.jpg\", width: 50%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gend er]\n)\n\n#figure(\n  image(\"image011.jpg\", width: 50%),\n  caption: [Fig 6. 5  Data Analysis]\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"image012.jpg\", width: 50%),\n  caption: [Fig  6. 6   Sleep Apnea prediction]\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"image013.jpg\", width: 50%),\n  caption: [Fig 6. 7  User Data Analysis]\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"image014.jpg\", width: 50%),\n  caption: [Fig 6. 8  Email system]\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\nDiscussions:\n\nAccuracy and Reliability:\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\nClinical Relevance:\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\nEthical and Privacy Considerations:\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\nFuture Directions:\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\nChallenges in Data Collection and Validation:\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   Impact on Personalized Healthcare:\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\nIntegration with Healthcare Systems:\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\nLongitudinal Monitoring and Predictive Maintenance:\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\nPublic Health Implications:\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\n\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\nRegulatory and Policy Considerations:\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\nUser Education and Engagement:\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\nLimitations and Future Research Directions:\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]\n\n  #v(1fr)\n]\n\n#let starting_page = 34\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"images/image007.png\", width: 70%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week],\n  numbering: none\n)\n\n#figure(\n  image(\"images/image008.png\", width: 70%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution],\n  numbering: none\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"images/image009.png\", width: 70%),\n  caption: [Fig 6. 3  Sleep apnea],\n  numbering: none\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"images/image010.png\", width: 70%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gender],\n  numbering: none\n)\n\n#figure(\n  image(\"images/image011.png\", width: 70%),\n  caption: [Fig 6. 5  Data Analysis],\n  numbering: none\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"images/image012.png\", width: 70%),\n  caption: [Fig  6. 6   Sleep Apnea prediction],\n  numbering: none\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"images/image013.png\", width: 70%),\n  caption: [Fig 6. 7  User Data Analysis],\n  numbering: none\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"images/image014.png\", width: 70%),\n  caption: [Fig 6. 8  Email system],\n  numbering: none\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\n*Discussions:*\n\n*Accuracy and Reliability:*\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\n*Clinical Relevance:*\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\n*Ethical and Privacy Considerations:*\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\n*Future Directions:*\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\n*Challenges in Data Collection and Validation:*\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   \n\n*Impact on Personalized Healthcare:*\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\n*Integration with Healthcare Systems:*\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\n*Longitudinal Monitoring and Predictive Maintenance:*\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\n#pagebreak()\n*Public Health Implications:*\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\n*Regulatory and Policy Considerations:*\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\n*User Education and Engagement:*\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\n*Limitations and Future Research Directions:*\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  {
  "input": {
    "starting_page": 27,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- VI  :  Results and  Discussions\n\n#figure(\n  image(\"image007.jpg\", width: 50%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week]\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"image008.jpg\", width: 50%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution]\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"image009.jpg\", width: 50%),\n  caption: [Fig 6. 3  Sleep apnea]\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"image010.jpg\", width: 50%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gend er]\n)\n\n#figure(\n  image(\"image011.jpg\", width: 50%),\n  caption: [Fig 6. 5  Data Analysis]\n)\n\n#figure(\n  image(\"image012.jpg\", width: 50%),\n  caption: [Fig  6. 6   Sleep Apnea prediction]\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"image013.jpg\", width: 50%),\n  caption: [Fig 6. 7  User Data Analysis]\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"image014.jpg\", width: 50%),\n  caption: [Fig 6. 8  Email system]\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\nDiscussions:\n\nAccuracy and Reliability:\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\nClinical Relevance:\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\nEthical and Privacy Considerations:\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\nFuture Directions:\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\nChallenges in Data Collection and Validation:\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   Impact on Personalized Healthcare:\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\nIntegration with Healthcare Systems:\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\nLongitudinal Monitoring and Predictive Maintenance:\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\nPublic Health Implications:\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\n\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\nRegulatory and Policy Considerations:\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\nUser Education and Engagement:\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\nLimitations and Future Research Directions:\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]\n\n  #v(1fr)\n]\n\n#let starting_page = 27\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#figure(\n  image(\"images/image007.png\", width: 70%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week],\n  numbering: none\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"images/image008.png\", width: 70%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution],\n  numbering: none\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"images/image009.png\", width: 70%),\n  caption: [Fig 6. 3  Sleep apnea],\n  numbering: none\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"images/image010.png\", width: 70%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gender],\n  numbering: none\n)\n\n#figure(\n  image(\"images/image011.png\", width: 70%),\n  caption: [Fig 6. 5  Data Analysis],\n  numbering: none\n)\n\n#figure(\n  image(\"images/image012.png\", width: 70%),\n  caption: [Fig  6. 6   Sleep Apnea prediction],\n  numbering: none\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"images/image013.png\", width: 70%),\n  caption: [Fig 6. 7  User Data Analysis],\n  numbering: none\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"images/image014.png\", width: 70%),\n  caption: [Fig 6. 8  Email system],\n  numbering: none\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\n*Discussions:*\n\n*Accuracy and Reliability:*\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\n*Clinical Relevance:*\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\n*Ethical and Privacy Considerations:*\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\n*Future Directions:*\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\n*Challenges in Data Collection and Validation:*\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   \n\n*Impact on Personalized Healthcare:*\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\n*Integration with Healthcare Systems:*\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\n*Longitudinal Monitoring and Predictive Maintenance:*\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\n#pagebreak()\n*Public Health Implications:*\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\n*Regulatory and Policy Considerations:*\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\n*User Education and Engagement:*\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\n*Limitations and Future Research Directions:*\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  {
  "input": {
    "starting_page": 43,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- VI  :  Results and  Discussions\n\n#figure(\n  image(\"image007.jpg\", width: 50%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week]\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"image008.jpg\", width: 50%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution]\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"image009.jpg\", width: 50%),\n  caption: [Fig 6. 3  Sleep apnea]\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"image010.jpg\", width: 50%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gend er]\n)\n\n#figure(\n  image(\"image011.jpg\", width: 50%),\n  caption: [Fig 6. 5  Data Analysis]\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"image012.jpg\", width: 50%),\n  caption: [Fig  6. 6   Sleep Apnea prediction]\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"image013.jpg\", width: 50%),\n  caption: [Fig 6. 7  User Data Analysis]\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"image014.jpg\", width: 50%),\n  caption: [Fig 6. 8  Email system]\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\nDiscussions:\n\nAccuracy and Reliability:\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\nFuture Directions:\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\nChallenges in Data Collection and Validation:\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   Impact on Personalized Healthcare:\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\nIntegration with Healthcare Systems:\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\nLongitudinal Monitoring and Predictive Maintenance:\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\nPublic Health Implications:\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\n\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\nRegulatory and Policy Considerations:\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\nUser Education and Engagement:\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\nLimitations and Future Research Directions:\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models.\n\nClinical Relevance:\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\nEthical and Privacy Considerations:\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]\n\n  #v(1fr)\n]\n\n#let starting_page = 43\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#figure(\n  image(\"images/image007.png\", width: 70%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week],\n  numbering: none\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"images/image008.png\", width: 70%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution],\n  numbering: none\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"images/image009.png\", width: 70%),\n  caption: [Fig 6. 3  Sleep apnea],\n  numbering: none\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"images/image010.png\", width: 70%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gender],\n  numbering: none\n)\n\n#figure(\n  image(\"images/image011.png\", width: 70%),\n  caption: [Fig 6. 5  Data Analysis],\n  numbering: none\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"images/image012.png\", width: 70%),\n  caption: [Fig  6. 6   Sleep Apnea prediction],\n  numbering: none\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"images/image013.png\", width: 70%),\n  caption: [Fig 6. 7  User Data Analysis],\n  numbering: none\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"images/image014.png\", width: 70%),\n  caption: [Fig 6. 8  Email system],\n  numbering: none\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\n*Discussions:*\n\n*Accuracy and Reliability:*\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\n*Future Directions:*\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\n*Challenges in Data Collection and Validation:*\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   \n\n*Impact on Personalized Healthcare:*\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\n*Integration with Healthcare Systems:*\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\n*Longitudinal Monitoring and Predictive Maintenance:*\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\n*Public Health Implications:*\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\n*Regulatory and Policy Considerations:*\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\n*User Education and Engagement:*\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\n*Limitations and Future Research Directions:*\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models.\n\n*Clinical Relevance:*\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\n*Ethical and Privacy Considerations:*\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes."
  },
  {
  "input": {
    "starting_page": 8,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt": "CHAPTER- VI  :  Results and  Discussions\n\n#figure(\n  image(\"image008.jpg\", width: 50%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution]\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"image007.jpg\", width: 50%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week]\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"image009.jpg\", width: 50%),\n  caption: [Fig 6. 3  Sleep apnea]\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"image010.jpg\", width: 50%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gend er]\n)\n\n#figure(\n  image(\"image011.jpg\", width: 50%),\n  caption: [Fig 6. 5  Data Analysis]\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"image012.jpg\", width: 50%),\n  caption: [Fig  6. 6   Sleep Apnea prediction]\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"image013.jpg\", width: 50%),\n  caption: [Fig 6. 7  User Data Analysis]\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"image014.jpg\", width: 50%),\n  caption: [Fig 6. 8  Email system]\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\nDiscussions:\n\nAccuracy and Reliability:\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\nClinical Relevance:\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\nEthical and Privacy Considerations:\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\nFuture Directions:\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\nChallenges in Data Collection and Validation:\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   Impact on Personalized Healthcare:\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\nIntegration with Healthcare Systems:\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\nLongitudinal Monitoring and Predictive Maintenance:\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\nPublic Health Implications:\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\n\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\nRegulatory and Policy Considerations:\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\nUser Education and Engagement:\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\nLimitations and Future Research Directions:\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VI]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]\n\n  #v(1fr)\n]\n\n#let starting_page = 8\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VI]\n#align(center)[#text(size: 16pt, weight: \"bold\")[RESULTS AND DISCUSSIONS]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n#figure(\n  image(\"images/image008.png\", width: 70%),\n  caption: [Fig 6. 2  Gender  and  Age Distribution],\n  numbering: none\n)\n\nFig 6. 2  shows the number of females and males  and   the distribution of people across different age groups. The x-axis shows age and the y-axis shows the  count.\n\n#figure(\n  image(\"images/image007.png\", width: 70%),\n  caption: [Fig 6.1 Sleep Duration Distribution  and  Exercise frequency during a  week],\n  numbering: none\n)\n\nThe above count plots Fig 6.1 shows the distribution of sleep duration vs count, the bars on the graph represent the number of people who slept for a certain amount of time.  It also  shows the Exercise frequency vs count, Similar to the sleep duration plot, the bars represent how many people exercised a certain number of times.\n\n#figure(\n  image(\"images/image009.png\", width: 70%),\n  caption: [Fig 6. 3  Sleep apnea],\n  numbering: none\n)\n\nFrom the above plot we can infer that not many people are suffering with sleep apnea. But few people are suffering because of lower REM cycle and more number of awakenings\n\n#figure(\n  image(\"images/image010.png\", width: 70%),\n  caption: [Fig 6. 4  Count of Sleep Apnea by Gender],\n  numbering: none\n)\n\n#figure(\n  image(\"images/image011.png\", width: 70%),\n  caption: [Fig 6. 5  Data Analysis],\n  numbering: none\n)\n\nFig 6. 5  shows the data set and the GUI representation of the analysis.\n\n#figure(\n  image(\"images/image012.png\", width: 70%),\n  caption: [Fig  6. 6   Sleep Apnea prediction],\n  numbering: none\n)\n\nThe above Fig 6. 6  shows the sleep apnea prediction data along with the  the  gender based  distribution.From  the distribution, we can infer that many male consuming alcohol are  suffering.In  general many men have sleep apnea  than women.\n\n#figure(\n  image(\"images/image013.png\", width: 70%),\n  caption: [Fig 6. 7  User Data Analysis],\n  numbering: none\n)\n\nThe above Fig 6. 7  shows the form where the user can input the data and predict whether they are suffering from sleep apnea or not.\n\n#figure(\n  image(\"images/image014.png\", width: 70%),\n  caption: [Fig 6. 8  Email system],\n  numbering: none\n)\n\nUsers can mail their reports to their respective mail IDs for further analysis purposes.\n\n*Discussions:*\n\n*Accuracy and Reliability:*\n\nDiscuss the accuracy of your sleep apnea prediction model and the factors influencing its reliability (e.g., quality and quantity of data, choice of algorithms).\n\nAddress limitations and potential biases in the predictive model, such as data variability among different demographics or conditions.\n\n*Clinical Relevance:*\n\nExplore the clinical implications of early sleep apnea detection using smart watch data.\n\nDiscuss how timely intervention based on predictive alerts could potentially improve patient outcomes and reduce healthcare costs associated with untreated sleep apnea.\n\n*Ethical and Privacy Considerations:*\n\nHighlight the ethical considerations related to data privacy and informed consent in collecting and analyzing sensitive health data from smart watches.\n\nDiscuss measures taken to ensure data security and user confidentiality throughout the data analysis and emergency response processes.\n\n*Future Directions:*\n\nPropose future enhancements or iterations of the smart watch system, such as incorporating additional biometric data or integrating with other healthcare monitoring devices.\n\nConsider opportunities for collaboration with healthcare providers or researchers to validate the predictive model in clinical settings and expand its utility.\n\n*Challenges in Data Collection and Validation:*\n\nDiscuss challenges encountered in collecting and validating the smart watch data used for sleep apnea prediction.\n\nAddress issues such as data quality, variability in sensor accuracy across different smart watch models, and the need for robust validation studies to ensure reliability of predictions   \n\n*Impact on Personalized Healthcare:*\n\nExplore the potential impact of personalized sleep apnea risk assessment based  on individualized smart watch data.\n\nDiscuss how tailored recommendations or interventions could be provided to users identified at risk, such as lifestyle modifications or medical consultations.\n\n*Integration with Healthcare Systems:*\n\nDiscuss the feasibility and benefits of integrating your smart watch system with existing healthcare systems, such as electronic health records (EHR) or telemedicine platforms.\n\nExplore opportunities for collaboration with healthcare providers to streamline patient care and enhance monitoring of sleep disorders.\n\n*Longitudinal Monitoring and Predictive Maintenance:*\n\nConsider the value of longitudinal monitoring using smart watches for detecting changes in sleep patterns and early signs of sleep disorders like sleep apnea.\n\nDiscuss strategies for predictive maintenance of the smart watch system itself, ensuring continuous and reliable operation for users.\n\n#pagebreak()\n*Public Health Implications:*\n\nDiscuss broader public health implications of using wearable technology for proactive health monitoring.\nAddress how widespread adoption of smart watches capable of detecting sleep apnea and cardiac anomalies could potentially reduce healthcare disparities and improve overall population health outcomes.\n\n*Regulatory and Policy Considerations:*\n\nExplore regulatory requirements and guidelines governing the development and deployment of medical devices or health-related applications using smart watches.\n\nDiscuss compliance with data protection regulations (e.g., GDPR, HIPAA) and ethical standards in handling sensitive health information.\n\n*User Education and Engagement:*\n\nDiscuss the importance of user education and engagement in maximizing the benefits of your smart watch system.\n\nExplore strategies for promoting awareness about sleep apnea, the role  of  wearable technology in health monitoring, and encouraging proactive health  behaviors among users.\n\n*Limitations and Future Research Directions:*\n\nAddress limitations of your current study, such as the need for larger datasets, longer-term validation studies, or improvements in sensor technology.\n\nPropose future research directions, such as exploring additional biomarkers or incorporating advanced AI techniques for more accurate prediction models."
  },
  {
  "input": {
    "starting_page": 38,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- VII  :  Conclusion\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:\n\nSleep  Apnea  Prediction:\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\nSOS System Integration:\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\nMethodology and Approach:\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\nContribution to Healthcare:\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\nFuture Scope\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n1.Advanced Wearable Technology Multi-Function Wearables:\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\nContinuous Monitoring: Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use .\n\nIntegration of Additional Health Factors Stress Monitoring:\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\nFood Intake Tracking: Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\nWater Intake Monitoring: Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\nComprehensive Data Analysis Holistic Health Analysis:\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\nPredictive Analytics: Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 38\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:\n\n*Sleep  Apnea  Prediction:*\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\n*SOS System Integration:*\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\n*Methodology and Approach:*\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\n#pagebreak()\n*Contribution to Healthcare:*\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\n*Future Scope*\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n*1.Advanced Wearable Technology Multi-Function Wearables:*\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\n*Continuous Monitoring:* Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use.\n\n*2.Integration of Additional Health Factors Stress Monitoring:*\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\n*Food Intake Tracking:* Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\n*Water Intake Monitoring:* Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\n*3.Comprehensive Data Analysis Holistic Health Analysis:*\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\n*Predictive Analytics:* Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures."
  },
  {
  "input": {
    "starting_page": 33,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- VII  :  Conclusion\n\nSleep  Apnea  Prediction:\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\nSOS System Integration:\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\nMethodology and Approach:\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\nContribution to Healthcare:\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\nFuture Scope\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n1.Advanced Wearable Technology Multi-Function Wearables:\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\nContinuous Monitoring: Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use .\n\nIntegration of Additional Health Factors Stress Monitoring:\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\nFood Intake Tracking: Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\nWater Intake Monitoring: Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\nComprehensive Data Analysis Holistic Health Analysis:\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\nPredictive Analytics: Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 33\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Sleep  Apnea  Prediction:*\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\n*SOS System Integration:*\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\n*Methodology and Approach:*\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\n*Contribution to Healthcare:*\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\n#pagebreak()\n*Future Scope*\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n*1.Advanced Wearable Technology Multi-Function Wearables:*\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\n*Continuous Monitoring:* Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use.\n\n*2.Integration of Additional Health Factors Stress Monitoring:*\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\n*Food Intake Tracking:* Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\n*Water Intake Monitoring:* Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\n*3.Comprehensive Data Analysis Holistic Health Analysis:*\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\n*Predictive Analytics:* Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures.\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:"
  },
  {
  "input": {
    "starting_page": 42,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- VII  :  Conclusion\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention.\n\nSOS System Integration:\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\nThe project's objectives were achieved through the following key components:\n\nSleep  Apnea  Prediction:\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\nMethodology and Approach:\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\nContribution to Healthcare:\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\nFuture Scope\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n1.Advanced Wearable Technology Multi-Function Wearables:\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\nContinuous Monitoring: Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use .\n\nIntegration of Additional Health Factors Stress Monitoring:\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\nFood Intake Tracking: Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\nWater Intake Monitoring: Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\nComprehensive Data Analysis Holistic Health Analysis:\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\nPredictive Analytics: Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 42\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention.\n\n*SOS System Integration:*\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\n The project's objectives were achieved through the following key components:\n\n*Sleep  Apnea  Prediction:*\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\n*Methodology and Approach:*\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\n#pagebreak()\n*Contribution to Healthcare:*\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\n*Future Scope*\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n*1.Advanced Wearable Technology Multi-Function Wearables:*\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\n*Continuous Monitoring:* Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use.\n\n*2.Integration of Additional Health Factors Stress Monitoring:*\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\n*Food Intake Tracking:* Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\n*Water Intake Monitoring:* Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\n*3.Comprehensive Data Analysis Holistic Health Analysis:*\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\n*Predictive Analytics:* Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures."
  },
  {
  "input": {
    "starting_page": 45,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- VII  :  Conclusion\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:\n\nSleep  Apnea  Prediction:\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\nSOS System Integration:\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\nContribution to Healthcare:\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\nMethodology and Approach:\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\nFuture Scope\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n1.Advanced Wearable Technology Multi-Function Wearables:\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\nContinuous Monitoring: Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use .\n\nIntegration of Additional Health Factors Stress Monitoring:\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\nFood Intake Tracking: Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\nWater Intake Monitoring: Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\nComprehensive Data Analysis Holistic Health Analysis:\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\nPredictive Analytics: Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures"
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 45\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:\n\n*Sleep  Apnea  Prediction:*\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\n*SOS System Integration:*\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\n*Contribution to Healthcare:*\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety.\n\n*Methodology and Approach:*\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\n*Future Scope*\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n*1.Advanced Wearable Technology Multi-Function Wearables:*\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\n*Continuous Monitoring:* Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use.\n\n*2.Integration of Additional Health Factors Stress Monitoring:*\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\n*Food Intake Tracking:* Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\n*Water Intake Monitoring:* Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\n*3.Comprehensive Data Analysis Holistic Health Analysis:*\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\n*Predictive Analytics:* Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures."
  },
  {
  "input": {
    "starting_page": 15,
    "academic_year": "2025-26",
    "department": "B.E/Dept of CSE/BNMIT",
    "project_title": "Sleep Apnea Detection",
    "contents_txt":"CHAPTER- VII  :  Conclusion\n\nFuture Scope\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n1.Advanced Wearable Technology Multi-Function Wearables:\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\nContinuous Monitoring: Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use .\n\nIntegration of Additional Health Factors Stress Monitoring:\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\nFood Intake Tracking: Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\nWater Intake Monitoring: Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\nComprehensive Data Analysis Holistic Health Analysis:\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\nPredictive Analytics: Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:\n\nSleep  Apnea  Prediction:\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\nSOS System Integration:\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\nMethodology and Approach:\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\nContribution to Healthcare:\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety."
  },
  "output": "#page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  header: none,\n  footer: none,\n  numbering: none\n)[\n  #set text(font: \"Times New Roman\", size: 12pt, fill: black)\n  #set align(center)\n  #v(1fr)\n\n  #text(size: 24pt, weight: \"bold\")[CHAPTER -- VII]\n  #v(0.5em)\n  #text(size: 24pt, weight: \"bold\")[CONCLUSION]\n\n  #v(1fr)\n]\n\n#let starting_page = 15\n#counter(page).update(starting_page+1)\n\n#let department = \"B.E/Dept of CSE/BNMIT\"\n#let academic_year = \"2025-26\"\n#let project_title = \"Sleep Apnea Detection\"\n\n#set page(\n  paper: \"a4\",\n  margin: (top: 1in, bottom: 1in, left: 1.25in, right: 1in),\n  numbering: \"1\",\n  header: context [\n    #if counter(page).get().first() >= starting_page+2 [\n      #set text(size: 12pt, font: \"Times New Roman\", weight: \"bold\")\n      #block(\n        width: 100%,\n        inset: (top: 5pt, bottom: 8pt),\n      )[\n        #stack(\n          dir: ttb,\n          project_title,\n          v(0.3em),\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0))\n        )\n      ]\n    ]\n  ],\n  footer: context [\n    #set text(size: 10pt, font: \"Times New Roman\")\n    #block(\n      width: 100%,\n      inset: (top: 8pt, bottom: 5pt),\n    )[\n      #stack(\n          dir: ttb,\n          spacing: 1.5pt, \n          line(length: 100%, stroke: 3pt + rgb(128, 0, 0)),\n          v(0.3em),\n          line(length: 100%, stroke: 0.7pt + rgb(128, 0, 0))\n        )\n      #grid(\n        columns: (1fr, 1fr, 1fr),\n        align: (left, center, right),\n      )[\n        #department\n      ][\n        Page #counter(page).display()\n      ][\n        #academic_year\n      ]\n    ]\n  ]\n)\n\n#text(size: 16pt, weight: \"bold\")[CHAPTER - VII]\n#align(center)[#text(size: 16pt, weight: \"bold\")[CONCLUSION]]\n\n#set par(justify: true, leading: 1.5em, spacing: 2em)\n\n*Future Scope*\n\nTo create a truly comprehensive and integrated health monitoring system that can predict sleep apnea, monitor critical heart rate drops, and include additional factors such as stress, food intake, and water intake, here are some further considerations:\n\n*1.Advanced Wearable Technology Multi-Function Wearables:*\n\nDeveloping wearables (like smartwatches) with sensors capable of monitoring a wide range of health metrics, including heart rate, respiratory rate, oxygen saturation, stress levels, hydration, and even nutritional intake.\n\n*Continuous Monitoring:* Ensuring the wearables provide continuous, real-time data collection and analysis, allowing for immediate intervention when  necessary. User Comfort: Designing the wearables to be lightweight, comfortable, and stylish to encourage regular use.\n\n*2.Integration of Additional Health Factors Stress Monitoring:*\n\nUtilizing heart rate variability (HRV) and other physiological indicators to assess stress levels. Incorporating features like guided breathing exercises or stress management tips.\n\n*Food Intake Tracking:* Implementing features to log food intake via manual entry, barcode scanning, or even advanced image recognition technology that can identify foods from pictures.\n\n*Water Intake Monitoring:* Including reminders and tracking capabilities for water intake, potentially integrating with smart water bottles that can automatically record hydration data.\n\n*3.Comprehensive Data Analysis Holistic Health Analysis:*\n\nUsing AI and machine learning to analyze the interplay between sleep quality, stress levels, dietary habits, and hydration status to provide personalized health insights.\n\n*Predictive Analytics:* Developing algorithms that can predict health issues based on combined data from various health factors, allowing for preventive measures.\n\nThe smart watch data  analyzer  developed in this project represents a significant advancement in health monitoring technology, focusing on predicting sleep  apnea  and preventing potential heart attacks through real-time monitoring and proactive intervention. The project's objectives were achieved through the following key components:\n\n*Sleep  Apnea  Prediction:*\n\nUtilized machine learning algorithms to analyze sleep data, including metrics like sleep stages, awakenings, and sleep efficiency, to predict the presence and severity of sleep apnea. This predictive capability offers early detection and intervention opportunities for individuals at risk.\n\n*SOS System Integration:*\n\nImplemented an SOS system that continuously monitors heart rate data from the smart watch. When anomalies such as a sudden drop in heart rate are detected, the system triggers immediate alerts to designated emergency contacts. This functionality aims to mitigate risks associated with heart-related emergencies, potentially saving lives.\n\n*Methodology and Approach:*\n\nThe methodology involved rigorous data collection, preprocessing, and feature engineering to derive meaningful  insights from sleep and lifestyle data. Statistical analyses validated the relationships  between  lifestyle factors (e.g., caffeine consumption) and sleep disturbances, supporting the predictive models' robustness.\n\nThis project underscores the potential of wearable technology in personalized healthcare, offering users proactive health monitoring capabilities and timely interventions. Future iterations could explore enhancing model accuracy, integrating additional health metrics, and optimizing the SOS system's responsiveness and usability.\n\n*Contribution to Healthcare:*\n\nBy integrating advanced data analytics with real-time health monitoring, the project contributes to improving health outcomes and enhancing quality of life. The ability to predict sleep apnea and prevent heart-related emergencies exemplifies the project's impact on public health and safety."
  }
]